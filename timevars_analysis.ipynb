{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import pandas as pd\n","import os, re, sqlite3\n","import numpy as np\n","import gc # ë©”ëª¨ë¦¬ ê´€ë¦¬ë¥¼ ìœ„í•œ ê°€ë¹„ì§€ ì»¬ë ‰í„°\n","\n","# === Cell 0: Paths & Imports (ê²½ë¡œ ì„¤ì •) ===\n","# âš ï¸ í˜„ì¬ MyDrive ë°”ë¡œ ì•„ë˜ íŒŒì¼ ìœ„ì¹˜ì— ë§ì¶° ìˆ˜ì •ëœ ê²½ë¡œì…ë‹ˆë‹¤.\n","DB_PATH = \"/content/drive/MyDrive/MIMIC4-hosp-icu.db\"\n","COHORT_CSV = \"/content/drive/MyDrive/cohort.csv\"\n","SAVE_PATH  = \"/content/drive/MyDrive/final_cohort_with_timevars.csv\"\n","\n","conn_settings = {\n","    'journal_mode': 'OFF',\n","    'synchronous': 'OFF',\n","    'temp_store': 'MEMORY',\n","    'cache_size': -500000\n","}\n","def get_db_connection(path):\n","    conn = sqlite3.connect(f\"file:{path}?mode=ro\", uri=True)\n","    for key, value in conn_settings.items():\n","        conn.execute(f\"PRAGMA {key}={value};\")\n","    conn.commit()\n","    return conn\n","\n","# DB ì—°ê²° (Cell 0ì—ì„œ DB_PATHë§Œ ì‚¬ìš©í•˜ì—¬ ì—°ê²°ì„ ì‹œë„í•˜ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„)\n","try:\n","    conn = get_db_connection(DB_PATH)\n","    conn.close()\n","    print(\"DB connection test successful.\")\n","except Exception as e:\n","    print(f\"DB ì—°ê²° ì‹¤íŒ¨: {e}\")\n","\n","# === Cell 1: Load cohort.csv & Fix Timezone Error ===\n","print(\"\\n=== Cell 1: Load cohort.csv ì‹œì‘ ===\")\n","try:\n","    cohort = pd.read_csv(COHORT_CSV)\n","    print(f\"[ì„ í–‰ 1] cohort.csv ë¡œë“œ ì™„ë£Œ (Shape: {cohort.shape})\")\n","\n","    # ì‹œê°„ ì»¬ëŸ¼ ìë™ íŒŒì‹± ë° UTC í†µì¼ (Timezone Error í•´ê²°)\n","    for c in cohort.columns:\n","        if any(k in c.lower() for k in [\"time\",\"date\",\"intime\",\"outtime\",\"admit\",\"disch\",\"ecg\"]):\n","            cohort[c] = pd.to_datetime(cohort[c], errors=\"coerce\", utc=True)\n","\n","    # door_to_ecg_min ì»¬ëŸ¼ ì•ˆì •í™”: ì‹œê°„ì°¨ê°€ ì´ë¯¸ ê³„ì‚°ëœ 'ë¶„' ë‹¨ìœ„ì´ë¯€ë¡œ floatìœ¼ë¡œ ë³€í™˜\n","    if 'door_to_ecg_min' in cohort.columns:\n","        cohort['door_to_ecg_min'] = pd.to_numeric(cohort['door_to_ecg_min'], errors='coerce')\n","        if np.issubdtype(cohort['door_to_ecg_min'].dtype, np.datetime64):\n","             cohort['door_to_ecg_min'] = (cohort['door_to_ecg_min'] - cohort['door_to_ecg_min'].min()).dt.total_seconds() / 60.0\n","\n","    print(f\"âœ… Timezone ë° door_to_ecg_min ì•ˆì •í™” ì™„ë£Œ.\")\n","\n","except Exception as e:\n","    print(f\"âŒ ERROR in Cell 1: {e}\")\n","\n","\n","# === Cell 2: admissions merge & Boarding Delay ===\n","print(\"\\n=== Cell 2: admissions merge & Boarding Delay ì‹œì‘ ===\")\n","try:\n","    conn = get_db_connection(DB_PATH)\n","\n","    admissions = pd.read_sql(\"\"\"\n","        SELECT subject_id, hadm_id, admittime, dischtime, admission_location\n","        FROM admissions\n","    \"\"\", conn, parse_dates=[\"admittime\",\"dischtime\"])\n","\n","    conn.close()\n","\n","    # ì‹œê°„ëŒ€ UTC í†µì¼ ë° ID ë‹¤ìš´ìºìŠ¤íŒ…\n","    admissions[\"admittime\"] = admissions[\"admittime\"].dt.tz_localize('UTC')\n","    admissions[\"dischtime\"] = admissions[\"dischtime\"].dt.tz_localize('UTC')\n","    admissions['subject_id'] = admissions['subject_id'].astype('int32')\n","    admissions['hadm_id'] = admissions['hadm_id'].astype('Int32')\n","\n","    cohort2 = cohort.merge(\n","        admissions, on=[\"subject_id\",\"hadm_id\"], how=\"left\"\n","    )\n","\n","    # Boarding Delay = (admittime - outtime) [ë¶„]\n","    cohort2[\"boarding_delay_min\"] = (cohort2[\"admittime\"] - cohort2[\"outtime\"]).dt.total_seconds() / 60.0\n","\n","    print(\"After admissions merge:\", cohort2.shape)\n","\n","    del admissions\n","    del cohort\n","    gc.collect()\n","\n","except Exception as e:\n","    print(f\"âŒ ERROR in Cell 2: {e}\")\n","\n","\n","# === Cell 3: Lab TAT (ì±„í˜ˆâ†’ê²°ê³¼ê¸°ë¡ ì‹œê°„) ê³„ì‚° ===\n","# âš ï¸ ì´ ì½”ë“œëŠ” Lab TAT ê³„ì‚° í›„ cohort2ì— ë³‘í•©í•˜ëŠ” ë¡œì§ì„ í¬í•¨í•©ë‹ˆë‹¤.\n","print(\"\\n=== Cell 3: Lab TAT ê³„ì‚° ì‹œì‘ ===\")\n","try:\n","    conn = get_db_connection(DB_PATH)\n","\n","    d_labitems = pd.read_sql(\"SELECT itemid, label FROM d_labitems\", conn)\n","    lab_targets = d_labitems[d_labitems[\"label\"].str.contains(r\"(?:troponin|ck[-\\s]?mb)\", flags=re.I, regex=True, na=False)].copy()\n","    target_itemids = sorted(set(lab_targets[\"itemid\"].tolist()))\n","\n","    tmp = cohort2.dropna(subset=[\"intime\",\"admittime\"])[[\"subject_id\",\"hadm_id\",\"intime\",\"admittime\"]].drop_duplicates().copy()\n","    tmp[\"win_start\"] = tmp[\"intime\"]\n","    tmp[\"win_end\"]   = tmp[\"admittime\"] + pd.Timedelta(hours=24)\n","\n","    hadms = tmp[\"hadm_id\"].dropna().unique().tolist()\n","    BATCH = 1500\n","    all_parts = []\n","\n","    for i in range(0, len(hadms), BATCH):\n","        batch = hadms[i:i+BATCH]\n","        placeholders = \",\".join([\"?\"]*len(batch))\n","        sql = f\"\"\"\n","            SELECT subject_id, hadm_id, charttime, storetime, itemid\n","            FROM labevents\n","            WHERE itemid IN ({\",\".join(map(str, target_itemids))})\n","              AND hadm_id IN ({placeholders})\n","        \"\"\"\n","        le = pd.read_sql(sql, conn, params=batch, parse_dates=[\"charttime\",\"storetime\"])\n","\n","        le[\"charttime\"] = le[\"charttime\"].dt.tz_localize('UTC')\n","        le[\"storetime\"] = le[\"storetime\"].dt.tz_localize('UTC')\n","\n","        if le.empty: continue\n","\n","        merged = le.merge(tmp[[\"subject_id\",\"hadm_id\",\"win_start\",\"win_end\"]],\n","                          on=[\"subject_id\",\"hadm_id\"], how=\"inner\")\n","\n","        in_win = merged[\n","            (merged[\"charttime\"].between(merged[\"win_start\"], merged[\"win_end\"])) |\n","            (merged[\"storetime\"].between(merged[\"win_start\"], merged[\"win_end\"]))\n","        ].copy()\n","\n","        if not in_win.empty:\n","            in_win[\"lab_tat_min\"] = (in_win[\"storetime\"] - in_win[\"charttime\"]).dt.total_seconds()/60.0\n","            all_parts.append(in_win[[\"subject_id\",\"hadm_id\",\"lab_tat_min\"]])\n","\n","    conn.close()\n","\n","    if all_parts:\n","        lab_rows = pd.concat(all_parts, ignore_index=True)\n","        lab_tat  = lab_rows.groupby([\"subject_id\",\"hadm_id\"], as_index=False)[\"lab_tat_min\"].median()\n","    else:\n","        lab_tat  = pd.DataFrame(columns=[\"subject_id\",\"hadm_id\",\"lab_tat_min\"])\n","\n","    print(\"Lab TAT available hadm:\", len(lab_tat))\n","\n","    cohort2 = cohort2.merge(lab_tat, on=[\"subject_id\",\"hadm_id\"], how=\"left\")\n","\n","except Exception as e:\n","    print(f\"âŒ ERROR in Cell 3: {e}\")\n","finally:\n","    gc.collect()\n","\n","# === Cell 4: Door-to-Antithrombotic (IV) ê³„ì‚° ===\n","# âš ï¸ ì´ ì½”ë“œëŠ” Antithrombotic ê³„ì‚° í›„ cohort2ì— ë³‘í•©í•˜ëŠ” ë¡œì§ì„ í¬í•¨í•©ë‹ˆë‹¤.\n","print(\"\\n=== Cell 4: Door-to-Antithrombotic (IV) ê³„ì‚° ì‹œì‘ ===\")\n","try:\n","    conn = get_db_connection(DB_PATH)\n","\n","    hadms = cohort2[\"hadm_id\"].dropna().unique().tolist()\n","    BATCH = 2000\n","    parts = []\n","\n","    drug_keywords = [r\"\\bheparin\\b\", r\"\\bbivalirudin\\b\", r\"\\beptifibatide\\b\",\n","                     r\"\\babciximab\\b\", r\"\\btirofiban\\b\", r\"\\bclopidogrel\\b\", r\"\\baspirin\\b\"]\n","    pat = re.compile(\"|\".join(drug_keywords), flags=re.I)\n","\n","    for i in range(0, len(hadms), BATCH):\n","        batch = hadms[i:i+BATCH]\n","        placeholders = \",\".join([\"?\"]*len(batch))\n","        sql = f\"\"\"\n","            SELECT subject_id, hadm_id, starttime, drug, route\n","            FROM prescriptions\n","            WHERE hadm_id IN ({placeholders})\n","              AND starttime IS NOT NULL\n","        \"\"\"\n","        df = pd.read_sql(sql, conn, params=batch, parse_dates=[\"starttime\"])\n","\n","        df[\"starttime\"] = df[\"starttime\"].dt.tz_localize('UTC')\n","\n","        if df.empty: continue\n","\n","        df[\"drug_lc\"] = df[\"drug\"].astype(str).str.lower()\n","        df[\"route_lc\"] = df[\"route\"].astype(str).str.lower()\n","\n","        # IV + í•­í˜ˆì „/í•­í˜ˆì†ŒíŒ í›„ë³´\n","        cand = df[\n","            df[\"drug_lc\"].str.contains(pat, na=False) &\n","            df[\"route_lc\"].str.contains(r\"\\biv\\b|intravenous\", na=False)\n","        ].copy()\n","\n","        if cand.empty: continue\n","\n","        cand = cand.merge(cohort2[[\"subject_id\",\"hadm_id\",\"intime\"]],\n","                          on=[\"subject_id\",\"hadm_id\"], how=\"inner\")\n","        cand[\"door_to_antithrombotic_min\"] = (cand[\"starttime\"] - cand[\"intime\"]).dt.total_seconds()/60.0\n","\n","        g = cand.groupby([\"subject_id\",\"hadm_id\"], as_index=False)[\"door_to_antithrombotic_min\"].min()\n","        parts.append(g)\n","\n","    conn.close()\n","\n","    if parts:\n","        anti_time = pd.concat(parts, ignore_index=True)\n","    else:\n","        anti_time = pd.DataFrame(columns=[\"subject_id\",\"hadm_id\",\"door_to_antithrombotic_min\"])\n","\n","    print(\"Antithrombotic (IV) available hadm:\", len(anti_time))\n","\n","    cohort2 = cohort2.merge(anti_time, on=[\"subject_id\",\"hadm_id\"], how=\"left\")\n","\n","except Exception as e:\n","    print(f\"âŒ ERROR in Cell 4: {e}\")\n","finally:\n","    gc.collect()\n","\n","# === Cell 5: Door-to-Cath (ì‹œìˆ  ì‹œê°) ê³„ì‚° - ë¯¼ê°ë„ ê°•í™” ë° ëˆ„ë½ ì…€ ë³µêµ¬ ===\n","# âš ï¸ ì´ ì½”ë“œëŠ” n=75 ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ 'ë¯¼ê°ë„ ê°•í™”' ë¡œì§ì…ë‹ˆë‹¤.\n","print(\"\\n=== Cell 5: Door-to-Cath ê³„ì‚° ì‹œì‘ (n=75 ê°œì„  ëª©í‘œ) ===\")\n","\n","try:\n","    conn = get_db_connection(DB_PATH)\n","\n","    # ---- ì˜µì…˜ í”Œë˜ê·¸ ----\n","    USE_PROC_ICD_PREFILTER = True\n","    INCLUDE_CABG_AS_CATH   = False\n","\n","    # ---- ì •ê·œì‹(ë¼ë²¨/ìœ ë‹›) ë³´ê°• ----\n","    label_patterns = [r\"\\bcath\\b|cath\\s*lab\", r\"catheter(?:ization)?\", r\"coronary\", r\"angiograph(?:y|ie)?\", r\"angioplast(?:y|ie)?\", r\"\\bpci\\b\", r\"\\bstent\\b\", r\"interventional cardiology\", r\"left heart cath\", r\"right heart cath\", r\"\\bPTCA\\b\", r\"\\bPTCA-A\\b\", r\"balloon\", r\"dilation\", r\"angio\", r\"stent placement\", r\"percutaneous intervention\"]\n","    kw_label = re.compile(\"|\".join(label_patterns), flags=re.I)\n","\n","    unit_patterns = [r\"\\bcath\\b|cath\\s*lab\", r\"cardiac cath\", r\"cardiac catheteriz\", r\"interventional|cardio interventional\", r\"\\bccl\\\\b\", r\"ep/cath\", r\"coronary angiography\", r\"ICU.*CATH\", r\"cath.*ICU\"]\n","    kw_unit = re.compile(\"|\".join(unit_patterns), flags=re.I)\n","\n","    # ---- 0) procedures_icdë¡œ cath-ê°€ëŠ¥ hadm ë¨¼ì € ì¢íˆê¸° ----\n","    def get_likely_cath_hadm(conn, cohort_hadm_df):\n","        proc_icd = pd.read_sql(\"\"\"\n","            SELECT subject_id, hadm_id, icd_version, icd_code\n","            FROM procedures_icd\n","            WHERE hadm_id IN (SELECT DISTINCT hadm_id FROM admissions)\n","        \"\"\", conn)\n","\n","        proc_icd['subject_id'] = proc_icd['subject_id'].astype('int32')\n","        proc_icd['hadm_id'] = proc_icd['hadm_id'].astype('Int32')\n","\n","        proc_icd[\"code_clean\"] = (proc_icd[\"icd_code\"].astype(str).str.replace(\".\",\"\", regex=False).str.upper())\n","\n","        # ICD-9: PCI/ê´€ìƒë™ë§¥ ì¡°ì˜/í™•ì¥ (ì—°êµ¬ ì œì•ˆì„œ ICD í™•ì¥ ì ìš©)\n","        mask9 = (proc_icd[\"icd_version\"]==9) & (\n","            proc_icd[\"code_clean\"].str.startswith((\"360\",\"0066\",\"8855\")) |\n","            proc_icd[\"code_clean\"].str.match(r\"(0066|8855)\", na=False)\n","        )\n","        # ICD-10-PCS: ì¡°ì˜(B211/B212), PCI(0270~0273*)\n","        mask10 = (proc_icd[\"icd_version\"]==10) & (\n","            proc_icd[\"code_clean\"].str.match(r\"(B211|B212|0270|0271|0272|0273)\", na=False)\n","        )\n","\n","        likely = proc_icd[mask9 | mask10][[\"subject_id\",\"hadm_id\"]].drop_duplicates()\n","        likely = likely.merge(cohort_hadm_df[[\"subject_id\",\"hadm_id\"]].drop_duplicates(),\n","                              on=[\"subject_id\",\"hadm_id\"], how=\"inner\")\n","\n","        del proc_icd\n","        gc.collect()\n","        return likely\n","\n","    if USE_PROC_ICD_PREFILTER:\n","        likely_cath = get_likely_cath_hadm(conn, cohort2)\n","        hadms = likely_cath[\"hadm_id\"].unique().tolist()\n","        print(\"procedures_icd ê¸°ë°˜ cath-ê°€ëŠ¥ hadm:\", len(hadms))\n","    else:\n","        hadms = cohort2[\"hadm_id\"].dropna().unique().tolist()\n","\n","    hadms.sort()\n","\n","    # ---- d_items ë¼ë²¨ ì¤€ë¹„ ----\n","    try:\n","        d_items = pd.read_sql(\"SELECT itemid, label FROM d_items\", conn)\n","        d_items[\"label_lc\"] = d_items[\"label\"].astype(str).str.lower()\n","    except Exception:\n","        d_items = pd.DataFrame(columns=[\"itemid\",\"label\",\"label_lc\"])\n","\n","    # ---- A) procedureevents ê¸°ë°˜ íƒìƒ‰ ----\n","    BATCH = 2000\n","    proc_parts = []\n","\n","    for i in range(0, len(hadms), BATCH):\n","        batch = hadms[i:i+BATCH]\n","        placeholders = \",\".join([\"?\"]*len(batch))\n","        sql = f\"\"\"\n","            SELECT subject_id, hadm_id, starttime, endtime, itemid\n","            FROM procedureevents\n","            WHERE hadm_id IN ({placeholders})\n","              AND starttime IS NOT NULL\n","        \"\"\"\n","        pe = pd.read_sql(sql, conn, params=batch, parse_dates=[\"starttime\",\"endtime\"])\n","        if pe.empty: continue\n","\n","        pe[\"starttime\"] = pe[\"starttime\"].dt.tz_localize('UTC')\n","        pe[\"endtime\"] = pe[\"endtime\"].dt.tz_localize('UTC')\n","\n","        if not d_items.empty:\n","            pe = pe.merge(d_items[[\"itemid\",\"label_lc\"]], on=\"itemid\", how=\"left\")\n","        else:\n","            pe[\"label_lc\"] = \"\"\n","\n","        pe_cath = pe[ pe[\"label_lc\"].str.contains(kw_label, na=False) ].copy()\n","        if pe_cath.empty: continue\n","\n","        pe_cath = pe_cath.merge(cohort2[[\"subject_id\",\"hadm_id\",\"intime\"]], on=[\"subject_id\",\"hadm_id\"], how=\"inner\")\n","        pe_cath[\"door_to_cath_min_A\"] = (pe_cath[\"starttime\"] - pe_cath[\"intime\"]).dt.total_seconds()/60.0\n","        gA = pe_cath.groupby([\"subject_id\",\"hadm_id\"], as_index=False)[\"door_to_cath_min_A\"].min()\n","        proc_parts.append(gA)\n","        # print(f\"proc batch {i//BATCH+1}: rows={len(pe)}, cath={len(pe_cath)}, hadm_A={len(gA)}\")\n","\n","    cath_A = (pd.concat(proc_parts, ignore_index=True)\n","              if proc_parts else\n","              pd.DataFrame(columns=[\"subject_id\",\"hadm_id\",\"door_to_cath_min_A\"]))\n","\n","    # ---- B) transfers ê¸°ë°˜ íƒìƒ‰ ----\n","    trf_parts = []\n","    for i in range(0, len(hadms), BATCH):\n","        batch = hadms[i:i+BATCH]\n","        placeholders = \",\".join([\"?\"]*len(batch))\n","        sql = f\"\"\"\n","            SELECT subject_id, hadm_id, intime, outtime, careunit\n","            FROM transfers\n","            WHERE hadm_id IN ({placeholders})\n","              AND intime IS NOT NULL\n","        \"\"\"\n","        tr = pd.read_sql(sql, conn, params=batch, parse_dates=[\"intime\",\"outtime\"])\n","        if tr.empty: continue\n","\n","        tr[\"intime\"] = tr[\"intime\"].dt.tz_localize('UTC')\n","        tr[\"outtime\"] = tr[\"outtime\"].dt.tz_localize('UTC')\n","\n","        tr[\"careunit_lc\"] = tr[\"careunit\"].astype(str).str.lower()\n","        tr_cath = tr[ tr[\"careunit_lc\"].str.contains(kw_unit, na=False) ].copy()\n","        if tr_cath.empty: continue\n","\n","        tr_cath = tr_cath.merge(cohort2[[\"subject_id\",\"hadm_id\",\"intime\"]], on=[\"subject_id\",\"hadm_id\"], how=\"inner\", suffixes=(\"\",\"_ed\"))\n","        tr_cath[\"door_to_cath_min_B\"] = (tr_cath[\"intime\"] - tr_cath[\"intime_ed\"]).dt.total_seconds()/60.0\n","        gB = tr_cath.groupby([\"subject_id\",\"hadm_id\"], as_index=False)[\"door_to_cath_min_B\"].min()\n","        trf_parts.append(gB)\n","        # print(f\"trf batch {i//BATCH+1}: rows={len(tr)}, cath_unit={len(tr_cath)}, hadm_B={len(gB)}\")\n","\n","    conn.close()\n","\n","    cath_B = (pd.concat(trf_parts, ignore_index=True)\n","              if trf_parts else\n","              pd.DataFrame(columns=[\"subject_id\",\"hadm_id\",\"door_to_cath_min_B\"]))\n","\n","    # ---- A ìš°ì„ , ì—†ìœ¼ë©´ Bë¡œ ë³´ì™„ ----\n","    door_to_cath = (cohort2[[\"subject_id\",\"hadm_id\"]]\n","                    .drop_duplicates()\n","                    .merge(cath_A, on=[\"subject_id\",\"hadm_id\"], how=\"left\")\n","                    .merge(cath_B, on=[\"subject_id\",\"hadm_id\"], how=\"left\"))\n","\n","    door_to_cath[\"door_to_cath_min\"] = door_to_cath[\"door_to_cath_min_A\"].combine_first(\n","        door_to_cath[\"door_to_cath_min_B\"]\n","    )\n","\n","    # âš ï¸ [ë³€ìˆ˜ ì •ì˜] ì´ ë³€ìˆ˜ê°€ Cell 6ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.\n","    cath_df = door_to_cath.copy()\n","\n","    print(\"Door-to-Cath (A or B):\", cath_df[\"door_to_cath_min\"].notna().sum())\n","\n","except Exception as e:\n","    print(f\"âŒ ERROR in Cell 5: {e}\")\n","finally:\n","    gc.collect()\n","\n","# === Cell 6: ì‹œê°„ë³€ìˆ˜ ë³‘í•© ë° ìµœì¢… ì €ì¥ ===\n","print(\"\\n=== Cell 6: ì‹œê°„ë³€ìˆ˜ ë³‘í•© ë° ìµœì¢… ì €ì¥ ì‹œì‘ ===\")\n","try:\n","    if 'cath_df' not in locals():\n","         cath_df = pd.DataFrame(columns=[\"subject_id\", \"hadm_id\", \"door_to_cath_min\"])\n","         print(\"âš ï¸ cath_df ë³€ìˆ˜ê°€ ì •ì˜ë˜ì§€ ì•Šì•„ ë¹ˆ DataFrameìœ¼ë¡œ ë³‘í•©ì„ ì‹œë„í•©ë‹ˆë‹¤.\")\n","\n","    # 1. Door-to-Cath ë³‘í•© (A, B ë°©ë²• ì¤‘ ìµœì†Œê°’)\n","    cohort2 = cohort2.merge(cath_df[[\"subject_id\", \"hadm_id\", \"door_to_cath_min\"]],\n","                            on=[\"subject_id\", \"hadm_id\"], how=\"left\")\n","\n","    # 2. Boarding DelayëŠ” Cell 2ì—ì„œ ì´ë¯¸ ê³„ì‚°ë¨\n","\n","    # 3. ìµœì¢… DF ì •ë¦¬ ë° ì¸ë±ìŠ¤ ì´ˆê¸°í™”\n","    out = cohort2.copy()\n","    out.reset_index(drop=True, inplace=True)\n","\n","    print(f\"Merged final shape: {out.shape}\")\n","\n","    # 4. ìµœì¢… ì €ì¥ (CSV)\n","    out.to_csv(SAVE_PATH, index=False, encoding='utf-8-sig')\n","\n","    print(f\"\\nâœ… ìµœì¢… ì½”í˜¸íŠ¸ íŒŒì¼ ì €ì¥ ì™„ë£Œ\")\n","    print(f\"ì €ì¥ ê²½ë¡œ: {SAVE_PATH}\")\n","\n","except Exception as e:\n","    print(f\"âŒ ERROR in Cell 6: {e}\")\n","finally:\n","    gc.collect()\n",""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EORjsp-T6r5Z","executionInfo":{"status":"ok","timestamp":1762479170124,"user_tz":-540,"elapsed":463365,"user":{"displayName":"ì§€ë¯¼ì„œ","userId":"05836704273870504470"}},"outputId":"0ed47adb-2c1a-4698-8ef6-57d40883073e"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["DB connection test successful.\n","\n","=== Cell 1: Load cohort.csv ì‹œì‘ ===\n","[ì„ í–‰ 1] cohort.csv ë¡œë“œ ì™„ë£Œ (Shape: (465, 11))\n","âœ… Timezone ë° door_to_ecg_min ì•ˆì •í™” ì™„ë£Œ.\n","\n","=== Cell 2: admissions merge & Boarding Delay ì‹œì‘ ===\n","After admissions merge: (465, 15)\n","\n","=== Cell 3: Lab TAT ê³„ì‚° ì‹œì‘ ===\n","Lab TAT available hadm: 433\n","\n","=== Cell 4: Door-to-Antithrombotic (IV) ê³„ì‚° ì‹œì‘ ===\n","Antithrombotic (IV) available hadm: 301\n","\n","=== Cell 5: Door-to-Cath ê³„ì‚° ì‹œì‘ (n=75 ê°œì„  ëª©í‘œ) ===\n","procedures_icd ê¸°ë°˜ cath-ê°€ëŠ¥ hadm: 464\n","Door-to-Cath (A or B): 75\n","\n","=== Cell 6: ì‹œê°„ë³€ìˆ˜ ë³‘í•© ë° ìµœì¢… ì €ì¥ ì‹œì‘ ===\n","Merged final shape: (465, 18)\n","\n","âœ… ìµœì¢… ì½”í˜¸íŠ¸ íŒŒì¼ ì €ì¥ ì™„ë£Œ\n","ì €ì¥ ê²½ë¡œ: /content/drive/MyDrive/final_cohort_with_timevars.csv\n"]}]},{"cell_type":"code","source":["# === Cell 5 (v2.2): Door-to-Cath proxy with ICD prefilter + maximal regex + safety guards ===\n","import re, sqlite3\n","import pandas as pd\n","import gc\n","import numpy as np\n","\n","# ---- ì˜µì…˜ ----\n","USE_PROC_ICD_PREFILTER = True\n","INCLUDE_CABG_AS_CATH   = False       # í•„ìš” ì‹œ CABGë„ í¬í•¨\n","ALLOW_NEG_MINUTES      = 30          # ë¬¸ê³¼ ì‹œê³„ ë¶ˆì¼ì¹˜ í—ˆìš© ë²”ìœ„(ë¶„) : -30ë¶„ê¹Œì§€ í—ˆìš©\n","MAX_MINUTES_WINDOW     = 72*60       # 72ì‹œê°„ ì´ˆê³¼ ê°’ì€ ì´ìƒì¹˜ë¡œ ì œê±°\n","\n","# ---- ì •ê·œì‹ ----\n","# procedureevents.label í›„ë³´ (A)\n","label_patterns = [\n","    r\"heart\\s*cath\", r\"cardiac\\s*proc\", r\"cardiac\\s*interven\",\n","    r\"\\bcath\\b\", r\"catheter(?:ization)?\",\n","    r\"cor\\s*angio(?:gram|graphy|grafi|gra|)?\",\n","    r\"coronary\",\n","    r\"angiograph(?:y|ie)?\", r\"angioplast(?:y|ie)?\",\n","    r\"\\bpci\\b\", r\"\\bptca\\b\", r\"\\bstent\\b\", r\"stent placement\",\n","    r\"balloon\", r\"dilation\",\n","    r\"percutaneous intervention\"\n","]\n","kw_label = re.compile(\"|\".join(label_patterns), flags=re.I)\n","\n","# transfers.careunit í›„ë³´ (B)\n","unit_patterns = [\n","    r\"cath\\s*lab\", r\"cardiac\\s*cath\", r\"interventional\", r\"cardio interventional\",\n","    r\"\\bccl\\b\", r\"ep/cath\",\n","    r\"icu.*cath|cath.*icu\", r\"\\bpcu\\b|\\bccu\\b\",\n","    r\"telemetry\", r\"monitoring\\s*unit\"\n","]\n","kw_unit = re.compile(\"|\".join(unit_patterns), flags=re.I)\n","\n","# ---- ì‹œê°„ëŒ€ ì•ˆì „ ë³´ì • ----\n","def to_utc_safe(s: pd.Series) -> pd.Series:\n","    \"\"\"naive -> UTC localize, tz-aware -> UTC convert, null safe\"\"\"\n","    s = pd.to_datetime(s, errors=\"coerce\", utc=False)\n","    try:\n","        if getattr(s.dt, \"tz\", None) is not None:\n","            return s.dt.tz_convert(\"UTC\")\n","        else:\n","            return s.dt.tz_localize(\"UTC\")\n","    except Exception:\n","        return pd.to_datetime(s.astype(str), errors=\"coerce\", utc=True)\n","\n","# ---- ICD prefilter ----\n","def get_likely_cath_hadm(conn, cohort_hadm_df):\n","    proc_icd = pd.read_sql(\"\"\"\n","        SELECT subject_id, hadm_id, icd_version, icd_code\n","        FROM procedures_icd\n","        WHERE hadm_id IN (SELECT DISTINCT hadm_id FROM admissions)\n","    \"\"\", conn)\n","\n","    proc_icd['subject_id'] = proc_icd['subject_id'].astype('Int64', errors='ignore')\n","    proc_icd['hadm_id']    = proc_icd['hadm_id'].astype('Int64', errors='ignore')\n","    proc_icd[\"code_clean\"] = (proc_icd[\"icd_code\"].astype(str)\n","                              .str.replace(\".\", \"\", regex=False)\n","                              .str.upper())\n","\n","    # ICD-9: PCI(360x), Stent(00.66), Coronary angio(88.55)\n","    mask9 = (proc_icd[\"icd_version\"]==9) & (\n","        proc_icd[\"code_clean\"].str.startswith((\"360\")) |\n","        proc_icd[\"code_clean\"].str.match(r\"(0066|8855)\", na=False)\n","    )\n","    # ICD-10-PCS: Coronary angio(B211/B212), PCI(0270~0273)\n","    mask10 = (proc_icd[\"icd_version\"]==10) & (\n","        proc_icd[\"code_clean\"].str.match(r\"(B211|B212|0270|0271|0272|0273)\", na=False)\n","    )\n","\n","    # ì˜µì…˜: CABG í¬í•¨\n","    if INCLUDE_CABG_AS_CATH:\n","        # ICD-9 CABG 36.1x, ICD-10-PCS 0210~0213 (ë³´ìˆ˜ì  ë²”ìœ„)\n","        mask9  = mask9  | ((proc_icd[\"icd_version\"]==9)  & proc_icd[\"code_clean\"].str.match(r\"361\\d\", na=False))\n","        mask10 = mask10 | ((proc_icd[\"icd_version\"]==10) & proc_icd[\"code_clean\"].str.match(r\"021[0-3]\", na=False))\n","\n","    likely = proc_icd[mask9 | mask10][[\"subject_id\",\"hadm_id\"]].drop_duplicates()\n","    likely = likely.merge(cohort_hadm_df[[\"subject_id\",\"hadm_id\"]].drop_duplicates(),\n","                          on=[\"subject_id\",\"hadm_id\"], how=\"inner\")\n","\n","    del proc_icd; gc.collect()\n","    return likely\n","\n","# ---- ì´ìƒì¹˜ ì»· ----\n","def apply_sanity_window(df: pd.DataFrame, col: str) -> pd.DataFrame:\n","    if col not in df.columns or df.empty:\n","        return df\n","    lo = -float(ALLOW_NEG_MINUTES)\n","    hi = float(MAX_MINUTES_WINDOW)\n","    return df[(df[col].notna()) & (df[col] >= lo) & (df[col] <= hi)].copy()\n","\n","print(\"\\n=== Cell 5: Door-to-Cath ì¶”ì¶œ (íŒë‹¨ ê¸°ì¤€ ìˆ˜ì • v2.2) ì‹œì‘ ===\")\n","try:\n","    conn = get_db_connection(DB_PATH)\n","\n","    # 0) ICD prefilter\n","    if USE_PROC_ICD_PREFILTER:\n","        likely_cath = get_likely_cath_hadm(conn, cohort2)\n","        hadms = sorted(likely_cath[\"hadm_id\"].dropna().unique().tolist())\n","        print(f\"procedures_icd ê¸°ë°˜ cath-ê°€ëŠ¥ hadm: {len(hadms)}\")\n","    else:\n","        hadms = sorted(cohort2[\"hadm_id\"].dropna().unique().tolist())\n","\n","    # d_items & cohort core\n","    d_items = pd.read_sql(\"SELECT itemid, label FROM d_items\", conn)\n","    d_items[\"label_lc\"] = d_items[\"label\"].astype(str).str.lower()\n","\n","    cohort_core = cohort2[[\"subject_id\",\"hadm_id\",\"intime\"]].drop_duplicates().copy()\n","    cohort_core[\"intime\"] = to_utc_safe(cohort_core[\"intime\"])\n","\n","    # ---- A) procedureevents ê¸°ë°˜ ----\n","    BATCH = 2000\n","    proc_parts = []\n","    for i in range(0, len(hadms), BATCH):\n","        batch = hadms[i:i+BATCH]\n","        placeholders = \",\".join([\"?\"]*len(batch))\n","        pe = pd.read_sql(f\"\"\"\n","            SELECT subject_id, hadm_id, starttime, endtime, itemid\n","            FROM procedureevents\n","            WHERE hadm_id IN ({placeholders}) AND starttime IS NOT NULL\n","        \"\"\", conn, params=batch)\n","        if pe.empty:\n","            continue\n","\n","        pe[\"starttime\"] = to_utc_safe(pe[\"starttime\"])\n","        pe[\"endtime\"]   = to_utc_safe(pe[\"endtime\"])\n","\n","        pe = pe.merge(d_items[[\"itemid\",\"label_lc\"]], on=\"itemid\", how=\"left\")\n","        pe_cath = pe[pe[\"label_lc\"].astype(str).str.contains(kw_label, na=False)].copy()\n","        if pe_cath.empty:\n","            continue\n","\n","        # merge í›„ intime ëª…ì‹œ ë¦¬ë„¤ì„(intime_door) â€” KeyError ë°©ì§€\n","        pe_cath = pe_cath.merge(cohort_core, on=[\"subject_id\",\"hadm_id\"], how=\"inner\")\n","        if \"intime_door\" not in pe_cath.columns and \"intime\" in pe_cath.columns:\n","            pe_cath = pe_cath.rename(columns={\"intime\": \"intime_door\"})\n","\n","        pe_cath[\"door_to_cath_min_A\"] = (pe_cath[\"starttime\"] - pe_cath[\"intime_door\"]).dt.total_seconds()/60.0\n","        pe_cath = apply_sanity_window(pe_cath, \"door_to_cath_min_A\")\n","\n","        gA = (pe_cath.sort_values(\"door_to_cath_min_A\")\n","                      .groupby([\"subject_id\",\"hadm_id\"], as_index=False)\n","                      .first()[[\"subject_id\",\"hadm_id\",\"door_to_cath_min_A\"]])\n","        proc_parts.append(gA)\n","\n","    cath_A = (pd.concat(proc_parts, ignore_index=True)\n","              if proc_parts else\n","              pd.DataFrame(columns=[\"subject_id\",\"hadm_id\",\"door_to_cath_min_A\"]))\n","\n","    # ---- B) transfers ê¸°ë°˜ ----\n","    trf_parts = []\n","    for i in range(0, len(hadms), BATCH):\n","        batch = hadms[i:i+BATCH]\n","        placeholders = \",\".join([\"?\"]*len(batch))\n","        tr = pd.read_sql(f\"\"\"\n","            SELECT subject_id, hadm_id, intime, outtime, careunit\n","            FROM transfers\n","            WHERE hadm_id IN ({placeholders}) AND intime IS NOT NULL\n","        \"\"\", conn, params=batch)\n","        if tr.empty:\n","            continue\n","\n","        tr[\"intime\"]  = to_utc_safe(tr[\"intime\"])\n","        tr[\"outtime\"] = to_utc_safe(tr[\"outtime\"])\n","        tr[\"careunit_lc\"] = tr[\"careunit\"].astype(str).str.lower()\n","\n","        tr_cath = tr[tr[\"careunit_lc\"].str.contains(kw_unit, na=False)].copy()\n","        if tr_cath.empty:\n","            continue\n","\n","        tr_cath = tr_cath.merge(cohort_core, on=[\"subject_id\",\"hadm_id\"], how=\"inner\", suffixes=(\"\",\"_door\"))\n","        tr_cath[\"door_to_cath_min_B\"] = (tr_cath[\"intime\"] - tr_cath[\"intime_door\"]).dt.total_seconds()/60.0\n","        tr_cath = apply_sanity_window(tr_cath, \"door_to_cath_min_B\")\n","\n","        gB = (tr_cath.sort_values(\"door_to_cath_min_B\")\n","                      .groupby([\"subject_id\",\"hadm_id\"], as_index=False)\n","                      .first()[[\"subject_id\",\"hadm_id\",\"door_to_cath_min_B\"]])\n","        trf_parts.append(gB)\n","\n","    conn.close()\n","\n","    cath_B = (pd.concat(trf_parts, ignore_index=True)\n","              if trf_parts else\n","              pd.DataFrame(columns=[\"subject_id\",\"hadm_id\",\"door_to_cath_min_B\"]))\n","\n","    # ---- A ìš°ì„ , ì—†ìœ¼ë©´ Bë¡œ ë³´ì™„ + ì¶œì²˜ í”Œë˜ê·¸ ----\n","    door_to_cath = (cohort2[[\"subject_id\",\"hadm_id\"]]\n","                    .drop_duplicates()\n","                    .merge(cath_A, on=[\"subject_id\",\"hadm_id\"], how=\"left\")\n","                    .merge(cath_B, on=[\"subject_id\",\"hadm_id\"], how=\"left\"))\n","\n","    door_to_cath[\"door_to_cath_min\"] = door_to_cath[\"door_to_cath_min_A\"].combine_first(\n","        door_to_cath[\"door_to_cath_min_B\"]\n","    )\n","\n","    # dtype ì¶©ëŒ ë°©ì§€: ë¬¸ìì—´ ì»¬ëŸ¼ì€ string dtype + pd.NAë¡œ ìƒì„±\n","    condA = door_to_cath[\"door_to_cath_min_A\"].notna()\n","    condB = door_to_cath[\"door_to_cath_min_B\"].notna()\n","\n","    door_to_cath[\"source\"] = pd.Series(pd.NA, index=door_to_cath.index, dtype=\"string\")\n","    door_to_cath.loc[condB, \"source\"] = \"B_transfers\"\n","    door_to_cath.loc[condA, \"source\"] = \"A_procedureevents\"\n","\n","    door_to_cath[\"inferred\"] = door_to_cath[\"source\"].eq(\"B_transfers\").fillna(False)\n","\n","    # ìµœì¢… ê²°ê³¼\n","    cath_df = door_to_cath.copy()\n","\n","    # ë¦¬í¬íŠ¸\n","    n_detect = cath_df['door_to_cath_min'].notna().sum()\n","    med_val  = cath_df.loc[cath_df['door_to_cath_min'].notna(), 'door_to_cath_min'].median()\n","    n_A      = cath_df['door_to_cath_min_A'].notna().sum()\n","    n_B      = cath_df['door_to_cath_min_B'].notna().sum()\n","    print(\"\\nâœ… Door-to-Cath ê°œì„ ëœ íƒì§€ ê²°ê³¼:\")\n","    print(f\"  â€¢ Door-to-Cath (A or B): {n_detect} ëª…\")\n","    print(f\"  â€¢ A(procedureevents) ë‹¨ë… íƒì§€: {n_A} ëª…\")\n","    print(f\"  â€¢ B(transfers) ë‹¨ë…/ë³´ì™„ íƒì§€: {n_B} ëª…\")\n","    print(f\"  â€¢ Door-to-Cath (Median): {med_val:.1f} ë¶„\")\n","    print(f\"  â€¢ Inferred(B) ë¹„ìœ¨: {cath_df['inferred'].sum()} / {n_detect} = {cath_df['inferred'].mean():.1%}\")\n","\n","except Exception as e:\n","    print(f\"âŒ ERROR in Cell 5 v2.2: {e}\")\n","    raise\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TqOs8JD8KLe2","executionInfo":{"status":"ok","timestamp":1762484795621,"user_tz":-540,"elapsed":11338,"user":{"displayName":"ì§€ë¯¼ì„œ","userId":"05836704273870504470"}},"outputId":"55b95b72-b252-4ee1-ea1b-72df4ddbfb39"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== Cell 5: Door-to-Cath ì¶”ì¶œ (íŒë‹¨ ê¸°ì¤€ ìˆ˜ì • v2.2) ì‹œì‘ ===\n","procedures_icd ê¸°ë°˜ cath-ê°€ëŠ¥ hadm: 464\n","\n","âœ… Door-to-Cath ê°œì„ ëœ íƒì§€ ê²°ê³¼:\n","  â€¢ Door-to-Cath (A or B): 276 ëª…\n","  â€¢ A(procedureevents) ë‹¨ë… íƒì§€: 65 ëª…\n","  â€¢ B(transfers) ë‹¨ë…/ë³´ì™„ íƒì§€: 264 ëª…\n","  â€¢ Door-to-Cath (Median): 176.5 ë¶„\n","  â€¢ Inferred(B) ë¹„ìœ¨: 211 / 276 = 45.5%\n"]}]},{"cell_type":"code","source":["# === Cell 7: PCI ì‹œìˆ  ì—¬ë¶€ ë³‘í•© ë° ìµœì¢… ì €ì¥ (Door-to-Cath í¬í•¨) ===\n","print(\"\\n=== Cell 7: PCI ì‹œìˆ  ì—¬ë¶€ ë³‘í•© ë° ìµœì¢… ì €ì¥ ì‹œì‘ ===\")\n","\n","try:\n","    conn = get_db_connection(DB_PATH)\n","    print(\"procedures_icd í…Œì´ë¸” ë¡œë“œ ì¤‘...\")\n","\n","    # PCI ê´€ë ¨ ICD ì½”ë“œ\n","    pci_icd9 = [\"0066\", \"3601\", \"3602\", \"3605\", \"3606\", \"3607\", \"3609\"]\n","    pci_icd10_prefix = [\"0270\", \"0271\", \"0272\", \"0273\"]\n","\n","    df_proc = pd.read_sql(\"\"\"\n","        SELECT subject_id, hadm_id, icd_code, icd_version\n","        FROM procedures_icd\n","    \"\"\", conn)\n","    conn.close()\n","\n","    # ì „ì²˜ë¦¬\n","    df_proc[\"icd_code\"] = df_proc[\"icd_code\"].astype(str).str.replace(\".\", \"\", regex=False).str.upper()\n","    df_proc[\"subject_id\"] = df_proc[\"subject_id\"].astype(\"int32\")\n","    df_proc[\"hadm_id\"] = df_proc[\"hadm_id\"].astype(\"Int32\")\n","\n","    # PCI í•„í„°\n","    mask9 = (df_proc[\"icd_version\"]==9) & (df_proc[\"icd_code\"].isin(pci_icd9))\n","    mask10 = (df_proc[\"icd_version\"]==10) & (df_proc[\"icd_code\"].str.startswith(tuple(pci_icd10_prefix)))\n","    pci_df = df_proc[mask9 | mask10][[\"subject_id\",\"hadm_id\"]].drop_duplicates()\n","\n","    # PCI ì—¬ë¶€ í”Œë˜ê·¸ ìƒì„±\n","    pci_df[\"PCI_flag\"] = True\n","\n","    print(f\"PCI ì‹œìˆ  í™˜ì ìˆ˜: {len(pci_df)}\")\n","\n","    # ì½”í˜¸íŠ¸ì— ë³‘í•©\n","    cohort_final_with_pci = cohort_final.merge(\n","        pci_df, on=[\"subject_id\",\"hadm_id\"], how=\"left\"\n","    )\n","    cohort_final_with_pci[\"PCI_flag\"] = cohort_final_with_pci[\"PCI_flag\"].fillna(False)\n","\n","    # ìµœì¢… ì €ì¥\n","    SAVE_PATH_PCI = \"/content/drive/MyDrive/final_cohort_with_timevars_pci.csv\"\n","    cohort_final_with_pci.to_csv(SAVE_PATH_PCI, index=False, encoding=\"utf-8-sig\")\n","\n","    print(f\"\\nâœ… PCI ë³‘í•© ì™„ë£Œ ë° ìµœì¢… ì €ì¥ ì™„ë£Œ\")\n","    print(f\"ì €ì¥ ê²½ë¡œ: {SAVE_PATH_PCI}\")\n","    print(f\"ìµœì¢… Shape: {cohort_final_with_pci.shape}\")\n","    print(f\"PCI=True í™˜ì ìˆ˜: {cohort_final_with_pci['PCI_flag'].sum()}\")\n","\n","except Exception as e:\n","    print(f\"âŒ ERROR in Cell 7: {e}\")\n","    import traceback; traceback.print_exc()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uXcSDJKCK0Bq","executionInfo":{"status":"ok","timestamp":1762484802397,"user_tz":-540,"elapsed":4480,"user":{"displayName":"ì§€ë¯¼ì„œ","userId":"05836704273870504470"}},"outputId":"948e6d7b-a478-4d0b-c531-ff225cc18a83"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== Cell 7: PCI ì‹œìˆ  ì—¬ë¶€ ë³‘í•© ë° ìµœì¢… ì €ì¥ ì‹œì‘ ===\n","procedures_icd í…Œì´ë¸” ë¡œë“œ ì¤‘...\n","PCI ì‹œìˆ  í™˜ì ìˆ˜: 8075\n","\n","âœ… PCI ë³‘í•© ì™„ë£Œ ë° ìµœì¢… ì €ì¥ ì™„ë£Œ\n","ì €ì¥ ê²½ë¡œ: /content/drive/MyDrive/final_cohort_with_timevars_pci.csv\n","ìµœì¢… Shape: (465, 22)\n","PCI=True í™˜ì ìˆ˜: 465\n"]}]},{"cell_type":"code","source":["import os, glob\n","import pandas as pd\n","\n","# -------------------------------\n","# 0) í›„ë³´ ë””ë ‰í„°ë¦¬(ìˆëŠ” ìª½ ìë™ ì‚¬ìš©)\n","# -------------------------------\n","CANDIDATE_DIRS = [\n","    \"/content/drive/MyDrive\", # Colab/Drive\n","    \".\",                      # í˜„ì¬ í´ë”\n","    \"/mnt/data\"               # ì—…ë¡œë“œ í´ë”\n","]\n","\n","pattern = \"final_cohort_with_timevars_pci*.csv\"\n","found = []\n","for d in CANDIDATE_DIRS:\n","    if os.path.isdir(d):\n","        found.extend(glob.glob(os.path.join(d, pattern)))\n","\n","if not found:\n","    raise FileNotFoundError(\"final_cohort_with_timevars_pci*.csv íŒŒì¼ì„ ì–´ëŠ ë””ë ‰í„°ë¦¬ì—ì„œë„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n","\n","# íŒŒì¼ ìˆ˜ì •ì‹œê°„ ê¸°ì¤€ ì •ë ¬(ìµœì‹ ì´ ì•)\n","found = sorted(found, key=lambda p: os.path.getmtime(p), reverse=True)\n","\n","# ìµœì‹ (new)ê³¼ ì§ì „(old) ì„ íƒ\n","path_new = found[0]\n","path_old = found[1] if len(found) > 1 else None\n","\n","print(\"ğŸ” ì°¾ì€ íŒŒì¼(ìµœì‹ â†’ì˜¤ë˜ëœ ìˆœ):\")\n","for p in found:\n","    print(\" -\", p)\n","print(f\"\\nğŸ†• new: {path_new}\")\n","print(f\"ğŸ—‚ï¸ old: {path_old}\\n\")\n","\n","# -------------------------------\n","# 1) ë¡œë“œ\n","# -------------------------------\n","new = pd.read_csv(path_new)\n","old = pd.read_csv(path_old) if path_old else None\n","\n","# -------------------------------\n","# 2) newì—ì„œ door_to_cath_min ë³µêµ¬(ë¹„ì–´ìˆëŠ” ê²½ìš°)\n","#    - _x/_yê°€ ë‚¨ì•„ìˆìœ¼ë©´ í•©ì¹˜ê³ ,\n","#    - ê·¸ë˜ë„ ë¹„ì–´ìˆìœ¼ë©´ oldì—ì„œ ê°’ë§Œ ê°€ì ¸ì™€ ìƒˆë¡œ ì±„ì›€\n","# -------------------------------\n","def combine_xy(df, base_name):\n","    x, y = f\"{base_name}_x\", f\"{base_name}_y\"\n","    if x in df.columns or y in df.columns:\n","        df[base_name] = df.get(x).combine_first(df.get(y))\n","        df.drop(columns=[c for c in [x, y] if c in df.columns], inplace=True, errors=\"ignore\")\n","    return df\n","\n","# 2-1) _x/_y ì •ë¦¬\n","new = combine_xy(new, \"door_to_cath_min\")\n","\n","# 2-2) ì•„ì§ë„ ë¹„ì–´ìˆìœ¼ë©´ oldì—ì„œ ë³µêµ¬\n","needs_restore = (\"door_to_cath_min\" not in new.columns) or new[\"door_to_cath_min\"].isna().all()\n","if needs_restore and old is not None:\n","    # oldì—ì„œë„ _x/_y ì •ë¦¬ í›„ ê°€ì ¸ì˜¤ê¸°\n","    old = combine_xy(old, \"door_to_cath_min\")\n","    if \"door_to_cath_min\" in old.columns:\n","        restore = old[[\"subject_id\",\"hadm_id\",\"door_to_cath_min\"]].copy()\n","        # newì— ë™ì¼ ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ì¼ë‹¨ ì‚­ì œ í›„ merge\n","        new = new.drop(columns=[\"door_to_cath_min\"], errors=\"ignore\")\n","        new = new.merge(restore, on=[\"subject_id\",\"hadm_id\"], how=\"left\")\n","        print(\"ğŸ§© door_to_cath_min ê°’ì„ oldì—ì„œ ë³µêµ¬í–ˆìŠµë‹ˆë‹¤.\")\n","    else:\n","        print(\"âš ï¸ old íŒŒì¼ì—ë„ door_to_cath_minì´ ì—†ì–´ ë³µêµ¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n","\n","# -------------------------------\n","# 3) ë°˜ì˜¬ë¦¼(+ì •ìˆ˜í™”) ì²˜ë¦¬\n","# -------------------------------\n","if \"door_to_cath_min\" in new.columns:\n","    # ìˆ«ì ë³€í™˜ í›„ ë°˜ì˜¬ë¦¼\n","    new[\"door_to_cath_min\"] = pd.to_numeric(new[\"door_to_cath_min\"], errors=\"coerce\")\n","    before_nonnull = new[\"door_to_cath_min\"].notna().sum()\n","    new[\"door_to_cath_min\"] = new[\"door_to_cath_min\"].round(0).astype(\"Int64\")\n","    after_nonnull = new[\"door_to_cath_min\"].notna().sum()\n","    print(f\"âœ… ë°˜ì˜¬ë¦¼/ì •ìˆ˜í™” ì™„ë£Œ (non-null: {before_nonnull} â†’ {after_nonnull})\")\n","else:\n","    print(\"âš ï¸ door_to_cath_min ì»¬ëŸ¼ì´ ìµœì¢…ì ìœ¼ë¡œë„ ì—†ìŠµë‹ˆë‹¤. (ë³µêµ¬ ì‹¤íŒ¨)\")\n","\n","# -------------------------------\n","# 4) ìµœì¢… ì €ì¥(ë®ì–´ì“°ê¸°)\n","# -------------------------------\n","new.to_csv(path_new, index=False, encoding=\"utf-8-sig\")\n","print(f\"ğŸ’¾ ìµœì¢… ë®ì–´ì“°ê¸° ì €ì¥ ì™„ë£Œ: {path_new}\")\n","\n","# ê°„ë‹¨ ê²€ì¦ ì¶œë ¥\n","if \"door_to_cath_min\" in new.columns:\n","    print(\"ë¯¸ë¦¬ë³´ê¸°:\", new[\"door_to_cath_min\"].head(10).tolist())\n","    print(\"non-null count:\", int(new[\"door_to_cath_min\"].notna().sum()))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QT5h7Hm_Q30a","executionInfo":{"status":"ok","timestamp":1762484681500,"user_tz":-540,"elapsed":110,"user":{"displayName":"ì§€ë¯¼ì„œ","userId":"05836704273870504470"}},"outputId":"bd838ef1-c1dd-49d8-9d16-cd920226958f"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ” ì°¾ì€ íŒŒì¼(ìµœì‹ â†’ì˜¤ë˜ëœ ìˆœ):\n"," - /content/drive/MyDrive/final_cohort_with_timevars_pci.csv\n","\n","ğŸ†• new: /content/drive/MyDrive/final_cohort_with_timevars_pci.csv\n","ğŸ—‚ï¸ old: None\n","\n","âœ… ë°˜ì˜¬ë¦¼/ì •ìˆ˜í™” ì™„ë£Œ (non-null: 0 â†’ 0)\n","ğŸ’¾ ìµœì¢… ë®ì–´ì“°ê¸° ì €ì¥ ì™„ë£Œ: /content/drive/MyDrive/final_cohort_with_timevars_pci.csv\n","ë¯¸ë¦¬ë³´ê¸°: [<NA>, <NA>, <NA>, <NA>, <NA>, <NA>, <NA>, <NA>, <NA>, <NA>]\n","non-null count: 0\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","# íŒŒì¼ ê²½ë¡œ\n","file_path = \"/content/drive/MyDrive/final_cohort_with_timevars_pci.csv\"\n","\n","# CSV ë¶ˆëŸ¬ì˜¤ê¸°\n","df = pd.read_csv(file_path)\n","\n","# ìˆ«ì ë³€í™˜ (ì•ˆ ë˜ëŠ” ê°’ì€ NaN)\n","for c in [\"door_to_cath_min_y\", \"door_to_cath_min_x\"]:\n","    if c in df.columns:\n","        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n","\n","# ìœ íš¨ ë²”ìœ„ ê¸°ì¤€ (âˆ’30ë¶„ ~ 4320ë¶„ = 72ì‹œê°„)\n","valid_y = df[\"door_to_cath_min_y\"].between(-30, 4320, inclusive=\"both\") if \"door_to_cath_min_y\" in df.columns else None\n","valid_x = df[\"door_to_cath_min_x\"].between(-30, 4320, inclusive=\"both\") if \"door_to_cath_min_x\" in df.columns else None\n","\n","# Y ìš°ì„ , ì—†ì„ ë•Œë§Œ ì •ìƒ ë²”ìœ„ Xë¡œ ëŒ€ì²´\n","cand_y = df[\"door_to_cath_min_y\"].where(valid_y) if valid_y is not None else pd.Series([pd.NA]*len(df))\n","cand_x = df[\"door_to_cath_min_x\"].where(valid_x) if valid_x is not None else pd.Series([pd.NA]*len(df))\n","\n","df[\"door_to_cath_min\"] = cand_y.combine_first(cand_x)\n","\n","# ë°˜ì˜¬ë¦¼ + ì •ìˆ˜í˜• ë³€í™˜\n","df[\"door_to_cath_min\"] = pd.to_numeric(df[\"door_to_cath_min\"], errors=\"coerce\").round(0).astype(\"Int64\")\n","\n","# ğŸ§¹ ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°\n","drop_cols = [\"door_to_cath_min_x\", \"door_to_cath_min_y\", \"source\", \"inferred\", \"PCI_flag\"]\n","df = df.drop(columns=[c for c in drop_cols if c in df.columns], errors=\"ignore\")\n","\n","# ì €ì¥ (ë®ì–´ì“°ê¸°)\n","df.to_csv(file_path, index=False, encoding=\"utf-8-sig\")\n","\n","# ê²°ê³¼ í™•ì¸\n","n_valid = df[\"door_to_cath_min\"].notna().sum()\n","print(f\"âœ… ì •ë¦¬ ì™„ë£Œ: door_to_cath_min ìœ íš¨ê°’ {n_valid} / {len(df)}\")\n","print(f\"ğŸ§¹ ì œê±°ëœ ì»¬ëŸ¼: {[c for c in drop_cols if c not in df.columns]}\")\n","print(f\"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {file_path}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hmDNlqxrT-IU","executionInfo":{"status":"ok","timestamp":1762485405924,"user_tz":-540,"elapsed":65,"user":{"displayName":"ì§€ë¯¼ì„œ","userId":"05836704273870504470"}},"outputId":"8cd81093-8ff2-46bb-c00c-676648090840"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… ì •ë¦¬ ì™„ë£Œ: door_to_cath_min ìœ íš¨ê°’ 277 / 465\n","ğŸ§¹ ì œê±°ëœ ì»¬ëŸ¼: ['door_to_cath_min_x', 'door_to_cath_min_y', 'source', 'inferred', 'PCI_flag']\n","ğŸ’¾ ì €ì¥ ì™„ë£Œ: /content/drive/MyDrive/final_cohort_with_timevars_pci.csv\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","# 1ï¸âƒ£ ì‹¤ì œ íŒŒì¼ ê²½ë¡œ (Colab Drive ê¸°ì¤€)\n","src_path = \"/content/drive/MyDrive/final_cohort_with_timevars_pci.csv\"\n","\n","# 2ï¸âƒ£ ì €ì¥ íŒŒì¼ëª… (ê°œì„  ë²„ì „)\n","save_path = \"/content/drive/MyDrive/final_cohort_with_timevars_pci_refined.csv\"\n","\n","# 3ï¸âƒ£ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n","df = pd.read_csv(src_path)\n","print(f\"ğŸ“‚ ì›ë³¸ ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ: {df.shape}\")\n","\n","# 4ï¸âƒ£ door_to_cath_min ìˆ«ì ë³€í™˜\n","df[\"door_to_cath_min\"] = pd.to_numeric(df[\"door_to_cath_min\"], errors=\"coerce\")\n","\n","# 5ï¸âƒ£ ì´ìƒì¹˜ ì¡°ê±´ ì •ì˜\n","lower_bound, upper_bound = -30, 4320  # (-30ë¶„ ~ 72ì‹œê°„)\n","\n","# 6ï¸âƒ£ ì´ìƒì¹˜ â†’ NaN ì²˜ë¦¬\n","mask_invalid = (df[\"door_to_cath_min\"] < lower_bound) | (df[\"door_to_cath_min\"] > upper_bound)\n","n_invalid = mask_invalid.sum()\n","df.loc[mask_invalid, \"door_to_cath_min\"] = pd.NA\n","\n","# 7ï¸âƒ£ ë°˜ì˜¬ë¦¼ ë° ì •ìˆ˜í˜• ë³€í™˜\n","df[\"door_to_cath_min\"] = df[\"door_to_cath_min\"].round(0).astype(\"Int64\")\n","\n","# 8ï¸âƒ£ ì €ì¥\n","df.to_csv(save_path, index=False, encoding=\"utf-8-sig\")\n","\n","print(f\"\"\"\n","âœ… ì €ì¥ ì™„ë£Œ: {save_path}\n","- ì œê±°ëœ ì´ìƒì¹˜(âˆ’30ë¶„ ë¯¸ë§Œ or 72ì‹œê°„ ì´ˆê³¼): {n_invalid}ê±´\n","- ìœ íš¨ê°’(non-null): {df['door_to_cath_min'].notna().sum()} / {len(df)}\n","\"\"\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T1uj0HPGVmkE","executionInfo":{"status":"ok","timestamp":1762485950139,"user_tz":-540,"elapsed":193,"user":{"displayName":"ì§€ë¯¼ì„œ","userId":"05836704273870504470"}},"outputId":"960d8f06-1750-434f-e113-557dba6d4913"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ“‚ ì›ë³¸ ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ: (465, 18)\n","\n","âœ… ì €ì¥ ì™„ë£Œ: /content/drive/MyDrive/final_cohort_with_timevars_pci_refined.csv\n","- ì œê±°ëœ ì´ìƒì¹˜(âˆ’30ë¶„ ë¯¸ë§Œ or 72ì‹œê°„ ì´ˆê³¼): 0ê±´\n","- ìœ íš¨ê°’(non-null): 277 / 465\n","\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","# 1ï¸âƒ£ ëŒ€ìƒ íŒŒì¼ ê²½ë¡œ\n","path = \"/content/drive/MyDrive/final_cohort_with_timevars_pci_refined.csv\"\n","\n","# 2ï¸âƒ£ ë‚¨ê¸¸ ì»¬ëŸ¼ (ë‚˜ë¨¸ì§€ëŠ” ìë™ ì œê±°)\n","keep_cols = [\n","    \"subject_id\",\n","    \"hadm_id\",\n","    \"intime\",\n","    \"outtime\",\n","    \"admittime\",\n","    \"lab_tat_min\",\n","    \"door_to_antithrombotic_min\",\n","    \"door_to_cath_min\",\n","    \"boarding_delay_min\"\n","]\n","\n","# 3ï¸âƒ£ íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n","df = pd.read_csv(path)\n","\n","# 4ï¸âƒ£ ì§€ì •ëœ ì»¬ëŸ¼ë§Œ ë‚¨ê¸°ê¸°\n","df = df[[c for c in keep_cols if c in df.columns]]\n","\n","# 5ï¸âƒ£ ì €ì¥ (ë®ì–´ì“°ê¸°)\n","df.to_csv(path, index=False, encoding=\"utf-8-sig\")\n","\n","print(f\"\"\"\n","âœ… ë¶ˆí•„ìš” ì»¬ëŸ¼ ì œê±° ì™„ë£Œ\n","- ë‚¨ì€ ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}\n","- ë‚¨ì€ ì»¬ëŸ¼ ëª©ë¡: {list(df.columns)}\n","ğŸ’¾ ì €ì¥ ì™„ë£Œ: {path}\n","\"\"\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AX8UOmEVcmv-","executionInfo":{"status":"ok","timestamp":1762487713539,"user_tz":-540,"elapsed":73,"user":{"displayName":"ì§€ë¯¼ì„œ","userId":"05836704273870504470"}},"outputId":"cb7729a9-eaf5-4f89-f127-1bdbbc86a359"},"execution_count":60,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","âœ… ë¶ˆí•„ìš” ì»¬ëŸ¼ ì œê±° ì™„ë£Œ\n","- ë‚¨ì€ ì»¬ëŸ¼ ìˆ˜: 9\n","- ë‚¨ì€ ì»¬ëŸ¼ ëª©ë¡: ['subject_id', 'hadm_id', 'intime', 'outtime', 'admittime', 'lab_tat_min', 'door_to_antithrombotic_min', 'door_to_cath_min', 'boarding_delay_min']\n","ğŸ’¾ ì €ì¥ ì™„ë£Œ: /content/drive/MyDrive/final_cohort_with_timevars_pci_refined.csv\n","\n"]}]},{"cell_type":"code","source":["!git config --global user.email \"ji85673@gmail.com\"\n","!git config --global user.name \"ji85673\"\n","\n","%cd /content/drive/MyDrive\n","!git clone https://github.com/sosomeet/DA_STEMI.git\n","%cd DA_STEMI\n","\n","!cp \"/content/drive/MyDrive/timevars_analysis.ipynb\" .\n","!cp \"/content/drive/MyDrive/final_cohort_with_timevars_pci_refined (1).csv\" ./final_cohort_with_timevars_pci_refined.csv\n","\n","!git add .\n","!git commit -m \"Door-to-Cath(PCI) ì¶”ì¶œ ë° ì‹œê°„ ë³€ìˆ˜ ì •ì œ ì½”ë“œ ì¶”ê°€\"\n","!git push origin main\n"],"metadata":{"id":"JHEaJN8wh9vK"},"execution_count":null,"outputs":[]}]}