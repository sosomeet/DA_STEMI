{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9e4c26e",
   "metadata": {},
   "outputs": [
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 370\u001b[0m\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[INFO] LightGBM 기반 PPM 베이스라인 학습 및 평가 완료.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 370\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 333\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;66;03m# 1) 데이터 로딩\u001b[39;00m\n\u001b[1;32m--> 333\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mload_ppm_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPPM_DATA_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;66;03m# 2) hadm_id 기준 Train/Val/Test split\u001b[39;00m\n\u001b[0;32m    336\u001b[0m     df_train, df_val, df_test \u001b[38;5;241m=\u001b[39m split_by_hadm(df)\n",
      "Cell \u001b[1;32mIn[1], line 45\u001b[0m, in \u001b[0;36mload_ppm_dataset\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_ppm_dataset\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[0;32m     29\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m    ver142 PPM prefix–next_event CSV 로딩.\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03m    필수 컬럼:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;124;03m      - full_trace_len\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     required_cols \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     48\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubject_id\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     49\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhadm_id\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_trace_len\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     60\u001b[0m     ]\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m required_cols:\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[0;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m parsers\u001b[38;5;241m.\u001b[39mTextReader(src, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32mparsers.pyx:581\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import lightgbm as lgb\n",
    "\n",
    "# ==============================\n",
    "# 0. 경로 및 기본 설정\n",
    "# ==============================\n",
    "\n",
    "PPM_DATA_PATH = \"./cohort/cohort_ver142_ppm_prefix_next_event.csv\"\n",
    "OUTPUT_DIR = \"./cohort\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "MODEL_NEXT_EVENT_PATH = os.path.join(OUTPUT_DIR, \"lgbm_ver142_next_event.txt\")\n",
    "MODEL_TIME_TO_NEXT_PATH = os.path.join(OUTPUT_DIR, \"lgbm_ver142_time_to_next.txt\")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.15   # 전체 hadm_id 중 15% test\n",
    "VAL_SIZE = 0.15    # 전체 hadm_id 중 15% validation (나머지 70% train)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 1. 데이터 로딩 & 기본 전처리\n",
    "# ==============================\n",
    "\n",
    "def load_ppm_dataset(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ver142 PPM prefix–next_event CSV 로딩.\n",
    "    필수 컬럼:\n",
    "      - subject_id\n",
    "      - hadm_id\n",
    "      - case_id\n",
    "      - prefix_len\n",
    "      - prefix_events_str\n",
    "      - current_event\n",
    "      - current_event_id\n",
    "      - next_event\n",
    "      - next_event_id\n",
    "      - time_since_start_min\n",
    "      - time_to_next_min\n",
    "      - full_trace_len\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    required_cols = [\n",
    "        \"subject_id\",\n",
    "        \"hadm_id\",\n",
    "        \"case_id\",\n",
    "        \"prefix_len\",\n",
    "        \"prefix_events_str\",\n",
    "        \"current_event\",\n",
    "        \"current_event_id\",\n",
    "        \"next_event\",\n",
    "        \"next_event_id\",\n",
    "        \"time_since_start_min\",\n",
    "        \"time_to_next_min\",\n",
    "        \"full_trace_len\",\n",
    "    ]\n",
    "    for c in required_cols:\n",
    "        if c not in df.columns:\n",
    "            raise ValueError(f\"PPM 데이터셋에 '{c}' 컬럼이 없습니다. 현재 컬럼: {list(df.columns)}\")\n",
    "\n",
    "    # 결측/이상치 제거 (기본적인 sanity check)\n",
    "    df = df.dropna(subset=[\"next_event_id\", \"time_since_start_min\", \"time_to_next_min\"])\n",
    "    # 시간 관련 값이 음수인 경우 제거 (이상치)\n",
    "    df = df[df[\"time_since_start_min\"] >= 0]\n",
    "    df = df[df[\"time_to_next_min\"] >= 0]\n",
    "\n",
    "    print(f\"[LOAD] PPM 데이터 로딩 완료: {len(df)} rows, {df['hadm_id'].nunique()} hadm_id\")\n",
    "    print(f\"[LOAD] next_event_id 고유 개수: {df['next_event_id'].nunique()}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 2. hadm_id 기준 Train / Val / Test Split\n",
    "# ==============================\n",
    "\n",
    "def split_by_hadm(df: pd.DataFrame,\n",
    "                  test_size: float = TEST_SIZE,\n",
    "                  val_size: float = VAL_SIZE,\n",
    "                  random_state: int = RANDOM_STATE):\n",
    "    \"\"\"\n",
    "    환자 입원 단위(hadm_id) 기준으로 train/val/test를 나눈다.\n",
    "    같은 hadm_id가 서로 다른 split에 섞이지 않도록 방지.\n",
    "    \"\"\"\n",
    "    unique_hadm = df[\"hadm_id\"].drop_duplicates().values\n",
    "    # 먼저 test 분리\n",
    "    hadm_train_val, hadm_test = train_test_split(\n",
    "        unique_hadm, test_size=test_size, random_state=random_state, shuffle=True\n",
    "    )\n",
    "    # train_val에서 다시 val 분리\n",
    "    val_ratio = val_size / (1.0 - test_size)\n",
    "    hadm_train, hadm_val = train_test_split(\n",
    "        hadm_train_val, test_size=val_ratio, random_state=random_state, shuffle=True\n",
    "    )\n",
    "\n",
    "    def _subset(hadm_ids):\n",
    "        return df[df[\"hadm_id\"].isin(hadm_ids)].copy()\n",
    "\n",
    "    df_train = _subset(hadm_train)\n",
    "    df_val = _subset(hadm_val)\n",
    "    df_test = _subset(hadm_test)\n",
    "\n",
    "    print(f\"[SPLIT] train hadm_id: {len(hadm_train)}, rows: {len(df_train)}\")\n",
    "    print(f\"[SPLIT] val   hadm_id: {len(hadm_val)}, rows: {len(df_val)}\")\n",
    "    print(f\"[SPLIT] test  hadm_id: {len(hadm_test)}, rows: {len(df_test)}\")\n",
    "\n",
    "    return df_train, df_val, df_test\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 3. Feature 구성\n",
    "# ==============================\n",
    "\n",
    "def build_feature_matrices(df_train: pd.DataFrame,\n",
    "                           df_val: pd.DataFrame,\n",
    "                           df_test: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    LightGBM에 넣을 feature matrix와 label 벡터 생성.\n",
    "    - 분류: next_event_id (0-index로 변환)\n",
    "    - 회귀: time_to_next_min\n",
    "    Feature는 기본적으로:\n",
    "      - prefix_len (연속형)\n",
    "      - time_since_start_min (연속형)\n",
    "      - full_trace_len (연속형)\n",
    "      - current_event_id (범주형)\n",
    "      - prefix_events_str (범주형)  ← sequence 자체를 하나의 category로 처리 (baseline)\n",
    "    \"\"\"\n",
    "    # 사용할 feature 컬럼\n",
    "    num_cols = [\"prefix_len\", \"time_since_start_min\", \"full_trace_len\"]\n",
    "    cat_cols = [\"current_event_id\", \"prefix_events_str\"]\n",
    "\n",
    "    # 범주형은 pandas category로 캐스팅\n",
    "    for c in cat_cols:\n",
    "        for df in [df_train, df_val, df_test]:\n",
    "            df[c] = df[c].astype(\"category\")\n",
    "\n",
    "    feature_cols = num_cols + cat_cols\n",
    "\n",
    "    X_train = df_train[feature_cols]\n",
    "    X_val = df_val[feature_cols]\n",
    "    X_test = df_test[feature_cols]\n",
    "\n",
    "    # LightGBM의 multiclass objective는 0 ~ num_class-1 범위의 label을 기대하므로 변환\n",
    "    y_train_cls = df_train[\"next_event_id\"].values - 1\n",
    "    y_val_cls = df_val[\"next_event_id\"].values - 1\n",
    "    y_test_cls = df_test[\"next_event_id\"].values - 1\n",
    "\n",
    "    # 회귀는 그대로 사용\n",
    "    y_train_reg = df_train[\"time_to_next_min\"].values\n",
    "    y_val_reg = df_val[\"time_to_next_min\"].values\n",
    "    y_test_reg = df_test[\"time_to_next_min\"].values\n",
    "\n",
    "    print(f\"[FEATURE] 사용 feature 컬럼: {feature_cols}\")\n",
    "    print(f\"[FEATURE] 분류 target 클래스 수: {len(np.unique(y_train_cls))}\")\n",
    "\n",
    "    return (\n",
    "        X_train, X_val, X_test,\n",
    "        y_train_cls, y_val_cls, y_test_cls,\n",
    "        y_train_reg, y_val_reg, y_test_reg,\n",
    "        num_cols, cat_cols\n",
    "    )\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 4. LightGBM 학습 함수들\n",
    "# ==============================\n",
    "\n",
    "def train_lgbm_classifier(\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    num_cols, cat_cols,\n",
    "    model_path: str\n",
    "):\n",
    "    \"\"\"\n",
    "    next_event_id 분류용 LightGBM 학습.\n",
    "    \"\"\"\n",
    "    num_class = len(np.unique(y_train))\n",
    "\n",
    "    # LightGBM Dataset 생성\n",
    "    train_data = lgb.Dataset(\n",
    "        X_train,\n",
    "        label=y_train,\n",
    "        categorical_feature=cat_cols,\n",
    "        free_raw_data=False\n",
    "    )\n",
    "    val_data = lgb.Dataset(\n",
    "        X_val,\n",
    "        label=y_val,\n",
    "        categorical_feature=cat_cols,\n",
    "        free_raw_data=False\n",
    "    )\n",
    "\n",
    "    params = {\n",
    "        \"objective\": \"multiclass\",\n",
    "        \"num_class\": num_class,\n",
    "        \"metric\": [\"multi_logloss\", \"multi_error\"],\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"num_leaves\": 31,\n",
    "        \"max_depth\": -1,\n",
    "        \"feature_fraction\": 0.9,\n",
    "        \"bagging_fraction\": 0.8,\n",
    "        \"bagging_freq\": 1,\n",
    "        \"min_data_in_leaf\": 30,\n",
    "        \"lambda_l2\": 1.0,\n",
    "        \"verbosity\": -1,\n",
    "        \"force_col_wise\": True,\n",
    "        \"seed\": RANDOM_STATE,\n",
    "    }\n",
    "\n",
    "    print(\"\\n[TRAIN] LightGBM 분류 모델 학습 시작 (next_event_id)...\")\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        valid_sets=[train_data, val_data],\n",
    "        valid_names=[\"train\", \"val\"],\n",
    "        num_boost_round=500,\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=50,\n",
    "    )\n",
    "\n",
    "    print(f\"[TRAIN] 최적 반복 수: {model.best_iteration}\")\n",
    "    model.save_model(model_path)\n",
    "    print(f\"[SAVE] 분류 모델 저장: {model_path}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_lgbm_regressor(\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    num_cols, cat_cols,\n",
    "    model_path: str\n",
    "):\n",
    "    \"\"\"\n",
    "    time_to_next_min 회귀용 LightGBM 학습.\n",
    "    \"\"\"\n",
    "    train_data = lgb.Dataset(\n",
    "        X_train,\n",
    "        label=y_train,\n",
    "        categorical_feature=cat_cols,\n",
    "        free_raw_data=False\n",
    "    )\n",
    "    val_data = lgb.Dataset(\n",
    "        X_val,\n",
    "        label=y_val,\n",
    "        categorical_feature=cat_cols,\n",
    "        free_raw_data=False\n",
    "    )\n",
    "\n",
    "    params = {\n",
    "        \"objective\": \"regression\",\n",
    "        \"metric\": [\"l2\", \"l1\"],\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"num_leaves\": 31,\n",
    "        \"max_depth\": -1,\n",
    "        \"feature_fraction\": 0.9,\n",
    "        \"bagging_fraction\": 0.8,\n",
    "        \"bagging_freq\": 1,\n",
    "        \"min_data_in_leaf\": 30,\n",
    "        \"lambda_l2\": 1.0,\n",
    "        \"verbosity\": -1,\n",
    "        \"force_col_wise\": True,\n",
    "        \"seed\": RANDOM_STATE,\n",
    "    }\n",
    "\n",
    "    print(\"\\n[TRAIN] LightGBM 회귀 모델 학습 시작 (time_to_next_min)...\")\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        valid_sets=[train_data, val_data],\n",
    "        valid_names=[\"train\", \"val\"],\n",
    "        num_boost_round=500,\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=50,\n",
    "    )\n",
    "\n",
    "    print(f\"[TRAIN] 최적 반복 수: {model.best_iteration}\")\n",
    "    model.save_model(model_path)\n",
    "    print(f\"[SAVE] 회귀 모델 저장: {model_path}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 5. 평가 함수\n",
    "# ==============================\n",
    "\n",
    "def eval_classifier(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    분류 모델 평가:\n",
    "      - Top-1 accuracy\n",
    "      - Top-3 accuracy\n",
    "      - 간단한 classification_report\n",
    "    \"\"\"\n",
    "    # 예측 확률\n",
    "    prob = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "    # Top-1\n",
    "    y_pred = np.argmax(prob, axis=1)\n",
    "    acc_top1 = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Top-3 accuracy\n",
    "    top3 = np.argsort(prob, axis=1)[:, -3:]\n",
    "    correct_top3 = np.any(top3 == y_test.reshape(-1, 1), axis=1)\n",
    "    acc_top3 = correct_top3.mean()\n",
    "\n",
    "    print(\"\\n[EVAL][CLASS] Test Top-1 Accuracy :\", acc_top1)\n",
    "    print(\"[EVAL][CLASS] Test Top-3 Accuracy :\", acc_top3)\n",
    "    print(\"\\n[EVAL][CLASS] Classification Report (Top-1 기준):\")\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "\n",
    "def eval_regressor(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    회귀 모델 평가:\n",
    "      - MAE\n",
    "      - RMSE\n",
    "    \"\"\"\n",
    "    pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "    mae = np.mean(np.abs(pred - y_test))\n",
    "    rmse = np.sqrt(np.mean((pred - y_test) ** 2))\n",
    "\n",
    "    print(\"\\n[EVAL][REG] Test MAE (min) :\", mae)\n",
    "    print(\"[EVAL][REG] Test RMSE (min):\", rmse)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 6. MAIN\n",
    "# ==============================\n",
    "\n",
    "def main():\n",
    "    # 1) 데이터 로딩\n",
    "    df = load_ppm_dataset(PPM_DATA_PATH)\n",
    "\n",
    "    # 2) hadm_id 기준 Train/Val/Test split\n",
    "    df_train, df_val, df_test = split_by_hadm(df)\n",
    "\n",
    "    # 3) Feature / Label 구성\n",
    "    (\n",
    "        X_train, X_val, X_test,\n",
    "        y_train_cls, y_val_cls, y_test_cls,\n",
    "        y_train_reg, y_val_reg, y_test_reg,\n",
    "        num_cols, cat_cols\n",
    "    ) = build_feature_matrices(df_train, df_val, df_test)\n",
    "\n",
    "    # 4) 분류 모델 학습 (next_event_id)\n",
    "    cls_model = train_lgbm_classifier(\n",
    "        X_train, y_train_cls,\n",
    "        X_val, y_val_cls,\n",
    "        num_cols, cat_cols,\n",
    "        MODEL_NEXT_EVENT_PATH\n",
    "    )\n",
    "\n",
    "    # 5) 회귀 모델 학습 (time_to_next_min)\n",
    "    reg_model = train_lgbm_regressor(\n",
    "        X_train, y_train_reg,\n",
    "        X_val, y_val_reg,\n",
    "        num_cols, cat_cols,\n",
    "        MODEL_TIME_TO_NEXT_PATH\n",
    "    )\n",
    "\n",
    "    # 6) Test 평가\n",
    "    eval_classifier(cls_model, X_test, y_test_cls)\n",
    "    eval_regressor(reg_model, X_test, y_test_reg)\n",
    "\n",
    "    print(\"\\n[INFO] LightGBM 기반 PPM 베이스라인 학습 및 평가 완료.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
