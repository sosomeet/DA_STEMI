{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "697c4fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[COHORT] 원래 STEMI 입원 후보 row 수: 1929\n",
      "[COHORT] subject_id 기준 '첫 STEMI 입원'만 남긴 row 수: 1878\n",
      "[LOAD] admissions, patients, icustays...\n",
      "[LOAD] edstays (있으면 로딩)...\n",
      "[LOAD] labevents_troponin ...\n",
      "[LOAD] labevents_troponin.csv 로딩: ../../data/MIMIC4-hosp-icu\\labevents_troponin.csv\n",
      "[LOAD] procedures_icd, prescriptions, ECG machine_measurements...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19084\\3676839491.py:142: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  prescriptions = pd.read_csv(os.path.join(HOSP_DIR, \"prescriptions.csv\"))\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19084\\3676839491.py:143: DtypeWarning: Columns (16,17,18,19,20,21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ecg = pd.read_csv(os.path.join(ECG_DIR, \"machine_measurements.csv\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DEBUG] ===== ED / ICU 매칭 상태 점검 =====\n",
      "[DEBUG][ED] edstays.csv 를 찾지 못했습니다. ED 이벤트는 생성되지 않습니다.\n",
      "\n",
      "[DEBUG][ICU] icustays 원본 row 수: 94458\n",
      "[DEBUG][ICU] (subject_id, hadm_id) 기준 merge 후 row 수: 963\n",
      "[DEBUG][ICU] 예시 5개:\n",
      "   subject_id   hadm_id               intime              outtime\n",
      "0    10010058  26359957  2147-11-18 03:19:00  2147-11-19 08:53:33\n",
      "1    10012438  22764825  2178-06-07 21:57:00  2178-06-08 15:51:15\n",
      "2    10013310  27682188  2153-05-06 18:28:00  2153-05-07 20:47:19\n",
      "3    10015860  24698912  2192-05-12 09:31:00  2192-05-13 00:55:45\n",
      "4    10055344  29209451  2171-10-31 19:38:25  2171-11-02 19:46:41\n",
      "[INFO] 최종 STEMI cohort size (첫 입원 기준): 1878\n",
      "[INFO] 기존 전체 Event Log 파일 삭제: ./cohort\\event_log_stemi_all.csv\n",
      "\n",
      "[INFO] Batch 처리중: rows 0 ~ 99 (size=100), version=140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19084\\3676839491.py:613: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_events = pd.concat(\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19084\\3676839491.py:547: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  all_events\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 전체 Event Log에 ED_ARRIVAL 이벤트가 없어 ED 이후 필터링을 건너뜁니다.\n",
      "[INFO]   → 생성된 이벤트 row 수: 1699\n",
      "[INFO]   → chunk 파일 저장: ./cohort\\cohort_ver140.csv\n",
      "[INFO]   → 전체 Event Log에 append: ./cohort\\event_log_stemi_all.csv\n",
      "\n",
      "[INFO] Batch 처리중: rows 100 ~ 199 (size=100), version=141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19084\\3676839491.py:613: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_events = pd.concat(\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19084\\3676839491.py:547: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  all_events\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 전체 Event Log에 ED_ARRIVAL 이벤트가 없어 ED 이후 필터링을 건너뜁니다.\n",
      "[INFO]   → 생성된 이벤트 row 수: 1810\n",
      "[INFO]   → chunk 파일 저장: ./cohort\\cohort_ver141.csv\n",
      "[INFO]   → 전체 Event Log에 append: ./cohort\\event_log_stemi_all.csv\n",
      "\n",
      "[INFO] Batch 처리중: rows 200 ~ 299 (size=100), version=141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19084\\3676839491.py:613: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_events = pd.concat(\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19084\\3676839491.py:547: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  all_events\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 전체 Event Log에 ED_ARRIVAL 이벤트가 없어 ED 이후 필터링을 건너뜁니다.\n",
      "[INFO]   → 생성된 이벤트 row 수: 1591\n",
      "[INFO]   → chunk 파일 저장: ./cohort\\cohort_ver141.csv\n",
      "[INFO]   → 전체 Event Log에 append: ./cohort\\event_log_stemi_all.csv\n",
      "\n",
      "[INFO] Batch 처리중: rows 300 ~ 399 (size=100), version=141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19084\\3676839491.py:613: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_events = pd.concat(\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19084\\3676839491.py:547: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  all_events\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 전체 Event Log에 ED_ARRIVAL 이벤트가 없어 ED 이후 필터링을 건너뜁니다.\n",
      "[INFO]   → 생성된 이벤트 row 수: 1747\n",
      "[INFO]   → chunk 파일 저장: ./cohort\\cohort_ver141.csv\n",
      "[INFO]   → 전체 Event Log에 append: ./cohort\\event_log_stemi_all.csv\n",
      "\n",
      "[INFO] Batch 처리중: rows 400 ~ 499 (size=100), version=141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19084\\3676839491.py:613: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_events = pd.concat(\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19084\\3676839491.py:547: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  all_events\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 전체 Event Log에 ED_ARRIVAL 이벤트가 없어 ED 이후 필터링을 건너뜁니다.\n",
      "[INFO]   → 생성된 이벤트 row 수: 2170\n",
      "[INFO]   → chunk 파일 저장: ./cohort\\cohort_ver141.csv\n",
      "[INFO]   → 전체 Event Log에 append: ./cohort\\event_log_stemi_all.csv\n",
      "\n",
      "[INFO] Batch 처리중: rows 500 ~ 599 (size=100), version=141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19084\\3676839491.py:613: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_events = pd.concat(\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19084\\3676839491.py:547: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  all_events\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 전체 Event Log에 ED_ARRIVAL 이벤트가 없어 ED 이후 필터링을 건너뜁니다.\n",
      "[INFO]   → 생성된 이벤트 row 수: 1788\n",
      "[INFO]   → chunk 파일 저장: ./cohort\\cohort_ver141.csv\n",
      "[INFO]   → 전체 Event Log에 append: ./cohort\\event_log_stemi_all.csv\n",
      "\n",
      "[INFO] Batch 처리중: rows 600 ~ 699 (size=100), version=141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19084\\3676839491.py:613: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_events = pd.concat(\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19084\\3676839491.py:547: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  all_events\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 전체 Event Log에 ED_ARRIVAL 이벤트가 없어 ED 이후 필터링을 건너뜁니다.\n",
      "[INFO]   → 생성된 이벤트 row 수: 2288\n",
      "[INFO]   → chunk 파일 저장: ./cohort\\cohort_ver141.csv\n",
      "[INFO]   → 전체 Event Log에 append: ./cohort\\event_log_stemi_all.csv\n",
      "\n",
      "[INFO] Batch 처리중: rows 700 ~ 799 (size=100), version=141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19084\\3676839491.py:613: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_events = pd.concat(\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19084\\3676839491.py:547: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  all_events\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 전체 Event Log에 ED_ARRIVAL 이벤트가 없어 ED 이후 필터링을 건너뜁니다.\n",
      "[INFO]   → 생성된 이벤트 row 수: 1729\n",
      "[INFO]   → chunk 파일 저장: ./cohort\\cohort_ver141.csv\n",
      "[INFO]   → 전체 Event Log에 append: ./cohort\\event_log_stemi_all.csv\n",
      "\n",
      "[INFO] Batch 처리중: rows 800 ~ 899 (size=100), version=141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19084\\3676839491.py:613: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_events = pd.concat(\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19084\\3676839491.py:547: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  all_events\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 전체 Event Log에 ED_ARRIVAL 이벤트가 없어 ED 이후 필터링을 건너뜁니다.\n",
      "[INFO]   → 생성된 이벤트 row 수: 1967\n",
      "[INFO]   → chunk 파일 저장: ./cohort\\cohort_ver141.csv\n",
      "[INFO]   → 전체 Event Log에 append: ./cohort\\event_log_stemi_all.csv\n",
      "\n",
      "[INFO] Batch 처리중: rows 900 ~ 999 (size=100), version=141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19084\\3676839491.py:613: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_events = pd.concat(\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19084\\3676839491.py:547: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  all_events\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 전체 Event Log에 ED_ARRIVAL 이벤트가 없어 ED 이후 필터링을 건너뜁니다.\n",
      "[INFO]   → 생성된 이벤트 row 수: 1894\n",
      "[INFO]   → chunk 파일 저장: ./cohort\\cohort_ver141.csv\n",
      "[INFO]   → 전체 Event Log에 append: ./cohort\\event_log_stemi_all.csv\n",
      "\n",
      "[INFO] Batch 처리중: rows 1000 ~ 1099 (size=100), version=141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19084\\3676839491.py:613: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_events = pd.concat(\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19084\\3676839491.py:547: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  all_events\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 전체 Event Log에 ED_ARRIVAL 이벤트가 없어 ED 이후 필터링을 건너뜁니다.\n",
      "[INFO]   → 생성된 이벤트 row 수: 1887\n",
      "[INFO]   → chunk 파일 저장: ./cohort\\cohort_ver141.csv\n",
      "[INFO]   → 전체 Event Log에 append: ./cohort\\event_log_stemi_all.csv\n",
      "\n",
      "[INFO] Batch 처리중: rows 1100 ~ 1199 (size=100), version=141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19084\\3676839491.py:613: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_events = pd.concat(\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19084\\3676839491.py:547: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  all_events\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 전체 Event Log에 ED_ARRIVAL 이벤트가 없어 ED 이후 필터링을 건너뜁니다.\n",
      "[INFO]   → 생성된 이벤트 row 수: 2040\n",
      "[INFO]   → chunk 파일 저장: ./cohort\\cohort_ver141.csv\n",
      "[INFO]   → 전체 Event Log에 append: ./cohort\\event_log_stemi_all.csv\n",
      "\n",
      "[INFO] Batch 처리중: rows 1200 ~ 1299 (size=100), version=141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19084\\3676839491.py:613: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_events = pd.concat(\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19084\\3676839491.py:547: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  all_events\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 전체 Event Log에 ED_ARRIVAL 이벤트가 없어 ED 이후 필터링을 건너뜁니다.\n",
      "[INFO]   → 생성된 이벤트 row 수: 1694\n",
      "[INFO]   → chunk 파일 저장: ./cohort\\cohort_ver141.csv\n",
      "[INFO]   → 전체 Event Log에 append: ./cohort\\event_log_stemi_all.csv\n",
      "\n",
      "[INFO] Batch 처리중: rows 1300 ~ 1399 (size=100), version=141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19084\\3676839491.py:613: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_events = pd.concat(\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19084\\3676839491.py:547: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  all_events\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 전체 Event Log에 ED_ARRIVAL 이벤트가 없어 ED 이후 필터링을 건너뜁니다.\n",
      "[INFO]   → 생성된 이벤트 row 수: 1951\n",
      "[INFO]   → chunk 파일 저장: ./cohort\\cohort_ver141.csv\n",
      "[INFO]   → 전체 Event Log에 append: ./cohort\\event_log_stemi_all.csv\n",
      "\n",
      "[INFO] Batch 처리중: rows 1400 ~ 1499 (size=100), version=141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19084\\3676839491.py:613: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_events = pd.concat(\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19084\\3676839491.py:547: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  all_events\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 전체 Event Log에 ED_ARRIVAL 이벤트가 없어 ED 이후 필터링을 건너뜁니다.\n",
      "[INFO]   → 생성된 이벤트 row 수: 1747\n",
      "[INFO]   → chunk 파일 저장: ./cohort\\cohort_ver141.csv\n",
      "[INFO]   → 전체 Event Log에 append: ./cohort\\event_log_stemi_all.csv\n",
      "\n",
      "[INFO] Batch 처리중: rows 1500 ~ 1599 (size=100), version=141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19084\\3676839491.py:613: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_events = pd.concat(\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19084\\3676839491.py:547: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  all_events\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 전체 Event Log에 ED_ARRIVAL 이벤트가 없어 ED 이후 필터링을 건너뜁니다.\n",
      "[INFO]   → 생성된 이벤트 row 수: 1952\n",
      "[INFO]   → chunk 파일 저장: ./cohort\\cohort_ver141.csv\n",
      "[INFO]   → 전체 Event Log에 append: ./cohort\\event_log_stemi_all.csv\n",
      "\n",
      "[INFO] Batch 처리중: rows 1600 ~ 1699 (size=100), version=141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19084\\3676839491.py:613: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_events = pd.concat(\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19084\\3676839491.py:547: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  all_events\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 전체 Event Log에 ED_ARRIVAL 이벤트가 없어 ED 이후 필터링을 건너뜁니다.\n",
      "[INFO]   → 생성된 이벤트 row 수: 1898\n",
      "[INFO]   → chunk 파일 저장: ./cohort\\cohort_ver141.csv\n",
      "[INFO]   → 전체 Event Log에 append: ./cohort\\event_log_stemi_all.csv\n",
      "\n",
      "[INFO] Batch 처리중: rows 1700 ~ 1799 (size=100), version=141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19084\\3676839491.py:613: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_events = pd.concat(\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19084\\3676839491.py:547: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  all_events\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 전체 Event Log에 ED_ARRIVAL 이벤트가 없어 ED 이후 필터링을 건너뜁니다.\n",
      "[INFO]   → 생성된 이벤트 row 수: 1642\n",
      "[INFO]   → chunk 파일 저장: ./cohort\\cohort_ver141.csv\n",
      "[INFO]   → 전체 Event Log에 append: ./cohort\\event_log_stemi_all.csv\n",
      "\n",
      "[INFO] Batch 처리중: rows 1800 ~ 1877 (size=78), version=141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19084\\3676839491.py:613: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_events = pd.concat(\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19084\\3676839491.py:547: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  all_events\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] 전체 Event Log에 ED_ARRIVAL 이벤트가 없어 ED 이후 필터링을 건너뜁니다.\n",
      "[INFO]   → 생성된 이벤트 row 수: 1499\n",
      "[INFO]   → chunk 파일 저장: ./cohort\\cohort_ver141.csv\n",
      "[INFO]   → 전체 Event Log에 append: ./cohort\\event_log_stemi_all.csv\n",
      "\n",
      "[INFO] 모든 batch 처리가 완료되었습니다.\n",
      "[INFO] 최종 전체 Event Log 경로: ./cohort\\event_log_stemi_all.csv\n",
      "[INFO] 마지막 사용된 version 번호: 141\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# ==============================\n",
    "# 0. 경로 및 기본 설정\n",
    "# ==============================\n",
    "\n",
    "COHORT_PATH = \"./cohort/cohort_ver50_only_subject_id.csv\"\n",
    "\n",
    "# MIMIC4-hosp-icu 폴더에 admissions, patients, icustays,\n",
    "# procedures_icd, prescriptions, labevents_troponin 등이 있다고 가정\n",
    "HOSP_DIR = \"../../data/MIMIC4-hosp-icu\"\n",
    "ICU_DIR = \"../../data/MIMIC4-hosp-icu\"\n",
    "ED_DIR = \"../../data/mimic-iv-ed\"\n",
    "ECG_DIR = \"../../data/mimic-iv-ecg\"\n",
    "\n",
    "OUTPUT_DIR = \"./cohort\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# 전체 Event Log를 하나로 합친 파일\n",
    "EVENT_LOG_FULL_PATH = os.path.join(OUTPUT_DIR, \"event_log_stemi_all.csv\")\n",
    "\n",
    "# 중간 저장 chunk 파일 이름 규칙\n",
    "CHUNK_VERSION_START = 140\n",
    "BATCH_SIZE = 100  # 한 번에 처리할 환자 수\n",
    "\n",
    "# Troponin 양성 기준 (예: 0.04 이상 양성)\n",
    "TROP_POS_THRESHOLD = 0.04\n",
    "\n",
    "# PCI ICD 코드 prefix (예시 – 필요시 조정 가능)\n",
    "PCI_ICD9_PREFIXES = [\"00.66\", \"36.0\"]\n",
    "PCI_ICD10_PREFIXES = [\"027\", \"929\"]\n",
    "\n",
    "# 항혈소판제 이름 리스트 (대소문자 무시)\n",
    "ANTI_PLT_DRUGS = [\"aspirin\", \"clopidogrel\", \"ticagrelor\", \"prasugrel\"]\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 1. 공통 유틸\n",
    "# ==============================\n",
    "\n",
    "def _to_datetime(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def _attrs_to_json(attrs: Dict[str, Any]) -> str:\n",
    "    \"\"\"attributes dict를 JSON 문자열로 변환.\"\"\"\n",
    "    clean = {}\n",
    "    for k, v in attrs.items():\n",
    "        if isinstance(v, float) and pd.isna(v):\n",
    "            clean[k] = None\n",
    "        else:\n",
    "            clean[k] = v\n",
    "    return json.dumps(clean, ensure_ascii=False)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 2. Cohort 및 원본 테이블 로딩\n",
    "# ==============================\n",
    "\n",
    "def load_labevents_troponin() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Troponin 전용 labevents_troponin.csv 로딩.\n",
    "    - HOSP_DIR 아래 또는 현재 작업 디렉토리에서 탐색.\n",
    "    \"\"\"\n",
    "    candidates = [\n",
    "        os.path.join(HOSP_DIR, \"labevents_troponin.csv\"),\n",
    "        \"./labevents_troponin.csv\",\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if os.path.exists(p):\n",
    "            print(f\"[LOAD] labevents_troponin.csv 로딩: {p}\")\n",
    "            df = pd.read_csv(p)\n",
    "            return df\n",
    "\n",
    "    raise FileNotFoundError(\n",
    "        \"labevents_troponin.csv 파일을 찾을 수 없습니다.\\n\"\n",
    "        f\"다음 경로 중 하나에 존재해야 합니다.\\n{candidates}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def load_cohort_first_stemi_admission(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    STEMI 대상 환자 코호트 id 목록 로딩.\n",
    "    - 입력 파일에는 동일 subject_id에 여러 hadm_id(STEMI 입원들)가 있을 수 있음.\n",
    "    - admissions.admittime 기준으로 각 subject_id마다 '가장 이른 hadm_id' 하나만 남김.\n",
    "    필수 컬럼: subject_id, hadm_id\n",
    "    \"\"\"\n",
    "    cohort_raw = pd.read_csv(path)\n",
    "\n",
    "    if \"subject_id\" not in cohort_raw.columns:\n",
    "        raise ValueError(f\"cohort 파일에 subject_id 컬럼이 없습니다. 현재 컬럼: {list(cohort_raw.columns)}\")\n",
    "    if \"hadm_id\" not in cohort_raw.columns:\n",
    "        raise ValueError(\n",
    "            \"cohort 파일에 hadm_id 컬럼이 없습니다.\\n\"\n",
    "            \"STEMI 분석은 hadm_id(입원 단위) 기준으로 해야 하므로,\\n\"\n",
    "            \"cohort_ver50_only_subject_id.csv를 subject_id, hadm_id 두 컬럼을 포함하도록 다시 만들어 주세요.\"\n",
    "        )\n",
    "\n",
    "    cohort_all = cohort_raw[[\"subject_id\", \"hadm_id\"]].drop_duplicates()\n",
    "\n",
    "    admissions_path = os.path.join(HOSP_DIR, \"admissions.csv\")\n",
    "    admissions = pd.read_csv(admissions_path)[[\"subject_id\", \"hadm_id\", \"admittime\"]]\n",
    "    admissions = _to_datetime(admissions, [\"admittime\"])\n",
    "\n",
    "    merged = cohort_all.merge(admissions, on=[\"subject_id\", \"hadm_id\"], how=\"left\")\n",
    "    merged = merged.sort_values([\"subject_id\", \"admittime\"])\n",
    "\n",
    "    first_stemi_adm = merged.groupby(\"subject_id\", as_index=False).head(1)\n",
    "    cohort_final = first_stemi_adm[[\"subject_id\", \"hadm_id\"]].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    print(f\"[COHORT] 원래 STEMI 입원 후보 row 수: {len(cohort_all)}\")\n",
    "    print(f\"[COHORT] subject_id 기준 '첫 STEMI 입원'만 남긴 row 수: {len(cohort_final)}\")\n",
    "\n",
    "    return cohort_final\n",
    "\n",
    "\n",
    "def load_source_tables() -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    MIMIC-IV 원본 테이블들을 로딩.\n",
    "    - Troponin은 labevents_troponin.csv만 사용.\n",
    "    \"\"\"\n",
    "    print(\"[LOAD] admissions, patients, icustays...\")\n",
    "    admissions = pd.read_csv(os.path.join(HOSP_DIR, \"admissions.csv\"))\n",
    "    patients = pd.read_csv(os.path.join(HOSP_DIR, \"patients.csv\"))\n",
    "    icustays = pd.read_csv(os.path.join(ICU_DIR, \"icustays.csv\"))\n",
    "\n",
    "    print(\"[LOAD] edstays (있으면 로딩)...\")\n",
    "    edstays_path = os.path.join(ED_DIR, \"edstays.csv\")\n",
    "    edstays = pd.read_csv(edstays_path) if os.path.exists(edstays_path) else None\n",
    "\n",
    "    print(\"[LOAD] labevents_troponin ...\")\n",
    "    labevents_trop = load_labevents_troponin()\n",
    "\n",
    "    print(\"[LOAD] procedures_icd, prescriptions, ECG machine_measurements...\")\n",
    "    procedures_icd = pd.read_csv(os.path.join(HOSP_DIR, \"procedures_icd.csv\"))\n",
    "    prescriptions = pd.read_csv(os.path.join(HOSP_DIR, \"prescriptions.csv\"))\n",
    "    ecg = pd.read_csv(os.path.join(ECG_DIR, \"machine_measurements.csv\"))\n",
    "\n",
    "    return {\n",
    "        \"admissions\": admissions,\n",
    "        \"patients\": patients,\n",
    "        \"icustays\": icustays,\n",
    "        \"edstays\": edstays,\n",
    "        \"labevents_trop\": labevents_trop,\n",
    "        \"procedures_icd\": procedures_icd,\n",
    "        \"prescriptions\": prescriptions,\n",
    "        \"ecg\": ecg,\n",
    "    }\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 2-1. 디버그용 ED/ICU 매칭 상황 확인\n",
    "# ==============================\n",
    "\n",
    "def debug_check_ed_icu(cohort: pd.DataFrame, tables: Dict[str, pd.DataFrame]):\n",
    "    edstays = tables[\"edstays\"]\n",
    "    icustays = tables[\"icustays\"]\n",
    "\n",
    "    print(\"\\n[DEBUG] ===== ED / ICU 매칭 상태 점검 =====\")\n",
    "\n",
    "    # 1) ED\n",
    "    if edstays is None:\n",
    "        print(\"[DEBUG][ED] edstays.csv 를 찾지 못했습니다. ED 이벤트는 생성되지 않습니다.\")\n",
    "    else:\n",
    "        print(f\"[DEBUG][ED] edstays 원본 row 수: {len(edstays)}\")\n",
    "        print(f\"[DEBUG][ED] cohort row 수: {len(cohort)}\")\n",
    "\n",
    "        ed_merge = edstays.merge(cohort, on=[\"subject_id\", \"hadm_id\"], how=\"inner\")\n",
    "        print(f\"[DEBUG][ED] (subject_id, hadm_id) 기준 merge 후 row 수: {len(ed_merge)}\")\n",
    "\n",
    "        if len(ed_merge) > 0:\n",
    "            print(\"[DEBUG][ED] 예시 5개:\")\n",
    "            print(ed_merge[[\"subject_id\", \"hadm_id\", \"intime\", \"outtime\"]].head())\n",
    "        else:\n",
    "            print(\"[DEBUG][ED] 매칭된 ED row가 없습니다.\")\n",
    "\n",
    "    # 2) ICU\n",
    "    icu = icustays\n",
    "    print(f\"\\n[DEBUG][ICU] icustays 원본 row 수: {len(icu)}\")\n",
    "    icu_merge = icu.merge(cohort, on=[\"subject_id\", \"hadm_id\"], how=\"inner\")\n",
    "    print(f\"[DEBUG][ICU] (subject_id, hadm_id) 기준 merge 후 row 수: {len(icu_merge)}\")\n",
    "\n",
    "    if len(icu_merge) > 0:\n",
    "        print(\"[DEBUG][ICU] 예시 5개:\")\n",
    "        print(icu_merge[[\"subject_id\", \"hadm_id\", \"intime\", \"outtime\"]].head())\n",
    "    else:\n",
    "        print(\"[DEBUG][ICU] 매칭된 ICU row가 없습니다.\")\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 3. Event 생성 함수들\n",
    "# ==============================\n",
    "\n",
    "def build_ed_events(cohort: pd.DataFrame, edstays: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ED_ARRIVAL, ED_DEPARTURE 이벤트 생성.\n",
    "    edstays: subject_id, hadm_id, intime, outtime 가정.\n",
    "    \"\"\"\n",
    "    if edstays is None:\n",
    "        return pd.DataFrame(columns=[\"subject_id\", \"hadm_id\", \"event_name\", \"timestamp\", \"attributes\"])\n",
    "\n",
    "    ed = edstays.merge(cohort, on=[\"subject_id\", \"hadm_id\"], how=\"inner\")\n",
    "    ed = _to_datetime(ed, [\"intime\", \"outtime\"])\n",
    "\n",
    "    events = []\n",
    "    for _, row in ed.iterrows():\n",
    "        if pd.notnull(row.get(\"intime\")):\n",
    "            events.append({\n",
    "                \"subject_id\": row[\"subject_id\"],\n",
    "                \"hadm_id\": row[\"hadm_id\"],\n",
    "                \"event_name\": \"ED_ARRIVAL\",\n",
    "                \"timestamp\": row[\"intime\"],\n",
    "                \"attributes\": _attrs_to_json({}),\n",
    "            })\n",
    "        if pd.notnull(row.get(\"outtime\")):\n",
    "            events.append({\n",
    "                \"subject_id\": row[\"subject_id\"],\n",
    "                \"hadm_id\": row[\"hadm_id\"],\n",
    "                \"event_name\": \"ED_DEPARTURE\",\n",
    "                \"timestamp\": row[\"outtime\"],\n",
    "                \"attributes\": _attrs_to_json({}),\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(events)\n",
    "\n",
    "\n",
    "def build_admission_events(cohort: pd.DataFrame,\n",
    "                           admissions: pd.DataFrame,\n",
    "                           patients: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    DISCHARGE, DEATH 이벤트 생성.\n",
    "    DISCHARGE: admissions.dischtime\n",
    "    DEATH: patients.dod\n",
    "    \"\"\"\n",
    "    adm = admissions.merge(cohort, on=[\"subject_id\", \"hadm_id\"], how=\"inner\")\n",
    "    adm = _to_datetime(adm, [\"admittime\", \"dischtime\"])\n",
    "    pat = _to_datetime(patients.copy(), [\"dod\"])\n",
    "\n",
    "    adm = adm.merge(pat[[\"subject_id\", \"dod\"]], on=\"subject_id\", how=\"left\")\n",
    "\n",
    "    events = []\n",
    "    for _, row in adm.iterrows():\n",
    "        if pd.notnull(row.get(\"dischtime\")):\n",
    "            events.append({\n",
    "                \"subject_id\": row[\"subject_id\"],\n",
    "                \"hadm_id\": row[\"hadm_id\"],\n",
    "                \"event_name\": \"DISCHARGE\",\n",
    "                \"timestamp\": row[\"dischtime\"],\n",
    "                \"attributes\": _attrs_to_json({}),\n",
    "            })\n",
    "        if pd.notnull(row.get(\"dod\")):\n",
    "            events.append({\n",
    "                \"subject_id\": row[\"subject_id\"],\n",
    "                \"hadm_id\": row[\"hadm_id\"],\n",
    "                \"event_name\": \"DEATH\",\n",
    "                \"timestamp\": row[\"dod\"],\n",
    "                \"attributes\": _attrs_to_json({}),\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(events)\n",
    "\n",
    "\n",
    "def build_icu_events(cohort: pd.DataFrame, icustays: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ICU_INTIME, ICU_OUTTIME 이벤트 생성.\n",
    "    icustays: subject_id, hadm_id, intime, outtime, first_careunit, last_careunit, stay_id 가정.\n",
    "    \"\"\"\n",
    "    icu = icustays.merge(cohort, on=[\"subject_id\", \"hadm_id\"], how=\"inner\")\n",
    "    icu = _to_datetime(icu, [\"intime\", \"outtime\"])\n",
    "\n",
    "    events = []\n",
    "    for _, row in icu.iterrows():\n",
    "        attrs = {\n",
    "            \"first_careunit\": row.get(\"first_careunit\", None),\n",
    "            \"last_careunit\": row.get(\"last_careunit\", None),\n",
    "            \"stay_id\": row.get(\"stay_id\", None),\n",
    "        }\n",
    "        if pd.notnull(row.get(\"intime\")):\n",
    "            events.append({\n",
    "                \"subject_id\": row[\"subject_id\"],\n",
    "                \"hadm_id\": row[\"hadm_id\"],\n",
    "                \"event_name\": \"ICU_INTIME\",\n",
    "                \"timestamp\": row[\"intime\"],\n",
    "                \"attributes\": _attrs_to_json(attrs),\n",
    "            })\n",
    "        if pd.notnull(row.get(\"outtime\")):\n",
    "            events.append({\n",
    "                \"subject_id\": row[\"subject_id\"],\n",
    "                \"hadm_id\": row[\"hadm_id\"],\n",
    "                \"event_name\": \"ICU_OUTTIME\",\n",
    "                \"timestamp\": row[\"outtime\"],\n",
    "                \"attributes\": _attrs_to_json(attrs),\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(events)\n",
    "\n",
    "\n",
    "def build_troponin_events(cohort: pd.DataFrame,\n",
    "                          labevents_trop: pd.DataFrame,\n",
    "                          positive_threshold: float) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    TROP_TAKEN, TROP_POSITIVE 이벤트 생성.\n",
    "    labevents_trop: Troponin만 필터된 labevents_troponin.csv\n",
    "      (subject_id, hadm_id, itemid, charttime, valuenum 등 포함)\n",
    "    \"\"\"\n",
    "    lab = labevents_trop.merge(cohort, on=[\"subject_id\", \"hadm_id\"], how=\"inner\")\n",
    "    lab = _to_datetime(lab, [\"charttime\"])\n",
    "\n",
    "    events = []\n",
    "\n",
    "    # TROP_TAKEN\n",
    "    for _, row in lab.iterrows():\n",
    "        if pd.notnull(row.get(\"charttime\")):\n",
    "            attrs = {\n",
    "                \"itemid\": row.get(\"itemid\", None),\n",
    "                \"valuenum\": row.get(\"valuenum\", None),\n",
    "                \"value\": row.get(\"value\", None),\n",
    "                \"flag\": row.get(\"flag\", None),\n",
    "            }\n",
    "            events.append({\n",
    "                \"subject_id\": row[\"subject_id\"],\n",
    "                \"hadm_id\": row[\"hadm_id\"],\n",
    "                \"event_name\": \"TROP_TAKEN\",\n",
    "                \"timestamp\": row[\"charttime\"],\n",
    "                \"attributes\": _attrs_to_json(attrs),\n",
    "            })\n",
    "\n",
    "    # TROP_POSITIVE: hadm_id 기준 첫 양성 시점\n",
    "    if \"valuenum\" in lab.columns:\n",
    "        lab_pos = lab[lab[\"valuenum\"] >= positive_threshold].copy()\n",
    "        lab_pos = lab_pos.dropna(subset=[\"charttime\"])\n",
    "        lab_pos_sorted = lab_pos.sort_values([\"subject_id\", \"hadm_id\", \"charttime\"])\n",
    "        first_pos = lab_pos_sorted.groupby([\"subject_id\", \"hadm_id\"], as_index=False).head(1)\n",
    "\n",
    "        for _, row in first_pos.iterrows():\n",
    "            attrs = {\n",
    "                \"itemid\": row.get(\"itemid\", None),\n",
    "                \"valuenum\": row.get(\"valuenum\", None),\n",
    "                \"value\": row.get(\"value\", None),\n",
    "                \"flag\": row.get(\"flag\", None),\n",
    "            }\n",
    "            events.append({\n",
    "                \"subject_id\": row[\"subject_id\"],\n",
    "                \"hadm_id\": row[\"hadm_id\"],\n",
    "                \"event_name\": \"TROP_POSITIVE\",\n",
    "                \"timestamp\": row[\"charttime\"],\n",
    "                \"attributes\": _attrs_to_json(attrs),\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(events)\n",
    "\n",
    "\n",
    "def build_ecg_events(cohort: pd.DataFrame, ecg: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ECG_TAKEN, ECG_STEMI_FLAG 이벤트 생성\n",
    "\n",
    "    - MIMIC-IV-ECG는 보통 hadm_id가 없고 subject_id만 있음.\n",
    "    - 우리는 cohort를 '각 subject당 첫 STEMI 입원(hadm_id 하나)'로 만들었으므로,\n",
    "      subject_id로만 join해서 해당 subject의 hadm_id를 붙인다.\n",
    "    - 시간 컬럼은 ecg_time 또는 charttime 중 존재하는 것을 사용.\n",
    "    \"\"\"\n",
    "\n",
    "    ecg_c = ecg.copy()\n",
    "\n",
    "    # 시간 컬럼 통일: ecg_time -> charttime\n",
    "    if \"charttime\" not in ecg_c.columns:\n",
    "        if \"ecg_time\" in ecg_c.columns:\n",
    "            ecg_c = ecg_c.rename(columns={\"ecg_time\": \"charttime\"})\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"ECG 데이터에 charttime/ecg_time 둘 다 없습니다. \"\n",
    "                \"시간 컬럼 이름을 확인해 주세요.\"\n",
    "            )\n",
    "\n",
    "    # cohort에서 subject_id당 hadm_id 하나만 남겨 join용 매핑 생성\n",
    "    subj_hadm_map = cohort[[\"subject_id\", \"hadm_id\"]].drop_duplicates()\n",
    "\n",
    "    # subject_id 기준으로 join\n",
    "    ecg_c = ecg_c.merge(subj_hadm_map, on=\"subject_id\", how=\"inner\")\n",
    "\n",
    "    # datetime 변환\n",
    "    ecg_c = _to_datetime(ecg_c, [\"charttime\"])\n",
    "\n",
    "    # STEMI flag 판정 (machine_measurements + report_0~)\n",
    "    def has_stemi_flag(row) -> bool:\n",
    "        mm = str(row.get(\"machine_measurements\", \"\")).upper()\n",
    "        reports = []\n",
    "        for i in range(30):\n",
    "            col = f\"report_{i}\"\n",
    "            if col in row.index:\n",
    "                reports.append(str(row.get(col, \"\")))\n",
    "        rep_text = \" \".join(reports).upper()\n",
    "        text = mm + \" \" + rep_text\n",
    "\n",
    "        if \"STEMI\" in text:\n",
    "            return True\n",
    "        if \"ST ELEVATION\" in text:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    ecg_c[\"is_stemi\"] = ecg_c.apply(has_stemi_flag, axis=1)\n",
    "\n",
    "    # 이벤트 생성\n",
    "    events = []\n",
    "    for _, row in ecg_c.iterrows():\n",
    "        ts = row.get(\"charttime\")\n",
    "        if pd.isna(ts):\n",
    "            continue\n",
    "\n",
    "        # ECG_TAKEN\n",
    "        events.append({\n",
    "            \"subject_id\": row[\"subject_id\"],\n",
    "            \"hadm_id\": row[\"hadm_id\"],   # cohort에서 붙인 첫 STEMI 입원\n",
    "            \"event_name\": \"ECG_TAKEN\",\n",
    "            \"timestamp\": ts,\n",
    "            \"attributes\": _attrs_to_json({}),\n",
    "        })\n",
    "\n",
    "        # ECG_STEMI_FLAG\n",
    "        if row[\"is_stemi\"]:\n",
    "            events.append({\n",
    "                \"subject_id\": row[\"subject_id\"],\n",
    "                \"hadm_id\": row[\"hadm_id\"],\n",
    "                \"event_name\": \"ECG_STEMI_FLAG\",\n",
    "                \"timestamp\": ts,\n",
    "                \"attributes\": _attrs_to_json({}),\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(events)\n",
    "\n",
    "\n",
    "def build_pci_events(cohort: pd.DataFrame,\n",
    "                     procedures_icd: pd.DataFrame,\n",
    "                     pci_icd9_prefixes: List[str],\n",
    "                     pci_icd10_prefixes: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    PCI_START 이벤트 생성 (날짜 단위).\n",
    "    procedures_icd: subject_id, hadm_id, icd_code, icd_version, chartdate 가정.\n",
    "    \"\"\"\n",
    "    proc = procedures_icd.merge(cohort, on=[\"subject_id\", \"hadm_id\"], how=\"inner\")\n",
    "    proc = _to_datetime(proc, [\"chartdate\"])\n",
    "\n",
    "    def is_pci(code: str, version: float) -> bool:\n",
    "        if pd.isna(code):\n",
    "            return False\n",
    "        code_str = str(code)\n",
    "        if version == 9:\n",
    "            return any(code_str.startswith(p) for p in pci_icd9_prefixes)\n",
    "        elif version == 10:\n",
    "            return any(code_str.startswith(p) for p in pci_icd10_prefixes)\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    proc[\"is_pci\"] = proc.apply(\n",
    "        lambda r: is_pci(r.get(\"icd_code\", None), r.get(\"icd_version\", None)), axis=1\n",
    "    )\n",
    "    pci_rows = proc[proc[\"is_pci\"]].copy()\n",
    "\n",
    "    events = []\n",
    "    for _, row in pci_rows.iterrows():\n",
    "        if pd.notnull(row.get(\"chartdate\")):\n",
    "            attrs = {\n",
    "                \"icd_code\": row.get(\"icd_code\", None),\n",
    "                \"icd_version\": row.get(\"icd_version\", None),\n",
    "            }\n",
    "            events.append({\n",
    "                \"subject_id\": row[\"subject_id\"],\n",
    "                \"hadm_id\": row[\"hadm_id\"],\n",
    "                \"event_name\": \"PCI_START\",\n",
    "                \"timestamp\": row[\"chartdate\"],\n",
    "                \"attributes\": _attrs_to_json(attrs),\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(events)\n",
    "\n",
    "\n",
    "def build_antiplatelet_events(cohort: pd.DataFrame,\n",
    "                              prescriptions: pd.DataFrame,\n",
    "                              drug_name_list: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ANTI_PLT_ORDER, ANTI_PLT_ADMIN 이벤트 생성.\n",
    "    prescriptions: subject_id, hadm_id, drug, starttime, stoptime,\n",
    "                   route, dose_val_rx, dose_unit_rx 가정.\n",
    "    \"\"\"\n",
    "    rx = prescriptions.merge(cohort, on=[\"subject_id\", \"hadm_id\"], how=\"inner\")\n",
    "    rx = _to_datetime(rx, [\"starttime\", \"stoptime\"])\n",
    "\n",
    "    drug_lower_list = [d.lower() for d in drug_name_list]\n",
    "    rx[\"drug_lower\"] = rx[\"drug\"].astype(str).str.lower()\n",
    "    rx = rx[rx[\"drug_lower\"].isin(drug_lower_list)]\n",
    "\n",
    "    events = []\n",
    "    for _, row in rx.iterrows():\n",
    "        if pd.notnull(row.get(\"starttime\")):\n",
    "            attrs = {\n",
    "                \"drug\": row.get(\"drug\", None),\n",
    "                \"route\": row.get(\"route\", None),\n",
    "                \"dose_val_rx\": row.get(\"dose_val_rx\", None),\n",
    "                \"dose_unit_rx\": row.get(\"dose_unit_rx\", None),\n",
    "            }\n",
    "            # ORDER\n",
    "            events.append({\n",
    "                \"subject_id\": row[\"subject_id\"],\n",
    "                \"hadm_id\": row[\"hadm_id\"],\n",
    "                \"event_name\": \"ANTI_PLT_ORDER\",\n",
    "                \"timestamp\": row[\"starttime\"],\n",
    "                \"attributes\": _attrs_to_json(attrs),\n",
    "            })\n",
    "            # ADMIN (지금은 starttime과 동일 시점으로 처리)\n",
    "            events.append({\n",
    "                \"subject_id\": row[\"subject_id\"],\n",
    "                \"hadm_id\": row[\"hadm_id\"],\n",
    "                \"event_name\": \"ANTI_PLT_ADMIN\",\n",
    "                \"timestamp\": row[\"starttime\"],\n",
    "                \"attributes\": _attrs_to_json(attrs),\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(events)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 4. 후처리: DISCHARGE/DEATH 이후 제거 + ED_ARRIVAL 이후만 유지\n",
    "# ==============================\n",
    "\n",
    "def trim_after_discharge_or_death(all_events: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    한 환자(hadm_id)에 대해 DISCHARGE 또는 DEATH 이후의 이벤트는 모두 제거.\n",
    "    - 둘 다 있으면 더 이른 시점을 기준으로 잘라냄.\n",
    "    \"\"\"\n",
    "    if all_events.empty:\n",
    "        return all_events\n",
    "\n",
    "    def _trim_one_stay(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        end_mask = df[\"event_name\"].isin([\"DISCHARGE\", \"DEATH\"])\n",
    "        if not end_mask.any():\n",
    "            return df\n",
    "        end_time = df.loc[end_mask, \"timestamp\"].min()\n",
    "        return df[df[\"timestamp\"] <= end_time]\n",
    "\n",
    "    return (\n",
    "        all_events\n",
    "        .groupby(\"hadm_id\", group_keys=False)\n",
    "        .apply(_trim_one_stay)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def filter_events_after_ed(all_events: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    한 환자(hadm_id)에 대해 ED_ARRIVAL 이후의 이벤트만 남긴다.\n",
    "    - ED_ARRIVAL이 여러 개면 가장 이른 ED_ARRIVAL 기준으로 자른다.\n",
    "    - ED_ARRIVAL이 전혀 없는 hadm_id는 통째로 제거.\n",
    "    \"\"\"\n",
    "    if all_events.empty:\n",
    "        return all_events\n",
    "\n",
    "    def _keep_from_first_ed(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        mask_ed = (df[\"event_name\"] == \"ED_ARRIVAL\")\n",
    "        if not mask_ed.any():\n",
    "            # ED 없는 입원은 제거\n",
    "            return pd.DataFrame(columns=df.columns)\n",
    "        start_time = df.loc[mask_ed, \"timestamp\"].min()\n",
    "        return df[df[\"timestamp\"] >= start_time]\n",
    "\n",
    "    return (\n",
    "        all_events\n",
    "        .groupby(\"hadm_id\", group_keys=False)\n",
    "        .apply(_keep_from_first_ed)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 5. Event Log 통합 함수\n",
    "# ==============================\n",
    "\n",
    "def build_event_log(cohort: pd.DataFrame, tables: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    주어진 cohort subset(subject_id, hadm_id)에 대해 Event Log 생성.\n",
    "    최종 컬럼:\n",
    "      - case_id\n",
    "      - subject_id\n",
    "      - hadm_id\n",
    "      - event_name\n",
    "      - timestamp\n",
    "      - attributes(JSON 문자열)\n",
    "    \"\"\"\n",
    "    admissions = tables[\"admissions\"]\n",
    "    patients = tables[\"patients\"]\n",
    "    icustays = tables[\"icustays\"]\n",
    "    edstays = tables[\"edstays\"]\n",
    "    labevents_trop = tables[\"labevents_trop\"]\n",
    "    procedures_icd = tables[\"procedures_icd\"]\n",
    "    prescriptions = tables[\"prescriptions\"]\n",
    "    ecg = tables[\"ecg\"]\n",
    "\n",
    "    ed_events = build_ed_events(cohort, edstays)\n",
    "    adm_events = build_admission_events(cohort, admissions, patients)\n",
    "    icu_events = build_icu_events(cohort, icustays)\n",
    "    trop_events = build_troponin_events(cohort, labevents_trop, TROP_POS_THRESHOLD)\n",
    "    ecg_events = build_ecg_events(cohort, ecg)\n",
    "    pci_events = build_pci_events(cohort, procedures_icd,\n",
    "                                  pci_icd9_prefixes=PCI_ICD9_PREFIXES,\n",
    "                                  pci_icd10_prefixes=PCI_ICD10_PREFIXES)\n",
    "    antiplatelet_events = build_antiplatelet_events(cohort, prescriptions, ANTI_PLT_DRUGS)\n",
    "\n",
    "    all_events = pd.concat(\n",
    "        [\n",
    "            ed_events,\n",
    "            adm_events,\n",
    "            icu_events,\n",
    "            trop_events,\n",
    "            ecg_events,\n",
    "            pci_events,\n",
    "            antiplatelet_events,\n",
    "        ],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    if all_events.empty:\n",
    "        return pd.DataFrame(columns=[\"case_id\", \"subject_id\", \"hadm_id\", \"event_name\", \"timestamp\", \"attributes\"])\n",
    "\n",
    "    # timestamp 정리\n",
    "    all_events = all_events.dropna(subset=[\"timestamp\"])\n",
    "    all_events[\"timestamp\"] = pd.to_datetime(all_events[\"timestamp\"], errors=\"coerce\")\n",
    "    all_events = all_events.dropna(subset=[\"timestamp\"])\n",
    "\n",
    "    # 우선 시간/이름 기준 정렬\n",
    "    all_events = all_events.sort_values(\n",
    "        by=[\"subject_id\", \"hadm_id\", \"timestamp\", \"event_name\"]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # 1) DISCHARGE / DEATH 이후 이벤트 제거\n",
    "    all_events = trim_after_discharge_or_death(all_events)\n",
    "\n",
    "    # 2) ED_ARRIVAL 이후 이벤트만 유지 (ED가 하나라도 있을 때만 적용)\n",
    "    if (all_events[\"event_name\"] == \"ED_ARRIVAL\").any():\n",
    "        all_events = filter_events_after_ed(all_events)\n",
    "    else:\n",
    "        print(\"[WARN] 전체 Event Log에 ED_ARRIVAL 이벤트가 없어 ED 이후 필터링을 건너뜁니다.\")\n",
    "\n",
    "    if all_events.empty:\n",
    "        return pd.DataFrame(columns=[\"case_id\", \"subject_id\", \"hadm_id\", \"event_name\", \"timestamp\", \"attributes\"])\n",
    "\n",
    "    # case_id = hadm_id\n",
    "    all_events[\"case_id\"] = all_events[\"hadm_id\"]\n",
    "\n",
    "    # 최종 컬럼 정리\n",
    "    all_events = all_events[\n",
    "        [\"case_id\", \"subject_id\", \"hadm_id\", \"event_name\", \"timestamp\", \"attributes\"]\n",
    "    ]\n",
    "\n",
    "    return all_events\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 6. batch + versioning MAIN\n",
    "# ==============================\n",
    "\n",
    "def main():\n",
    "    # 1) 코호트 로딩 (각 subject당 첫 STEMI 입원만 남김)\n",
    "    cohort = load_cohort_first_stemi_admission(COHORT_PATH)\n",
    "    tables = load_source_tables()\n",
    "\n",
    "    # 디버그용 ED/ICU 매칭 상태 확인\n",
    "    debug_check_ed_icu(cohort, tables)\n",
    "\n",
    "    print(f\"[INFO] 최종 STEMI cohort size (첫 입원 기준): {len(cohort)}\")\n",
    "\n",
    "    # 기존 전체 Event Log 파일이 있으면 삭제\n",
    "    if os.path.exists(EVENT_LOG_FULL_PATH):\n",
    "        os.remove(EVENT_LOG_FULL_PATH)\n",
    "        print(f\"[INFO] 기존 전체 Event Log 파일 삭제: {EVENT_LOG_FULL_PATH}\")\n",
    "\n",
    "    header_written_full = False\n",
    "    version = CHUNK_VERSION_START\n",
    "\n",
    "    n = len(cohort)\n",
    "    for start in range(0, n, BATCH_SIZE):\n",
    "        end = min(start + BATCH_SIZE, n)\n",
    "        sub_cohort = cohort.iloc[start:end].copy()\n",
    "\n",
    "        print(f\"\\n[INFO] Batch 처리중: rows {start} ~ {end-1} \"\n",
    "              f\"(size={len(sub_cohort)}), version={version}\")\n",
    "\n",
    "        event_log_chunk = build_event_log(sub_cohort, tables)\n",
    "        print(f\"[INFO]   → 생성된 이벤트 row 수: {len(event_log_chunk)}\")\n",
    "\n",
    "        # (1) batch 전용 파일 저장: cohort_ver{version}.csv\n",
    "        chunk_path = os.path.join(OUTPUT_DIR, f\"cohort_ver{version}.csv\")\n",
    "        event_log_chunk.to_csv(chunk_path, index=False)\n",
    "        print(f\"[INFO]   → chunk 파일 저장: {chunk_path}\")\n",
    "\n",
    "        # (2) 전체 Event Log 파일에 append\n",
    "        if len(event_log_chunk) > 0:\n",
    "            mode = \"a\"\n",
    "            header = not header_written_full\n",
    "            event_log_chunk.to_csv(EVENT_LOG_FULL_PATH,\n",
    "                                   mode=mode,\n",
    "                                   header=header,\n",
    "                                   index=False)\n",
    "            header_written_full = True\n",
    "            print(f\"[INFO]   → 전체 Event Log에 append: {EVENT_LOG_FULL_PATH}\")\n",
    "        else:\n",
    "            print(\"[WARN]   → 이 batch에서는 생성된 이벤트가 없습니다.\")\n",
    "\n",
    "        version = 141\n",
    "\n",
    "    print(\"\\n[INFO] 모든 batch 처리가 완료되었습니다.\")\n",
    "    print(f\"[INFO] 최종 전체 Event Log 경로: {EVENT_LOG_FULL_PATH}\")\n",
    "    print(f\"[INFO] 마지막 사용된 version 번호: {141}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2444b4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
