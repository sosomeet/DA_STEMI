{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697c4fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[COHORT] 입력 cohort row 수: 1930\n",
      "[COHORT] 입력 cohort 고유 subject 수: 1878\n",
      "[COHORT] 입력 cohort 고유 hadm 수: 1929\n",
      "[COHORT] (subject_id, hadm_id) 중복 제거 후 row 수: 1929\n",
      "[COHORT] 최종 고유 subject 수: 1878\n",
      "[COHORT] 최종 고유 hadm 수: 1929\n",
      "[LOAD] admissions, patients, icustays...\n",
      "[LOAD] edstays (있으면 로딩)...\n",
      "[LOAD] labevents_troponin ...\n",
      "[LOAD] labevents_troponin.csv 로딩: ../../data/MIMIC4-hosp-icu\\labevents_troponin.csv\n",
      "[LOAD] procedures_icd, prescriptions, ECG machine_measurements...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_26072\\2319555427.py:138: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  prescriptions = pd.read_csv(os.path.join(HOSP_DIR, \"prescriptions.csv\"))\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_26072\\2319555427.py:139: DtypeWarning: Columns (16,17,18,19,20,21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ecg = pd.read_csv(os.path.join(ECG_DIR, \"machine_measurements.csv\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DEBUG] ===== ED / ICU 매칭 상태 점검 =====\n",
      "[DEBUG][ED] edstays 원본 row 수: 425087\n",
      "[DEBUG][ED] cohort row 수: 1929\n",
      "[DEBUG][ED] (subject_id, hadm_id) 기준 merge 후 row 수: 1933\n",
      "[DEBUG][ED] 예시 5개:\n",
      "   subject_id     hadm_id               intime              outtime\n",
      "0    10000764  27897940.0  2132-10-14 19:31:00  2132-10-14 23:32:59\n",
      "1    10010058  26359957.0  2147-11-18 00:50:00  2147-11-18 03:19:00\n",
      "2    10012438  22764825.0  2178-06-07 19:33:00  2178-06-07 21:57:00\n",
      "3    10013310  21243435.0  2153-05-26 08:56:00  2153-05-26 14:18:39\n",
      "4    10013310  27682188.0  2153-05-06 10:21:00  2153-05-06 18:28:00\n",
      "\n",
      "[DEBUG][ICU] icustays 원본 row 수: 94458\n",
      "[DEBUG][ICU] (subject_id, hadm_id) 기준 merge 후 row 수: 983\n",
      "[DEBUG][ICU] 예시 5개:\n",
      "   subject_id   hadm_id               intime              outtime\n",
      "0    10010058  26359957  2147-11-18 03:19:00  2147-11-19 08:53:33\n",
      "1    10012438  22764825  2178-06-07 21:57:00  2178-06-08 15:51:15\n",
      "2    10013310  27682188  2153-05-06 18:28:00  2153-05-07 20:47:19\n",
      "3    10015860  24698912  2192-05-12 09:31:00  2192-05-13 00:55:45\n",
      "4    10055344  29209451  2171-10-31 19:38:25  2171-11-02 19:46:41\n",
      "[INFO] 최종 STEMI cohort size (모든 hadm): 1929\n",
      "[INFO] 최종 고유 hadm 수: 1929\n",
      "[INFO] 기존 전체 Event Log 파일 삭제: ./cohort\\event_log_stemi_all.csv\n",
      "\n",
      "[INFO] Batch 처리중: rows 0 ~ 99 (size=100), version=140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_26072\\2319555427.py:593: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_events = pd.concat(\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_26072\\2319555427.py:622: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  all_events.groupby(\"hadm_id\", group_keys=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]   → 생성된 이벤트 row 수: 2351\n",
      "[INFO]   → 포함된 hadm_id 수: 100\n",
      "[INFO]   → chunk 파일 저장: ./cohort\\cohort_ver140.csv\n",
      "[INFO]   → 전체 Event Log에 append: ./cohort\\event_log_stemi_all.csv\n",
      "\n",
      "[INFO] Batch 처리중: rows 100 ~ 199 (size=100), version=140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_26072\\2319555427.py:593: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_events = pd.concat(\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_26072\\2319555427.py:622: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  all_events.groupby(\"hadm_id\", group_keys=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]   → 생성된 이벤트 row 수: 2245\n",
      "[INFO]   → 포함된 hadm_id 수: 100\n",
      "[INFO]   → chunk 파일 저장: ./cohort\\cohort_ver140.csv\n",
      "[INFO]   → 전체 Event Log에 append: ./cohort\\event_log_stemi_all.csv\n",
      "\n",
      "[INFO] Batch 처리중: rows 200 ~ 299 (size=100), version=140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_26072\\2319555427.py:593: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_events = pd.concat(\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_26072\\2319555427.py:622: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  all_events.groupby(\"hadm_id\", group_keys=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]   → 생성된 이벤트 row 수: 2842\n",
      "[INFO]   → 포함된 hadm_id 수: 100\n",
      "[INFO]   → chunk 파일 저장: ./cohort\\cohort_ver140.csv\n",
      "[INFO]   → 전체 Event Log에 append: ./cohort\\event_log_stemi_all.csv\n",
      "\n",
      "[INFO] Batch 처리중: rows 300 ~ 399 (size=100), version=140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_26072\\2319555427.py:593: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_events = pd.concat(\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_26072\\2319555427.py:622: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  all_events.groupby(\"hadm_id\", group_keys=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]   → 생성된 이벤트 row 수: 3093\n",
      "[INFO]   → 포함된 hadm_id 수: 100\n",
      "[INFO]   → chunk 파일 저장: ./cohort\\cohort_ver140.csv\n",
      "[INFO]   → 전체 Event Log에 append: ./cohort\\event_log_stemi_all.csv\n",
      "\n",
      "[INFO] Batch 처리중: rows 400 ~ 499 (size=100), version=140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_26072\\2319555427.py:593: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_events = pd.concat(\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_26072\\2319555427.py:622: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  all_events.groupby(\"hadm_id\", group_keys=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]   → 생성된 이벤트 row 수: 2617\n",
      "[INFO]   → 포함된 hadm_id 수: 100\n",
      "[INFO]   → chunk 파일 저장: ./cohort\\cohort_ver140.csv\n",
      "[INFO]   → 전체 Event Log에 append: ./cohort\\event_log_stemi_all.csv\n",
      "\n",
      "[INFO] Batch 처리중: rows 500 ~ 599 (size=100), version=140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_26072\\2319555427.py:593: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_events = pd.concat(\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_26072\\2319555427.py:622: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  all_events.groupby(\"hadm_id\", group_keys=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]   → 생성된 이벤트 row 수: 2392\n",
      "[INFO]   → 포함된 hadm_id 수: 100\n",
      "[INFO]   → chunk 파일 저장: ./cohort\\cohort_ver140.csv\n",
      "[INFO]   → 전체 Event Log에 append: ./cohort\\event_log_stemi_all.csv\n",
      "\n",
      "[INFO] Batch 처리중: rows 600 ~ 699 (size=100), version=140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_26072\\2319555427.py:593: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_events = pd.concat(\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_26072\\2319555427.py:622: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  all_events.groupby(\"hadm_id\", group_keys=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]   → 생성된 이벤트 row 수: 2644\n",
      "[INFO]   → 포함된 hadm_id 수: 100\n",
      "[INFO]   → chunk 파일 저장: ./cohort\\cohort_ver140.csv\n",
      "[INFO]   → 전체 Event Log에 append: ./cohort\\event_log_stemi_all.csv\n",
      "\n",
      "[INFO] Batch 처리중: rows 700 ~ 799 (size=100), version=140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_26072\\2319555427.py:593: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_events = pd.concat(\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_26072\\2319555427.py:622: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  all_events.groupby(\"hadm_id\", group_keys=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]   → 생성된 이벤트 row 수: 2405\n",
      "[INFO]   → 포함된 hadm_id 수: 100\n",
      "[INFO]   → chunk 파일 저장: ./cohort\\cohort_ver140.csv\n",
      "[INFO]   → 전체 Event Log에 append: ./cohort\\event_log_stemi_all.csv\n",
      "\n",
      "[INFO] Batch 처리중: rows 800 ~ 899 (size=100), version=140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_26072\\2319555427.py:593: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_events = pd.concat(\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_26072\\2319555427.py:622: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  all_events.groupby(\"hadm_id\", group_keys=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]   → 생성된 이벤트 row 수: 1790\n",
      "[INFO]   → 포함된 hadm_id 수: 100\n",
      "[INFO]   → chunk 파일 저장: ./cohort\\cohort_ver140.csv\n",
      "[INFO]   → 전체 Event Log에 append: ./cohort\\event_log_stemi_all.csv\n",
      "\n",
      "[INFO] Batch 처리중: rows 900 ~ 999 (size=100), version=140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_26072\\2319555427.py:593: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_events = pd.concat(\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_26072\\2319555427.py:622: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  all_events.groupby(\"hadm_id\", group_keys=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]   → 생성된 이벤트 row 수: 1705\n",
      "[INFO]   → 포함된 hadm_id 수: 100\n",
      "[INFO]   → chunk 파일 저장: ./cohort\\cohort_ver140.csv\n",
      "[INFO]   → 전체 Event Log에 append: ./cohort\\event_log_stemi_all.csv\n",
      "\n",
      "[INFO] Batch 처리중: rows 1000 ~ 1099 (size=100), version=140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_26072\\2319555427.py:593: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_events = pd.concat(\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_26072\\2319555427.py:622: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  all_events.groupby(\"hadm_id\", group_keys=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]   → 생성된 이벤트 row 수: 1698\n",
      "[INFO]   → 포함된 hadm_id 수: 100\n",
      "[INFO]   → chunk 파일 저장: ./cohort\\cohort_ver140.csv\n",
      "[INFO]   → 전체 Event Log에 append: ./cohort\\event_log_stemi_all.csv\n",
      "\n",
      "[INFO] Batch 처리중: rows 1100 ~ 1199 (size=100), version=140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_26072\\2319555427.py:593: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_events = pd.concat(\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_26072\\2319555427.py:622: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  all_events.groupby(\"hadm_id\", group_keys=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]   → 생성된 이벤트 row 수: 1843\n",
      "[INFO]   → 포함된 hadm_id 수: 100\n",
      "[INFO]   → chunk 파일 저장: ./cohort\\cohort_ver140.csv\n",
      "[INFO]   → 전체 Event Log에 append: ./cohort\\event_log_stemi_all.csv\n",
      "\n",
      "[INFO] Batch 처리중: rows 1200 ~ 1299 (size=100), version=140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_26072\\2319555427.py:593: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_events = pd.concat(\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_26072\\2319555427.py:622: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  all_events.groupby(\"hadm_id\", group_keys=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]   → 생성된 이벤트 row 수: 1811\n",
      "[INFO]   → 포함된 hadm_id 수: 100\n",
      "[INFO]   → chunk 파일 저장: ./cohort\\cohort_ver140.csv\n",
      "[INFO]   → 전체 Event Log에 append: ./cohort\\event_log_stemi_all.csv\n",
      "\n",
      "[INFO] Batch 처리중: rows 1300 ~ 1399 (size=100), version=140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_26072\\2319555427.py:593: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_events = pd.concat(\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_26072\\2319555427.py:622: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  all_events.groupby(\"hadm_id\", group_keys=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]   → 생성된 이벤트 row 수: 1864\n",
      "[INFO]   → 포함된 hadm_id 수: 100\n",
      "[INFO]   → chunk 파일 저장: ./cohort\\cohort_ver140.csv\n",
      "[INFO]   → 전체 Event Log에 append: ./cohort\\event_log_stemi_all.csv\n",
      "\n",
      "[INFO] Batch 처리중: rows 1400 ~ 1499 (size=100), version=140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_26072\\2319555427.py:593: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_events = pd.concat(\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_26072\\2319555427.py:622: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  all_events.groupby(\"hadm_id\", group_keys=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]   → 생성된 이벤트 row 수: 1902\n",
      "[INFO]   → 포함된 hadm_id 수: 100\n",
      "[INFO]   → chunk 파일 저장: ./cohort\\cohort_ver140.csv\n",
      "[INFO]   → 전체 Event Log에 append: ./cohort\\event_log_stemi_all.csv\n",
      "\n",
      "[INFO] Batch 처리중: rows 1500 ~ 1599 (size=100), version=140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_26072\\2319555427.py:593: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_events = pd.concat(\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_26072\\2319555427.py:622: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  all_events.groupby(\"hadm_id\", group_keys=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]   → 생성된 이벤트 row 수: 1776\n",
      "[INFO]   → 포함된 hadm_id 수: 100\n",
      "[INFO]   → chunk 파일 저장: ./cohort\\cohort_ver140.csv\n",
      "[INFO]   → 전체 Event Log에 append: ./cohort\\event_log_stemi_all.csv\n",
      "\n",
      "[INFO] Batch 처리중: rows 1600 ~ 1699 (size=100), version=140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_26072\\2319555427.py:593: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_events = pd.concat(\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_26072\\2319555427.py:622: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  all_events.groupby(\"hadm_id\", group_keys=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]   → 생성된 이벤트 row 수: 1747\n",
      "[INFO]   → 포함된 hadm_id 수: 100\n",
      "[INFO]   → chunk 파일 저장: ./cohort\\cohort_ver140.csv\n",
      "[INFO]   → 전체 Event Log에 append: ./cohort\\event_log_stemi_all.csv\n",
      "\n",
      "[INFO] Batch 처리중: rows 1700 ~ 1799 (size=100), version=140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_26072\\2319555427.py:593: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_events = pd.concat(\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_26072\\2319555427.py:622: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  all_events.groupby(\"hadm_id\", group_keys=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]   → 생성된 이벤트 row 수: 1810\n",
      "[INFO]   → 포함된 hadm_id 수: 100\n",
      "[INFO]   → chunk 파일 저장: ./cohort\\cohort_ver140.csv\n",
      "[INFO]   → 전체 Event Log에 append: ./cohort\\event_log_stemi_all.csv\n",
      "\n",
      "[INFO] Batch 처리중: rows 1800 ~ 1899 (size=100), version=140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_26072\\2319555427.py:593: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_events = pd.concat(\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_26072\\2319555427.py:622: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  all_events.groupby(\"hadm_id\", group_keys=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]   → 생성된 이벤트 row 수: 1752\n",
      "[INFO]   → 포함된 hadm_id 수: 100\n",
      "[INFO]   → chunk 파일 저장: ./cohort\\cohort_ver140.csv\n",
      "[INFO]   → 전체 Event Log에 append: ./cohort\\event_log_stemi_all.csv\n",
      "\n",
      "[INFO] Batch 처리중: rows 1900 ~ 1928 (size=29), version=140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_26072\\2319555427.py:593: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_events = pd.concat(\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_26072\\2319555427.py:622: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  all_events.groupby(\"hadm_id\", group_keys=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]   → 생성된 이벤트 row 수: 530\n",
      "[INFO]   → 포함된 hadm_id 수: 29\n",
      "[INFO]   → chunk 파일 저장: ./cohort\\cohort_ver140.csv\n",
      "[INFO]   → 전체 Event Log에 append: ./cohort\\event_log_stemi_all.csv\n",
      "\n",
      "[INFO] 모든 batch 처리가 완료되었습니다.\n",
      "[INFO] 최종 전체 Event Log 경로: ./cohort\\event_log_stemi_all.csv\n",
      "[INFO] 마지막 사용된 version 번호: 139\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ==============================\n",
    "# 0. 경로 및 기본 설정\n",
    "# ==============================\n",
    "\n",
    "COHORT_PATH = \"./cohort/cohort_ver50_only_subject_id.csv\"\n",
    "\n",
    "HOSP_DIR = \"../../data/MIMIC4-hosp-icu\"\n",
    "ICU_DIR = \"../../data/MIMIC4-hosp-icu\"\n",
    "ED_DIR = \"../../data/mimic-iv-ed/ed\"\n",
    "ECG_DIR = \"../../data/mimic-iv-ecg\"\n",
    "\n",
    "OUTPUT_DIR = \"./cohort\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# 전체 Event Log를 하나로 합친 파일\n",
    "EVENT_LOG_FULL_PATH = os.path.join(OUTPUT_DIR, \"event_log_stemi_all.csv\")\n",
    "\n",
    "# 중간 저장 chunk 파일 이름 규칙\n",
    "CHUNK_VERSION_START = 140\n",
    "BATCH_SIZE = 100  # 한 번에 처리할 환자 수\n",
    "\n",
    "# Troponin 양성 기준 (예: 0.04 이상 양성)\n",
    "TROP_POS_THRESHOLD = 0.04\n",
    "\n",
    "# PCI ICD 코드 prefix (예시 – 필요시 조정 가능)\n",
    "PCI_ICD9_PREFIXES = [\"00.66\", \"36.0\"]\n",
    "PCI_ICD10_PREFIXES = [\"027\", \"929\"]\n",
    "\n",
    "# 항혈소판제 이름 리스트 (대소문자 무시)\n",
    "ANTI_PLT_DRUGS = [\"aspirin\", \"clopidogrel\", \"ticagrelor\", \"prasugrel\"]\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 1. 공통 유틸\n",
    "# ==============================\n",
    "\n",
    "def _to_datetime(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    datetime 파싱.\n",
    "    연도 이상치(미래 연도)는 그대로 허용하고, 파싱 실패만 NaT 처리.\n",
    "    \"\"\"\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def _attrs_to_json(attrs: Dict[str, Any]) -> str:\n",
    "    \"\"\"attributes dict를 JSON 문자열로 변환.\"\"\"\n",
    "    clean = {}\n",
    "    for k, v in attrs.items():\n",
    "        if isinstance(v, float) and pd.isna(v):\n",
    "            clean[k] = None\n",
    "        else:\n",
    "            clean[k] = v\n",
    "    return json.dumps(clean, ensure_ascii=False)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 2. Cohort 및 원본 테이블 로딩\n",
    "# ==============================\n",
    "\n",
    "def load_labevents_troponin() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Troponin 전용 labevents_troponin.csv 로딩.\n",
    "    - HOSP_DIR 아래 또는 현재 작업 디렉토리에서 탐색.\n",
    "    \"\"\"\n",
    "    candidates = [\n",
    "        os.path.join(HOSP_DIR, \"labevents_troponin.csv\"),\n",
    "        \"./labevents_troponin.csv\",\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if os.path.exists(p):\n",
    "            print(f\"[LOAD] labevents_troponin.csv 로딩: {p}\")\n",
    "            df = pd.read_csv(p)\n",
    "            return df\n",
    "\n",
    "    raise FileNotFoundError(\n",
    "        \"labevents_troponin.csv 파일을 찾을 수 없습니다.\\n\"\n",
    "        f\"다음 경로 중 하나에 존재해야 합니다.\\n{candidates}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def load_cohort_all_hadm(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    cohort 파일에서 (subject_id, hadm_id) 모든 조합을 사용.\n",
    "    - 같은 (subject_id, hadm_id)는 중복만 제거.\n",
    "    필수 컬럼: subject_id, hadm_id\n",
    "    \"\"\"\n",
    "    cohort_raw = pd.read_csv(path)\n",
    "\n",
    "    if \"subject_id\" not in cohort_raw.columns:\n",
    "        raise ValueError(f\"cohort 파일에 subject_id 컬럼이 없습니다. 현재 컬럼: {list(cohort_raw.columns)}\")\n",
    "    if \"hadm_id\" not in cohort_raw.columns:\n",
    "        raise ValueError(\n",
    "            \"cohort 파일에 hadm_id 컬럼이 없습니다.\\n\"\n",
    "            \"STEMI 분석은 hadm_id(입원 단위) 기준으로 해야 합니다.\"\n",
    "        )\n",
    "\n",
    "    print(f\"[COHORT] 입력 cohort row 수: {len(cohort_raw)}\")\n",
    "    print(f\"[COHORT] 입력 cohort 고유 subject 수: {cohort_raw['subject_id'].nunique()}\")\n",
    "    print(f\"[COHORT] 입력 cohort 고유 hadm 수: {cohort_raw['hadm_id'].nunique()}\")\n",
    "\n",
    "    cohort_final = cohort_raw[[\"subject_id\", \"hadm_id\"]].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    print(f\"[COHORT] (subject_id, hadm_id) 중복 제거 후 row 수: {len(cohort_final)}\")\n",
    "    print(f\"[COHORT] 최종 고유 subject 수: {cohort_final['subject_id'].nunique()}\")\n",
    "    print(f\"[COHORT] 최종 고유 hadm 수: {cohort_final['hadm_id'].nunique()}\")\n",
    "\n",
    "    return cohort_final\n",
    "\n",
    "\n",
    "def load_source_tables() -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    MIMIC-IV 원본 테이블들을 로딩.\n",
    "    - Troponin은 labevents_troponin.csv만 사용.\n",
    "    \"\"\"\n",
    "    print(\"[LOAD] admissions, patients, icustays...\")\n",
    "    admissions = pd.read_csv(os.path.join(HOSP_DIR, \"admissions.csv\"))\n",
    "    patients = pd.read_csv(os.path.join(HOSP_DIR, \"patients.csv\"))\n",
    "    icustays = pd.read_csv(os.path.join(ICU_DIR, \"icustays.csv\"))\n",
    "\n",
    "    print(\"[LOAD] edstays (있으면 로딩)...\")\n",
    "    edstays_path = os.path.join(ED_DIR, \"edstays.csv\")\n",
    "    edstays = pd.read_csv(edstays_path) if os.path.exists(edstays_path) else None\n",
    "\n",
    "    print(\"[LOAD] labevents_troponin ...\")\n",
    "    labevents_trop = load_labevents_troponin()\n",
    "\n",
    "    print(\"[LOAD] procedures_icd, prescriptions, ECG machine_measurements...\")\n",
    "    procedures_icd = pd.read_csv(os.path.join(HOSP_DIR, \"procedures_icd.csv\"))\n",
    "    prescriptions = pd.read_csv(os.path.join(HOSP_DIR, \"prescriptions.csv\"))\n",
    "    ecg = pd.read_csv(os.path.join(ECG_DIR, \"machine_measurements.csv\"))\n",
    "\n",
    "    return {\n",
    "        \"admissions\": admissions,\n",
    "        \"patients\": patients,\n",
    "        \"icustays\": icustays,\n",
    "        \"edstays\": edstays,\n",
    "        \"labevents_trop\": labevents_trop,\n",
    "        \"procedures_icd\": procedures_icd,\n",
    "        \"prescriptions\": prescriptions,\n",
    "        \"ecg\": ecg,\n",
    "    }\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 3. Event 생성 함수들\n",
    "# ==============================\n",
    "\n",
    "def build_real_ed_events(cohort: pd.DataFrame,\n",
    "                         edstays: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    실제 ED_ARRIVAL, ED_DEPARTURE 이벤트 생성 (edstays 기반).\n",
    "    surrogate는 여기에서 만들지 않고, 나중에 전체 이벤트를 본 뒤\n",
    "    '최초 timestamp' 기반으로 ED_ARRIVAL_SURR를 생성한다.\n",
    "    \"\"\"\n",
    "    if edstays is None:\n",
    "        return pd.DataFrame(columns=[\"subject_id\", \"hadm_id\", \"event_name\", \"timestamp\", \"attributes\"])\n",
    "\n",
    "    ed = edstays.merge(cohort, on=[\"subject_id\", \"hadm_id\"], how=\"inner\")\n",
    "    ed = _to_datetime(ed, [\"intime\", \"outtime\"])\n",
    "\n",
    "    events = []\n",
    "    for _, row in ed.iterrows():\n",
    "        sid = row[\"subject_id\"]\n",
    "        hid = row[\"hadm_id\"]\n",
    "\n",
    "        if pd.notnull(row.get(\"intime\")):\n",
    "            events.append({\n",
    "                \"subject_id\": sid,\n",
    "                \"hadm_id\": hid,\n",
    "                \"event_name\": \"ED_ARRIVAL\",\n",
    "                \"timestamp\": row[\"intime\"],\n",
    "                \"attributes\": _attrs_to_json({}),\n",
    "            })\n",
    "        if pd.notnull(row.get(\"outtime\")):\n",
    "            events.append({\n",
    "                \"subject_id\": sid,\n",
    "                \"hadm_id\": hid,\n",
    "                \"event_name\": \"ED_DEPARTURE\",\n",
    "                \"timestamp\": row[\"outtime\"],\n",
    "                \"attributes\": _attrs_to_json({}),\n",
    "            })\n",
    "\n",
    "    if not events:\n",
    "        return pd.DataFrame(columns=[\"subject_id\", \"hadm_id\", \"event_name\", \"timestamp\", \"attributes\"])\n",
    "\n",
    "    df = pd.DataFrame(events)\n",
    "    df = _to_datetime(df, [\"timestamp\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_admission_events(cohort: pd.DataFrame,\n",
    "                           admissions: pd.DataFrame,\n",
    "                           patients: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    DISCHARGE, DEATH 이벤트 생성.\n",
    "    DISCHARGE: admissions.dischtime\n",
    "    DEATH: patients.dod\n",
    "    \"\"\"\n",
    "    adm = admissions.merge(cohort, on=[\"subject_id\", \"hadm_id\"], how=\"inner\")\n",
    "    adm = _to_datetime(adm, [\"admittime\", \"dischtime\"])\n",
    "    pat = _to_datetime(patients.copy(), [\"dod\"])\n",
    "\n",
    "    adm = adm.merge(pat[[\"subject_id\", \"dod\"]], on=\"subject_id\", how=\"left\")\n",
    "\n",
    "    events = []\n",
    "    for _, row in adm.iterrows():\n",
    "        if pd.notnull(row.get(\"dischtime\")):\n",
    "            events.append({\n",
    "                \"subject_id\": row[\"subject_id\"],\n",
    "                \"hadm_id\": row[\"hadm_id\"],\n",
    "                \"event_name\": \"DISCHARGE\",\n",
    "                \"timestamp\": row[\"dischtime\"],\n",
    "                \"attributes\": _attrs_to_json({}),\n",
    "            })\n",
    "        if pd.notnull(row.get(\"dod\")):\n",
    "            events.append({\n",
    "                \"subject_id\": row[\"subject_id\"],\n",
    "                \"hadm_id\": row[\"hadm_id\"],\n",
    "                \"event_name\": \"DEATH\",\n",
    "                \"timestamp\": row[\"dod\"],\n",
    "                \"attributes\": _attrs_to_json({}),\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(events)\n",
    "\n",
    "\n",
    "def build_icu_events(cohort: pd.DataFrame, icustays: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ICU_INTIME, ICU_OUTTIME 이벤트 생성.\n",
    "    icustays: subject_id, hadm_id, intime, outtime, first_careunit, last_careunit, stay_id 가정.\n",
    "    \"\"\"\n",
    "    icu = icustays.merge(cohort, on=[\"subject_id\", \"hadm_id\"], how=\"inner\")\n",
    "    icu = _to_datetime(icu, [\"intime\", \"outtime\"])\n",
    "\n",
    "    events = []\n",
    "    for _, row in icu.iterrows():\n",
    "        attrs = {\n",
    "            \"first_careunit\": row.get(\"first_careunit\", None),\n",
    "            \"last_careunit\": row.get(\"last_careunit\", None),\n",
    "            \"stay_id\": row.get(\"stay_id\", None),\n",
    "        }\n",
    "        if pd.notnull(row.get(\"intime\")):\n",
    "            events.append({\n",
    "                \"subject_id\": row[\"subject_id\"],\n",
    "                \"hadm_id\": row[\"hadm_id\"],\n",
    "                \"event_name\": \"ICU_INTIME\",\n",
    "                \"timestamp\": row[\"intime\"],\n",
    "                \"attributes\": _attrs_to_json(attrs),\n",
    "            })\n",
    "        if pd.notnull(row.get(\"outtime\")):\n",
    "            events.append({\n",
    "                \"subject_id\": row[\"subject_id\"],\n",
    "                \"hadm_id\": row[\"hadm_id\"],\n",
    "                \"event_name\": \"ICU_OUTTIME\",\n",
    "                \"timestamp\": row[\"outtime\"],\n",
    "                \"attributes\": _attrs_to_json(attrs),\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(events)\n",
    "\n",
    "\n",
    "def build_troponin_events(cohort: pd.DataFrame,\n",
    "                          labevents_trop: pd.DataFrame,\n",
    "                          positive_threshold: float) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    TROP_TAKEN, TROP_POSITIVE 이벤트 생성.\n",
    "    labevents_trop: Troponin만 필터된 labevents_troponin.csv\n",
    "      (subject_id, hadm_id, itemid, charttime, valuenum 등 포함)\n",
    "    \"\"\"\n",
    "    lab = labevents_trop.merge(cohort, on=[\"subject_id\", \"hadm_id\"], how=\"inner\")\n",
    "    lab = _to_datetime(lab, [\"charttime\"])\n",
    "\n",
    "    events = []\n",
    "\n",
    "    # TROP_TAKEN\n",
    "    for _, row in lab.iterrows():\n",
    "        if pd.notnull(row.get(\"charttime\")):\n",
    "            attrs = {\n",
    "                \"itemid\": row.get(\"itemid\", None),\n",
    "                \"valuenum\": row.get(\"valuenum\", None),\n",
    "                \"value\": row.get(\"value\", None),\n",
    "                \"flag\": row.get(\"flag\", None),\n",
    "            }\n",
    "            events.append({\n",
    "                \"subject_id\": row[\"subject_id\"],\n",
    "                \"hadm_id\": row[\"hadm_id\"],\n",
    "                \"event_name\": \"TROP_TAKEN\",\n",
    "                \"timestamp\": row[\"charttime\"],\n",
    "                \"attributes\": _attrs_to_json(attrs),\n",
    "            })\n",
    "\n",
    "    # TROP_POSITIVE: hadm_id 기준 첫 양성 시점\n",
    "    if \"valuenum\" in lab.columns:\n",
    "        lab_pos = lab[lab[\"valuenum\"] >= positive_threshold].copy()\n",
    "        lab_pos = lab_pos.dropna(subset=[\"charttime\"])\n",
    "        lab_pos_sorted = lab_pos.sort_values([\"subject_id\", \"hadm_id\", \"charttime\"])\n",
    "        first_pos = lab_pos_sorted.groupby([\"subject_id\", \"hadm_id\"], as_index=False).head(1)\n",
    "\n",
    "        for _, row in first_pos.iterrows():\n",
    "            attrs = {\n",
    "                \"itemid\": row.get(\"itemid\", None),\n",
    "                \"valuenum\": row.get(\"valuenum\", None),\n",
    "                \"value\": row.get(\"value\", None),\n",
    "                \"flag\": row.get(\"flag\", None),\n",
    "            }\n",
    "            events.append({\n",
    "                \"subject_id\": row[\"subject_id\"],\n",
    "                \"hadm_id\": row[\"hadm_id\"],\n",
    "                \"event_name\": \"TROP_POSITIVE\",\n",
    "                \"timestamp\": row[\"charttime\"],\n",
    "                \"attributes\": _attrs_to_json(attrs),\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(events)\n",
    "\n",
    "\n",
    "def build_ecg_events(cohort: pd.DataFrame, ecg: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ECG_TAKEN, ECG_STEMI_FLAG 이벤트 생성\n",
    "\n",
    "    - machine_measurements.csv는 보통 hadm_id가 없고 subject_id만 있음.\n",
    "    - subject_id로 cohort와 join해서 hadm_id를 붙임.\n",
    "    - 시간 컬럼은 ecg_time 또는 charttime 중 존재하는 것을 사용.\n",
    "    \"\"\"\n",
    "    ecg_c = ecg.copy()\n",
    "    if \"charttime\" not in ecg_c.columns:\n",
    "        if \"ecg_time\" in ecg_c.columns:\n",
    "            ecg_c = ecg_c.rename(columns={\"ecg_time\": \"charttime\"})\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"ECG(machine_measurements) 데이터에 charttime/ecg_time 둘 다 없습니다. \"\n",
    "                \"시간 컬럼 이름을 확인해 주세요.\"\n",
    "            )\n",
    "\n",
    "    subj_hadm_map = cohort[[\"subject_id\", \"hadm_id\"]].drop_duplicates()\n",
    "    ecg_c = ecg_c.merge(subj_hadm_map, on=\"subject_id\", how=\"inner\")\n",
    "    ecg_c = _to_datetime(ecg_c, [\"charttime\"])\n",
    "\n",
    "    def has_stemi_flag(row) -> bool:\n",
    "        mm = str(row.get(\"machine_measurements\", \"\")).upper()\n",
    "        reports = []\n",
    "        for i in range(30):\n",
    "            col = f\"report_{i}\"\n",
    "            if col in row.index:\n",
    "                reports.append(str(row.get(col, \"\")))\n",
    "        rep_text = \" \".join(reports).upper()\n",
    "        text = mm + \" \" + rep_text\n",
    "\n",
    "        if \"STEMI\" in text:\n",
    "            return True\n",
    "        if \"ST ELEVATION\" in text:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    ecg_c[\"is_stemi\"] = ecg_c.apply(has_stemi_flag, axis=1)\n",
    "\n",
    "    events = []\n",
    "    for _, row in ecg_c.iterrows():\n",
    "        ts = row.get(\"charttime\")\n",
    "        if pd.isna(ts):\n",
    "            continue\n",
    "\n",
    "        events.append({\n",
    "            \"subject_id\": row[\"subject_id\"],\n",
    "            \"hadm_id\": row[\"hadm_id\"],\n",
    "            \"event_name\": \"ECG_TAKEN\",\n",
    "            \"timestamp\": ts,\n",
    "            \"attributes\": _attrs_to_json({}),\n",
    "        })\n",
    "\n",
    "        if row[\"is_stemi\"]:\n",
    "            events.append({\n",
    "                \"subject_id\": row[\"subject_id\"],\n",
    "                \"hadm_id\": row[\"hadm_id\"],\n",
    "                \"event_name\": \"ECG_STEMI_FLAG\",\n",
    "                \"timestamp\": ts,\n",
    "                \"attributes\": _attrs_to_json({}),\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(events)\n",
    "\n",
    "\n",
    "def build_pci_events(cohort: pd.DataFrame,\n",
    "                     procedures_icd: pd.DataFrame,\n",
    "                     pci_icd9_prefixes: List[str],\n",
    "                     pci_icd10_prefixes: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    PCI_START 이벤트 생성 (날짜 단위).\n",
    "    procedures_icd: subject_id, hadm_id, icd_code, icd_version, chartdate 가정.\n",
    "    \"\"\"\n",
    "    proc = procedures_icd.merge(cohort, on=[\"subject_id\", \"hadm_id\"], how=\"inner\")\n",
    "    proc = _to_datetime(proc, [\"chartdate\"])\n",
    "\n",
    "    def is_pci(code: Any, version: Any) -> bool:\n",
    "        if pd.isna(code):\n",
    "            return False\n",
    "        code_str = str(code)\n",
    "        if version == 9:\n",
    "            return any(code_str.startswith(p) for p in pci_icd9_prefixes)\n",
    "        elif version == 10:\n",
    "            return any(code_str.startswith(p) for p in pci_icd10_prefixes)\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    proc[\"is_pci\"] = proc.apply(\n",
    "        lambda r: is_pci(r.get(\"icd_code\", None), r.get(\"icd_version\", None)), axis=1\n",
    "    )\n",
    "    pci_rows = proc[proc[\"is_pci\"]].copy()\n",
    "\n",
    "    events = []\n",
    "    for _, row in pci_rows.iterrows():\n",
    "        ts = row.get(\"chartdate\")\n",
    "        if pd.isna(ts):\n",
    "            continue\n",
    "        attrs = {\n",
    "            \"icd_code\": row.get(\"icd_code\", None),\n",
    "            \"icd_version\": row.get(\"icd_version\", None),\n",
    "        }\n",
    "        events.append({\n",
    "            \"subject_id\": row[\"subject_id\"],\n",
    "            \"hadm_id\": row[\"hadm_id\"],\n",
    "            \"event_name\": \"PCI_START\",\n",
    "            \"timestamp\": ts,\n",
    "            \"attributes\": _attrs_to_json(attrs),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(events)\n",
    "\n",
    "\n",
    "def build_antiplatelet_events(cohort: pd.DataFrame,\n",
    "                              prescriptions: pd.DataFrame,\n",
    "                              drug_name_list: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ANTI_PLT_ORDER, ANTI_PLT_ADMIN 이벤트 생성.\n",
    "    prescriptions: subject_id, hadm_id, drug, starttime, stoptime, route, dose_val_rx, dose_unit_rx 가정.\n",
    "    \"\"\"\n",
    "    rx = prescriptions.merge(cohort, on=[\"subject_id\", \"hadm_id\"], how=\"inner\")\n",
    "    rx = _to_datetime(rx, [\"starttime\", \"stoptime\"])\n",
    "\n",
    "    drug_lower_list = [d.lower() for d in drug_name_list]\n",
    "    rx[\"drug_lower\"] = rx[\"drug\"].astype(str).str.lower()\n",
    "    rx = rx[rx[\"drug_lower\"].isin(drug_lower_list)]\n",
    "\n",
    "    events = []\n",
    "    for _, row in rx.iterrows():\n",
    "        if pd.notnull(row.get(\"starttime\")):\n",
    "            attrs = {\n",
    "                \"drug\": row.get(\"drug\", None),\n",
    "                \"route\": row.get(\"route\", None),\n",
    "                \"dose_val_rx\": row.get(\"dose_val_rx\", None),\n",
    "                \"dose_unit_rx\": row.get(\"dose_unit_rx\", None),\n",
    "            }\n",
    "            events.append({\n",
    "                \"subject_id\": row[\"subject_id\"],\n",
    "                \"hadm_id\": row[\"hadm_id\"],\n",
    "                \"event_name\": \"ANTI_PLT_ORDER\",\n",
    "                \"timestamp\": row[\"starttime\"],\n",
    "                \"attributes\": _attrs_to_json(attrs),\n",
    "            })\n",
    "            events.append({\n",
    "                \"subject_id\": row[\"subject_id\"],\n",
    "                \"hadm_id\": row[\"hadm_id\"],\n",
    "                \"event_name\": \"ANTI_PLT_ADMIN\",\n",
    "                \"timestamp\": row[\"starttime\"],\n",
    "                \"attributes\": _attrs_to_json(attrs),\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(events)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 4. Event Log 통합 함수\n",
    "# ==============================\n",
    "\n",
    "def build_event_log(cohort: pd.DataFrame, tables: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    주어진 cohort subset(subject_id, hadm_id)에 대해 Event Log 생성.\n",
    "\n",
    "    최종 컬럼:\n",
    "      - case_id\n",
    "      - subject_id\n",
    "      - hadm_id\n",
    "      - event_name\n",
    "      - timestamp\n",
    "      - attributes(JSON 문자열)\n",
    "\n",
    "    로직:\n",
    "      1) ED를 제외한 모든 이벤트(ADMISSION, ICU, TROP, ECG, PCI, ANTI-PLT)를 먼저 생성\n",
    "      2) 이 이벤트들의 최소 timestamp와 admissions.admittime을 이용해\n",
    "         hadm별 '최초 timestamp'를 계산\n",
    "      3) 실제 ED(ED_ARRIVAL/ED_DEPARTURE)를 edstays에서 생성\n",
    "      4) ED가 전혀 없는 hadm에 대해서는\n",
    "         최초 timestamp 기반 ED_ARRIVAL_SURR 생성\n",
    "      5) DISCHARGE/DEATH 이후 이벤트는 잘라냄\n",
    "    \"\"\"\n",
    "    admissions = tables[\"admissions\"]\n",
    "    patients = tables[\"patients\"]\n",
    "    icustays = tables[\"icustays\"]\n",
    "    edstays = tables[\"edstays\"]\n",
    "    labevents_trop = tables[\"labevents_trop\"]\n",
    "    procedures_icd = tables[\"procedures_icd\"]\n",
    "    prescriptions = tables[\"prescriptions\"]\n",
    "    ecg = tables[\"ecg\"]\n",
    "\n",
    "    # 1) ED를 제외한 이벤트 먼저 생성\n",
    "    adm_events = build_admission_events(cohort, admissions, patients)\n",
    "    icu_events = build_icu_events(cohort, icustays)\n",
    "    trop_events = build_troponin_events(cohort, labevents_trop, TROP_POS_THRESHOLD)\n",
    "    ecg_events = build_ecg_events(cohort, ecg)\n",
    "    pci_events = build_pci_events(cohort, procedures_icd,\n",
    "                                  pci_icd9_prefixes=PCI_ICD9_PREFIXES,\n",
    "                                  pci_icd10_prefixes=PCI_ICD10_PREFIXES)\n",
    "    antiplatelet_events = build_antiplatelet_events(cohort, prescriptions, ANTI_PLT_DRUGS)\n",
    "\n",
    "    pre_events = pd.concat(\n",
    "        [\n",
    "            adm_events,\n",
    "            icu_events,\n",
    "            trop_events,\n",
    "            ecg_events,\n",
    "            pci_events,\n",
    "            antiplatelet_events,\n",
    "        ],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # 2) hadm별 최초 timestamp (pre_events 기준)\n",
    "    if not pre_events.empty:\n",
    "        pre_events = pre_events.dropna(subset=[\"timestamp\"])\n",
    "        pre_events[\"timestamp\"] = pd.to_datetime(pre_events[\"timestamp\"], errors=\"coerce\")\n",
    "        pre_events = pre_events.dropna(subset=[\"timestamp\"])\n",
    "        earliest_from_events = pre_events.groupby(\"hadm_id\")[\"timestamp\"].min()\n",
    "    else:\n",
    "        earliest_from_events = pd.Series(dtype=\"datetime64[ns]\")\n",
    "\n",
    "    # admissions.admittime 기반 fallback\n",
    "    adm_subset = admissions.merge(cohort, on=[\"subject_id\", \"hadm_id\"], how=\"inner\")\n",
    "    adm_subset = _to_datetime(adm_subset, [\"admittime\"])\n",
    "    earliest_admit = (\n",
    "        adm_subset.dropna(subset=[\"admittime\"])\n",
    "        .groupby(\"hadm_id\")[\"admittime\"].min()\n",
    "        if not adm_subset.empty else pd.Series(dtype=\"datetime64[ns]\")\n",
    "    )\n",
    "\n",
    "    # 3) 실제 ED 이벤트 생성\n",
    "    real_ed_events = build_real_ed_events(cohort, edstays)\n",
    "    hadm_with_real_ed = set(real_ed_events[\"hadm_id\"].unique()) if not real_ed_events.empty else set()\n",
    "\n",
    "    # 4) surrogate ED_ARRIVAL 생성\n",
    "    surrogate_rows = []\n",
    "    for _, row in cohort.iterrows():\n",
    "        sid = row[\"subject_id\"]\n",
    "        hid = row[\"hadm_id\"]\n",
    "\n",
    "        # 이미 실제 ED가 있으면 surrogate 생성 안 함\n",
    "        if hid in hadm_with_real_ed:\n",
    "            continue\n",
    "\n",
    "        ts = None\n",
    "        if hid in earliest_from_events.index:\n",
    "            ts = earliest_from_events.loc[hid]\n",
    "        elif hid in earliest_admit.index:\n",
    "            ts = earliest_admit.loc[hid]\n",
    "\n",
    "        if ts is None or pd.isna(ts):\n",
    "            # 완전히 timestamp를 알 수 없는 hadm → 이벤트를 만들 수 없음\n",
    "            continue\n",
    "\n",
    "        surrogate_rows.append({\n",
    "            \"subject_id\": sid,\n",
    "            \"hadm_id\": hid,\n",
    "            \"event_name\": \"ED_ARRIVAL_SURR\",\n",
    "            \"timestamp\": ts,\n",
    "            \"attributes\": _attrs_to_json({\"source\": \"earliest_timestamp\"}),\n",
    "        })\n",
    "\n",
    "    surrogate_ed_events = (\n",
    "        pd.DataFrame(surrogate_rows)\n",
    "        if surrogate_rows\n",
    "        else pd.DataFrame(columns=[\"subject_id\", \"hadm_id\", \"event_name\", \"timestamp\", \"attributes\"])\n",
    "    )\n",
    "\n",
    "    # 5) 전체 이벤트 합치기\n",
    "    all_events = pd.concat(\n",
    "        [\n",
    "            pre_events,\n",
    "            real_ed_events,\n",
    "            surrogate_ed_events,\n",
    "        ],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    if all_events.empty:\n",
    "        return pd.DataFrame(columns=[\"case_id\", \"subject_id\", \"hadm_id\", \"event_name\", \"timestamp\", \"attributes\"])\n",
    "\n",
    "    all_events = all_events.dropna(subset=[\"timestamp\"])\n",
    "    all_events[\"timestamp\"] = pd.to_datetime(all_events[\"timestamp\"], errors=\"coerce\")\n",
    "    all_events = all_events.dropna(subset=[\"timestamp\"])\n",
    "\n",
    "    all_events = all_events.sort_values(\n",
    "        by=[\"subject_id\", \"hadm_id\", \"timestamp\", \"event_name\"]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # 6) 한 입원(hadm_id)에서 DISCHARGE 또는 DEATH 이후 이벤트 제거\n",
    "    def trim_after_end(df_one_adm: pd.DataFrame) -> pd.DataFrame:\n",
    "        end_idx = df_one_adm[df_one_adm[\"event_name\"].isin([\"DISCHARGE\", \"DEATH\"])].index\n",
    "        if len(end_idx) == 0:\n",
    "            return df_one_adm\n",
    "        cutoff = end_idx.min()\n",
    "        return df_one_adm.loc[:cutoff]\n",
    "\n",
    "    all_events = (\n",
    "        all_events.groupby(\"hadm_id\", group_keys=False)\n",
    "                  .apply(trim_after_end)\n",
    "                  .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # case_id = hadm_id\n",
    "    all_events[\"case_id\"] = all_events[\"hadm_id\"]\n",
    "\n",
    "    # 최종 컬럼 순서 정리\n",
    "    all_events = all_events[\n",
    "        [\"case_id\", \"subject_id\", \"hadm_id\", \"event_name\", \"timestamp\", \"attributes\"]\n",
    "    ]\n",
    "\n",
    "    return all_events\n",
    "\n",
    "\n",
    "def debug_check_ed_icu(cohort: pd.DataFrame, tables: Dict[str, pd.DataFrame]):\n",
    "    edstays = tables[\"edstays\"]\n",
    "    icustays = tables[\"icustays\"]\n",
    "\n",
    "    print(\"\\n[DEBUG] ===== ED / ICU 매칭 상태 점검 =====\")\n",
    "\n",
    "    # 1) ED\n",
    "    if edstays is None:\n",
    "        print(\"[DEBUG][ED] edstays.csv 를 찾지 못했습니다. 실제 ED 이벤트는 생성되지 않습니다.\")\n",
    "        print(\"            다만 earliest timestamp/admittime 기반 ED_ARRIVAL_SURR 는 생성됩니다.\")\n",
    "    else:\n",
    "        print(f\"[DEBUG][ED] edstays 원본 row 수: {len(edstays)}\")\n",
    "        print(f\"[DEBUG][ED] cohort row 수: {len(cohort)}\")\n",
    "\n",
    "        ed_merge = edstays.merge(cohort, on=[\"subject_id\", \"hadm_id\"], how=\"inner\")\n",
    "        print(f\"[DEBUG][ED] (subject_id, hadm_id) 기준 merge 후 row 수: {len(ed_merge)}\")\n",
    "\n",
    "        if len(ed_merge) > 0:\n",
    "            print(\"[DEBUG][ED] 예시 5개:\")\n",
    "            print(ed_merge[[\"subject_id\", \"hadm_id\", \"intime\", \"outtime\"]].head())\n",
    "        else:\n",
    "            print(\"[DEBUG][ED] 매칭된 ED row가 없습니다. 모든 hadm은 surrogate ED_ARRIVAL만 갖게 됩니다.\")\n",
    "\n",
    "    # 2) ICU\n",
    "    icu = icustays\n",
    "    print(f\"\\n[DEBUG][ICU] icustays 원본 row 수: {len(icu)}\")\n",
    "    icu_merge = icu.merge(cohort, on=[\"subject_id\", \"hadm_id\"], how=\"inner\")\n",
    "    print(f\"[DEBUG][ICU] (subject_id, hadm_id) 기준 merge 후 row 수: {len(icu_merge)}\")\n",
    "\n",
    "    if len(icu_merge) > 0:\n",
    "        print(\"[DEBUG][ICU] 예시 5개:\")\n",
    "        print(icu_merge[[\"subject_id\", \"hadm_id\", \"intime\", \"outtime\"]].head())\n",
    "    else:\n",
    "        print(\"[DEBUG][ICU] 매칭된 ICU row가 없습니다. 이 경우 ICU_INTIME/OUTTIME 이벤트는 생성되지 않습니다.\")\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 5. batch + versioning MAIN\n",
    "# ==============================\n",
    "\n",
    "def main():\n",
    "    # 1) 코호트 로딩 (모든 hadm 사용)\n",
    "    cohort = load_cohort_all_hadm(COHORT_PATH)\n",
    "    tables = load_source_tables()\n",
    "\n",
    "    debug_check_ed_icu(cohort, tables)\n",
    "\n",
    "    print(f\"[INFO] 최종 STEMI cohort size (모든 hadm): {len(cohort)}\")\n",
    "    print(f\"[INFO] 최종 고유 hadm 수: {cohort['hadm_id'].nunique()}\")\n",
    "\n",
    "    # 기존 전체 Event Log 파일이 있으면 삭제\n",
    "    if os.path.exists(EVENT_LOG_FULL_PATH):\n",
    "        os.remove(EVENT_LOG_FULL_PATH)\n",
    "        print(f\"[INFO] 기존 전체 Event Log 파일 삭제: {EVENT_LOG_FULL_PATH}\")\n",
    "\n",
    "    header_written_full = False\n",
    "    version = CHUNK_VERSION_START\n",
    "\n",
    "    n = len(cohort)\n",
    "    for start in range(0, n, BATCH_SIZE):\n",
    "        end = min(start + BATCH_SIZE, n)\n",
    "        sub_cohort = cohort.iloc[start:end].copy()\n",
    "\n",
    "        print(f\"\\n[INFO] Batch 처리중: rows {start} ~ {end-1} \"\n",
    "              f\"(size={len(sub_cohort)}), version={version}\")\n",
    "\n",
    "        event_log_chunk = build_event_log(sub_cohort, tables)\n",
    "        num_hadm_chunk = event_log_chunk[\"hadm_id\"].nunique() if not event_log_chunk.empty else 0\n",
    "        print(f\"[INFO]   → 생성된 이벤트 row 수: {len(event_log_chunk)}\")\n",
    "        print(f\"[INFO]   → 포함된 hadm_id 수: {num_hadm_chunk}\")\n",
    "\n",
    "        # (1) batch 전용 파일 저장: cohort_ver{version}.csv\n",
    "        chunk_path = os.path.join(OUTPUT_DIR, f\"cohort_ver{version}.csv\")\n",
    "        event_log_chunk.to_csv(chunk_path, index=False)\n",
    "        print(f\"[INFO]   → chunk 파일 저장: {chunk_path}\")\n",
    "\n",
    "        # (2) 전체 Event Log 파일에 append\n",
    "        if len(event_log_chunk) > 0:\n",
    "            mode = \"a\"\n",
    "            header = not header_written_full\n",
    "            event_log_chunk.to_csv(\n",
    "                EVENT_LOG_FULL_PATH,\n",
    "                mode=mode,\n",
    "                header=header,\n",
    "                index=False\n",
    "            )\n",
    "            header_written_full = True\n",
    "            print(f\"[INFO]   → 전체 Event Log에 append: {EVENT_LOG_FULL_PATH}\")\n",
    "        else:\n",
    "            print(\"[WARN]   → 이 batch에서는 생성된 이벤트가 없습니다.\")\n",
    "\n",
    "        version = 140\n",
    "\n",
    "    print(\"\\n[INFO] 모든 batch 처리가 완료되었습니다.\")\n",
    "    print(f\"[INFO] 최종 전체 Event Log 경로: {EVENT_LOG_FULL_PATH}\")\n",
    "    print(f\"[INFO] 마지막 사용된 version 번호: {version}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2444b4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
