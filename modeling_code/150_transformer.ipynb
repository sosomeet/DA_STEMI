{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bec6adf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] device: cpu\n",
      "[LOAD] 데이터 로딩 완료\n",
      " - shape : (40817, 27)\n",
      "[PREP] race -> race_enc 인코딩 완료\n",
      "[PREP] delay_label 생성 완료 (75% 기준: 168.10)\n",
      "[SPLIT] hadm_id 기준 분할 완료\n",
      " - 전체 hadm_id: 1929\n",
      " - train hadm_id: 1350 , rows: 28412\n",
      " - val   hadm_id: 289 , rows: 5723\n",
      " - test  hadm_id: 290 , rows: 6682\n",
      "[INFO] Transformer input feature 수: 26\n",
      "[INFO] Feature 예시: ['age', 'gender', 'race_enc', 'arrival_transport', 'prefix_len', 'current_event_id', 'time_since_start_min', 'time_since_ed', 'time_since_last', 'is_night']\n",
      "[INFO] target_next_evt num_class: 14\n",
      "[PREP] Feature StandardScaler 적용 완료\n",
      "[PREP] 회귀 타깃 표준화 완료 (time_to_next_std, remain_los_std)\n",
      "[SEQ] Train: (1350, 128, 26) (1350, 128)\n",
      "[SEQ] Val  : (289, 128, 26) (289, 128)\n",
      "[SEQ] Test : (290, 128, 26) (290, 128)\n",
      "[INFO] mort_pos_ratio=0.0810, pos_weight_mort=11.35\n",
      "[INFO] delay_pos_ratio=0.2489, pos_weight_delay=3.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 01] TrainLoss=16.0110 | Val mortality AUC=0.7378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 02] TrainLoss=13.2749 | Val mortality AUC=0.7795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 03] TrainLoss=11.5749 | Val mortality AUC=0.9339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 04] TrainLoss=9.8191 | Val mortality AUC=0.9249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 05] TrainLoss=9.3752 | Val mortality AUC=0.9975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 06] TrainLoss=7.3548 | Val mortality AUC=0.9837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 07] TrainLoss=6.6432 | Val mortality AUC=0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 08] TrainLoss=6.2682 | Val mortality AUC=0.9908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 09] TrainLoss=5.4926 | Val mortality AUC=0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] TrainLoss=5.0109 | Val mortality AUC=0.9954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11] TrainLoss=4.2461 | Val mortality AUC=0.9986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12] TrainLoss=3.6805 | Val mortality AUC=0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13] TrainLoss=3.1199 | Val mortality AUC=0.9951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14] TrainLoss=2.6223 | Val mortality AUC=0.9988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15] TrainLoss=2.3058 | Val mortality AUC=0.9922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 16] TrainLoss=3.6419 | Val mortality AUC=0.9918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 17] TrainLoss=2.9653 | Val mortality AUC=0.9964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 18] TrainLoss=2.4921 | Val mortality AUC=0.9349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 19] TrainLoss=2.6160 | Val mortality AUC=0.9809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 20] TrainLoss=2.5613 | Val mortality AUC=0.9827\n",
      "[INFO] Best Val mortality AUC: 0.9988 모델 로드 완료\n",
      "\n",
      "[FINAL EVAL]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: {'mortality': {'AUC': np.float64(0.9994616825747086), 'AP': np.float64(0.9900258173858376), 'ACC': 0.9775726060348792, 'PREC': 0.7839127471029311, 'REC': 1.0, 'F1': 0.8788689338937715}, 'delay': {'AUC': np.float64(0.9883536413868453), 'AP': np.float64(0.9647584776118696), 'ACC': 0.9272701545862959, 'PREC': 0.7848764377633527, 'REC': 0.9763422581102139, 'F1': 0.8702020202020202}, 'next_event': {'ACC': 0.9475043333687078, 'PREC': 0.8370103029686334, 'REC': 0.8434321984148788, 'F1': 0.8344057096208176}, 'time_to_next': {'RMSE': 0.7709130914464908, 'MAE': 0.4610585985545264}, 'remain_los': {'RMSE': 0.9229646543185377, 'MAE': 0.6480720407922537}}\n",
      "Val  : {'mortality': {'AUC': np.float64(0.9827373228065657), 'AP': np.float64(0.9136102533698841), 'ACC': 0.9578892189411148, 'PREC': 0.757754800590842, 'REC': 0.8694915254237288, 'F1': 0.8097868981846882}, 'delay': {'AUC': np.float64(0.9857378455803261), 'AP': np.float64(0.960423668884319), 'ACC': 0.9241656473877337, 'PREC': 0.7835616438356164, 'REC': 0.9734513274336283, 'F1': 0.8682452944748027}, 'next_event': {'ACC': 0.9315044557050498, 'PREC': 0.8211272601845813, 'REC': 0.8257164088620236, 'F1': 0.8161500572734333}, 'time_to_next': {'RMSE': 0.8286089521922476, 'MAE': 0.48986655261738626}, 'remain_los': {'RMSE': 1.0392882117723594, 'MAE': 0.7123876610859714}}\n",
      "Test : {'mortality': {'AUC': np.float64(0.950632842212199), 'AP': np.float64(0.8504594855640369), 'ACC': 0.9431229033241842, 'PREC': 0.6706263498920086, 'REC': 0.9013062409288825, 'F1': 0.7690402476780186}, 'delay': {'AUC': np.float64(0.9821564478570969), 'AP': np.float64(0.9510024418528753), 'ACC': 0.9194876486733761, 'PREC': 0.7761786600496278, 'REC': 0.9530773918342474, 'F1': 0.8555798687089715}, 'next_event': {'ACC': 0.921164989326014, 'PREC': 0.8666803079865103, 'REC': 0.8601210941442196, 'F1': 0.8575403590060513}, 'time_to_next': {'RMSE': 0.8932696441962036, 'MAE': 0.5030342852940309}, 'remain_los': {'RMSE': 1.043753316789434, 'MAE': 0.6867198589152507}}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score,\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    mean_squared_error, mean_absolute_error\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# 0. 설정\n",
    "# ================================\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "\n",
    "DATA_PATH = \"./../cohort/cohort_ver151_reorder_col.csv\"\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "LR = 1e-3\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"[INFO] device:\", device)\n",
    "\n",
    "# ================================\n",
    "# 1. 데이터 로딩\n",
    "# ================================\n",
    "df = pd.read_csv(DATA_PATH, low_memory=False)\n",
    "print(\"[LOAD] 데이터 로딩 완료\")\n",
    "print(\" - shape :\", df.shape)\n",
    "\n",
    "# ================================\n",
    "# 2. 기본 전처리\n",
    "#    - race 인코딩\n",
    "#    - time_to_next_clip / log1p\n",
    "#    - delay_label 생성 (75% 기준)\n",
    "# ================================\n",
    "if \"race\" in df.columns:\n",
    "    le_race = LabelEncoder()\n",
    "    df[\"race_enc\"] = le_race.fit_transform(df[\"race\"].astype(str))\n",
    "    print(\"[PREP] race -> race_enc 인코딩 완료\")\n",
    "else:\n",
    "    print(\"[WARN] race 컬럼 없음, race_enc 미생성\")\n",
    "\n",
    "if \"target_time_to_next\" not in df.columns:\n",
    "    raise ValueError(\"target_time_to_next 컬럼이 없습니다.\")\n",
    "\n",
    "clip_value = df[\"target_time_to_next\"].quantile(0.995)\n",
    "df[\"time_to_next_clip\"] = df[\"target_time_to_next\"].clip(upper=clip_value)\n",
    "df[\"time_to_next_log1p\"] = np.log1p(df[\"time_to_next_clip\"])\n",
    "\n",
    "if \"delay_label\" not in df.columns:\n",
    "    delay_thr = df[\"time_to_next_clip\"].quantile(0.75)\n",
    "    df[\"delay_label\"] = (df[\"time_to_next_clip\"] > delay_thr).astype(int)\n",
    "    print(f\"[PREP] delay_label 생성 완료 (75% 기준: {delay_thr:.2f})\")\n",
    "\n",
    "# ================================\n",
    "# 3. hadm_id 기준 Train / Val / Test 분할\n",
    "# ================================\n",
    "def split_by_hadm(df_in, random_state=42, train_ratio=0.7, val_ratio=0.15):\n",
    "    if \"hadm_id\" not in df_in.columns:\n",
    "        raise ValueError(\"hadm_id 컬럼이 없습니다.\")\n",
    "\n",
    "    hadm_ids = df_in[\"hadm_id\"].unique()\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    rng.shuffle(hadm_ids)\n",
    "\n",
    "    n = len(hadm_ids)\n",
    "    n_train = int(n * train_ratio)\n",
    "    n_val = int(n * val_ratio)\n",
    "\n",
    "    hadm_train = hadm_ids[:n_train]\n",
    "    hadm_val   = hadm_ids[n_train:n_train + n_val]\n",
    "    hadm_test  = hadm_ids[n_train + n_val:]\n",
    "\n",
    "    df_train = df_in[df_in[\"hadm_id\"].isin(hadm_train)].copy()\n",
    "    df_val   = df_in[df_in[\"hadm_id\"].isin(hadm_val)].copy()\n",
    "    df_test  = df_in[df_in[\"hadm_id\"].isin(hadm_test)].copy()\n",
    "\n",
    "    print(\"[SPLIT] hadm_id 기준 분할 완료\")\n",
    "    print(\" - 전체 hadm_id:\", n)\n",
    "    print(\" - train hadm_id:\", len(hadm_train), \", rows:\", len(df_train))\n",
    "    print(\" - val   hadm_id:\", len(hadm_val),   \", rows:\", len(df_val))\n",
    "    print(\" - test  hadm_id:\", len(hadm_test),  \", rows:\", len(df_test))\n",
    "\n",
    "    return df_train, df_val, df_test\n",
    "\n",
    "df_train, df_val, df_test = split_by_hadm(df, random_state=RANDOM_STATE)\n",
    "\n",
    "# ================================\n",
    "# 4. Feature Engineering\n",
    "# ================================\n",
    "def add_features(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_fe = df_in.copy()\n",
    "\n",
    "    # HR/BP 비율\n",
    "    if \"current_heart_rate\" in df_fe.columns and \"current_mean_bp\" in df_fe.columns:\n",
    "        df_fe[\"hr_bp_ratio\"] = df_fe[\"current_heart_rate\"] / (df_fe[\"current_mean_bp\"].abs() + 1.0)\n",
    "    else:\n",
    "        df_fe[\"hr_bp_ratio\"] = 0.0\n",
    "\n",
    "    # 마지막 이벤트 시간 / 전체 시간 비율\n",
    "    if \"time_since_last\" in df_fe.columns and \"time_since_start_min\" in df_fe.columns:\n",
    "        df_fe[\"time_last_ratio\"] = df_fe[\"time_since_last\"] / (df_fe[\"time_since_start_min\"].abs() + 1.0)\n",
    "    else:\n",
    "        df_fe[\"time_last_ratio\"] = 0.0\n",
    "\n",
    "    # pathway 진행도\n",
    "    if \"pathway_stage\" in df_fe.columns and \"prefix_len\" in df_fe.columns:\n",
    "        df_fe[\"event_progress\"] = df_fe[\"pathway_stage\"] / (df_fe[\"prefix_len\"] + 1.0)\n",
    "    else:\n",
    "        df_fe[\"event_progress\"] = 0.0\n",
    "\n",
    "    # 글로벌 median 기준 delay risk\n",
    "    if \"time_since_last\" in df.columns:\n",
    "        global_median = df[\"time_since_last\"].median()\n",
    "        df_fe[\"risk_delay\"] = (df_fe[\"time_since_last\"] > global_median).astype(int)\n",
    "    else:\n",
    "        df_fe[\"risk_delay\"] = 0\n",
    "\n",
    "    # STEMI 누적 위험도\n",
    "    has_stemi = \"stemi_flag\" in df_fe.columns\n",
    "    has_cum_stemi = \"cum_stemi_cnt\" in df_fe.columns\n",
    "    if has_stemi and has_cum_stemi:\n",
    "        df_fe[\"risk_stemi\"] = df_fe[\"stemi_flag\"] * df_fe[\"cum_stemi_cnt\"]\n",
    "    elif has_stemi:\n",
    "        df_fe[\"risk_stemi\"] = df_fe[\"stemi_flag\"]\n",
    "    else:\n",
    "        df_fe[\"risk_stemi\"] = 0\n",
    "\n",
    "    # Troponin 이상 여부\n",
    "    if \"last_trop\" in df_fe.columns:\n",
    "        df_fe[\"trop_abnormal\"] = (df_fe[\"last_trop\"] > 0.04).astype(int)\n",
    "    else:\n",
    "        df_fe[\"trop_abnormal\"] = 0\n",
    "\n",
    "    return df_fe\n",
    "\n",
    "df_train = add_features(df_train)\n",
    "df_val   = add_features(df_val)\n",
    "df_test  = add_features(df_test)\n",
    "\n",
    "# ================================\n",
    "# 5. Transformer 입력 feature 정의\n",
    "# ================================\n",
    "candidate_cols = [\n",
    "    \"age\", \"gender\", \"race_enc\",\n",
    "    \"arrival_transport\",\n",
    "    \"prefix_len\", \"current_event_id\",\n",
    "    \"time_since_start_min\", \"time_since_ed\", \"time_since_last\",\n",
    "    \"is_night\",\n",
    "    \"cum_ecg_cnt\", \"cum_trop_cnt\",\n",
    "    \"stemi_flag\", \"trop_pos_flag\",\n",
    "    \"last_trop\", \"run_max_trop\", \"trop_trend\",\n",
    "    \"pci_status\",\n",
    "    \"current_heart_rate\", \"current_mean_bp\",\n",
    "    \"hr_bp_ratio\", \"time_last_ratio\", \"event_progress\",\n",
    "    \"risk_delay\", \"risk_stemi\", \"trop_abnormal\",\n",
    "]\n",
    "\n",
    "feature_cols = [c for c in candidate_cols if c in df_train.columns]\n",
    "print(\"[INFO] Transformer input feature 수:\", len(feature_cols))\n",
    "print(\"[INFO] Feature 예시:\", feature_cols[:10])\n",
    "\n",
    "num_next_classes = int(df[\"target_next_evt\"].max())\n",
    "print(\"[INFO] target_next_evt num_class:\", num_next_classes)\n",
    "\n",
    "# ================================\n",
    "# 6. Feature / 타깃 정규화 (성능 향상을 위해 추가)\n",
    "# ================================\n",
    "# 6-1) 입력 feature 표준화\n",
    "scaler_X = StandardScaler()\n",
    "scaler_X.fit(df_train[feature_cols].values)\n",
    "\n",
    "df_train[feature_cols] = scaler_X.transform(df_train[feature_cols].values)\n",
    "df_val[feature_cols]   = scaler_X.transform(df_val[feature_cols].values)\n",
    "df_test[feature_cols]  = scaler_X.transform(df_test[feature_cols].values)\n",
    "\n",
    "print(\"[PREP] Feature StandardScaler 적용 완료\")\n",
    "\n",
    "# 6-2) 회귀 타깃 표준화 (time_to_next_log1p, remain_los)\n",
    "if \"target_remain_los\" not in df_train.columns:\n",
    "    raise ValueError(\"target_remain_los 컬럼이 없습니다.\")\n",
    "\n",
    "ttn_mean = df_train[\"time_to_next_log1p\"].mean()\n",
    "ttn_std  = df_train[\"time_to_next_log1p\"].std() + 1e-8\n",
    "\n",
    "los_mean = df_train[\"target_remain_los\"].mean()\n",
    "los_std  = df_train[\"target_remain_los\"].std() + 1e-8\n",
    "\n",
    "def add_target_std(df_in):\n",
    "    df_out = df_in.copy()\n",
    "    df_out[\"time_to_next_std\"] = (df_out[\"time_to_next_log1p\"] - ttn_mean) / ttn_std\n",
    "    df_out[\"remain_los_std\"]   = (df_out[\"target_remain_los\"] - los_mean) / los_std\n",
    "    return df_out\n",
    "\n",
    "df_train = add_target_std(df_train)\n",
    "df_val   = add_target_std(df_val)\n",
    "df_test  = add_target_std(df_test)\n",
    "\n",
    "print(\"[PREP] 회귀 타깃 표준화 완료 (time_to_next_std, remain_los_std)\")\n",
    "\n",
    "# ================================\n",
    "# 7. hadm_id 단위 시퀀스 생성 (5 task)\n",
    "# ================================\n",
    "def build_sequences_multi(df_split: pd.DataFrame, feature_cols, max_seq_len: int):\n",
    "    X_seqs = []\n",
    "    M_seqs = []\n",
    "    y_mort_list  = []\n",
    "    y_next_list  = []\n",
    "    y_ttn_list   = []\n",
    "    y_los_list   = []\n",
    "    y_delay_list = []\n",
    "\n",
    "    sort_key = \"time_since_start_min\" if \"time_since_start_min\" in df_split.columns else \"prefix_len\"\n",
    "\n",
    "    for _, g in df_split.groupby(\"hadm_id\"):\n",
    "        g = g.sort_values(sort_key)\n",
    "\n",
    "        seq_X = g[feature_cols].values.astype(np.float32)\n",
    "\n",
    "        y_mort  = g[\"target_mortality\"].values.astype(np.int64)\n",
    "        y_next  = (g[\"target_next_evt\"].values - 1).astype(np.int64)  # 1~K → 0~K-1\n",
    "        y_ttn   = g[\"time_to_next_std\"].values.astype(np.float32)\n",
    "        y_los   = g[\"remain_los_std\"].values.astype(np.float32)\n",
    "        y_delay = g[\"delay_label\"].values.astype(np.int64)\n",
    "\n",
    "        T = len(seq_X)\n",
    "        T_use = min(T, max_seq_len)\n",
    "\n",
    "        seq_X = seq_X[:T_use]\n",
    "        y_mort = y_mort[:T_use]\n",
    "        y_next = y_next[:T_use]\n",
    "        y_ttn  = y_ttn[:T_use]\n",
    "        y_los  = y_los[:T_use]\n",
    "        y_delay = y_delay[:T_use]\n",
    "\n",
    "        pad_len = max_seq_len - T_use\n",
    "\n",
    "        if pad_len > 0:\n",
    "            pad_X = np.zeros((pad_len, seq_X.shape[1]), dtype=np.float32)\n",
    "            seq_X = np.concatenate([seq_X, pad_X], axis=0)\n",
    "\n",
    "            pad_int = np.zeros((pad_len,), dtype=np.int64)\n",
    "            pad_float = np.zeros((pad_len,), dtype=np.float32)\n",
    "\n",
    "            y_mort  = np.concatenate([y_mort, pad_int], axis=0)\n",
    "            y_next  = np.concatenate([y_next, pad_int], axis=0)\n",
    "            y_ttn   = np.concatenate([y_ttn,  pad_float], axis=0)\n",
    "            y_los   = np.concatenate([y_los,  pad_float], axis=0)\n",
    "            y_delay = np.concatenate([y_delay, pad_int], axis=0)\n",
    "\n",
    "        mask = np.zeros((max_seq_len,), dtype=np.float32)\n",
    "        mask[:T_use] = 1.0\n",
    "\n",
    "        X_seqs.append(seq_X)\n",
    "        M_seqs.append(mask)\n",
    "        y_mort_list.append(y_mort)\n",
    "        y_next_list.append(y_next)\n",
    "        y_ttn_list.append(y_ttn)\n",
    "        y_los_list.append(y_los)\n",
    "        y_delay_list.append(y_delay)\n",
    "\n",
    "    X_seqs = np.stack(X_seqs, axis=0)\n",
    "    M_seqs = np.stack(M_seqs, axis=0)\n",
    "    y_mort_arr  = np.stack(y_mort_list, axis=0)\n",
    "    y_next_arr  = np.stack(y_next_list, axis=0)\n",
    "    y_ttn_arr   = np.stack(y_ttn_list, axis=0)\n",
    "    y_los_arr   = np.stack(y_los_list, axis=0)\n",
    "    y_delay_arr = np.stack(y_delay_list, axis=0)\n",
    "\n",
    "    return X_seqs, M_seqs, y_mort_arr, y_next_arr, y_ttn_arr, y_los_arr, y_delay_arr\n",
    "\n",
    "X_train_np, M_train_np, y_mort_tr, y_next_tr, y_ttn_tr, y_los_tr, y_delay_tr = build_sequences_multi(df_train, feature_cols, MAX_SEQ_LEN)\n",
    "X_val_np,   M_val_np,   y_mort_va, y_next_va, y_ttn_va, y_los_va, y_delay_va = build_sequences_multi(df_val,   feature_cols, MAX_SEQ_LEN)\n",
    "X_test_np,  M_test_np,  y_mort_te, y_next_te, y_ttn_te, y_los_te, y_delay_te = build_sequences_multi(df_test,  feature_cols, MAX_SEQ_LEN)\n",
    "\n",
    "print(\"[SEQ] Train:\", X_train_np.shape, y_mort_tr.shape)\n",
    "print(\"[SEQ] Val  :\", X_val_np.shape,   y_mort_va.shape)\n",
    "print(\"[SEQ] Test :\", X_test_np.shape,  y_mort_te.shape)\n",
    "\n",
    "# ================================\n",
    "# 8. Dataset / DataLoader\n",
    "# ================================\n",
    "class PPMSeqDataset(Dataset):\n",
    "    def __init__(self, X, M, y_mort, y_next, y_ttn, y_los, y_delay):\n",
    "        self.X = X\n",
    "        self.M = M\n",
    "        self.y_mort = y_mort\n",
    "        self.y_next = y_next\n",
    "        self.y_ttn = y_ttn\n",
    "        self.y_los = y_los\n",
    "        self.y_delay = y_delay\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.from_numpy(self.X[idx]),\n",
    "            torch.from_numpy(self.M[idx]),\n",
    "            torch.from_numpy(self.y_mort[idx]),\n",
    "            torch.from_numpy(self.y_next[idx]),\n",
    "            torch.from_numpy(self.y_ttn[idx]),\n",
    "            torch.from_numpy(self.y_los[idx]),\n",
    "            torch.from_numpy(self.y_delay[idx]),\n",
    "        )\n",
    "\n",
    "train_dataset = PPMSeqDataset(X_train_np, M_train_np, y_mort_tr, y_next_tr, y_ttn_tr, y_los_tr, y_delay_tr)\n",
    "val_dataset   = PPMSeqDataset(X_val_np,   M_val_np,   y_mort_va, y_next_va, y_ttn_va, y_los_va, y_delay_va)\n",
    "test_dataset  = PPMSeqDataset(X_test_np,  M_test_np,  y_mort_te, y_next_te, y_ttn_te, y_los_te, y_delay_te)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# ================================\n",
    "# 9. Transformer 모델 정의 (5-task head, 모델 용량 살짝 증가)\n",
    "# ================================\n",
    "class PPMTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, num_next_classes, d_model=128, nhead=4, num_layers=3, dim_ff=256, max_len=MAX_SEQ_LEN):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        self.pos_embedding = nn.Embedding(max_len, d_model)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_ff,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Task-specific heads\n",
    "        self.mort_head  = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, 1)\n",
    "        )\n",
    "        self.delay_head = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, 1)\n",
    "        )\n",
    "        self.next_head  = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, num_next_classes)\n",
    "        )\n",
    "        self.ttn_head   = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, 1)\n",
    "        )\n",
    "        self.los_head   = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        B, T, _ = x.size()\n",
    "        h = self.input_proj(x)\n",
    "\n",
    "        positions = torch.arange(T, device=x.device).unsqueeze(0).expand(B, T)\n",
    "        pos_emb = self.pos_embedding(positions)\n",
    "        h = h + pos_emb\n",
    "\n",
    "        src_key_padding_mask = (mask == 0)\n",
    "        h_enc = self.encoder(h, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "        mort_logits  = self.mort_head(h_enc).squeeze(-1)\n",
    "        delay_logits = self.delay_head(h_enc).squeeze(-1)\n",
    "        next_logits  = self.next_head(h_enc)\n",
    "        ttn_pred     = self.ttn_head(h_enc).squeeze(-1)\n",
    "        los_pred     = self.los_head(h_enc).squeeze(-1)\n",
    "\n",
    "        return {\n",
    "            \"mort_logits\": mort_logits,\n",
    "            \"delay_logits\": delay_logits,\n",
    "            \"next_logits\": next_logits,\n",
    "            \"ttn_pred\": ttn_pred,\n",
    "            \"los_pred\": los_pred,\n",
    "        }\n",
    "\n",
    "input_dim = len(feature_cols)\n",
    "model = PPMTransformer(input_dim=input_dim, num_next_classes=num_next_classes).to(device)\n",
    "\n",
    "# ================================\n",
    "# 10. loss 함수 및 optimizer\n",
    "#    - 사망/지연: pos_weight BCEWithLogits\n",
    "#    - 회귀: MSE (표준화된 타깃 기준)\n",
    "#    - 태스크별 loss 가중치 강화\n",
    "# ================================\n",
    "mort_pos_ratio = df_train[\"target_mortality\"].mean()\n",
    "delay_pos_ratio = df_train[\"delay_label\"].mean()\n",
    "\n",
    "pos_weight_mort = (1.0 - mort_pos_ratio) / max(mort_pos_ratio, 1e-6)\n",
    "pos_weight_delay = (1.0 - delay_pos_ratio) / max(delay_pos_ratio, 1e-6)\n",
    "\n",
    "print(f\"[INFO] mort_pos_ratio={mort_pos_ratio:.4f}, pos_weight_mort={pos_weight_mort:.2f}\")\n",
    "print(f\"[INFO] delay_pos_ratio={delay_pos_ratio:.4f}, pos_weight_delay={pos_weight_delay:.2f}\")\n",
    "\n",
    "bce_mort = nn.BCEWithLogitsLoss(\n",
    "    pos_weight=torch.tensor([pos_weight_mort], device=device),\n",
    "    reduction=\"none\",\n",
    ")\n",
    "bce_delay = nn.BCEWithLogitsLoss(\n",
    "    pos_weight=torch.tensor([pos_weight_delay], device=device),\n",
    "    reduction=\"none\",\n",
    ")\n",
    "ce_loss  = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "mse_loss = nn.MSELoss(reduction=\"none\")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# ================================\n",
    "# 11. metric 함수들\n",
    "# ================================\n",
    "def compute_metrics_binary(y_true, y_proba, threshold=0.5):\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "    if len(np.unique(y_true)) > 1:\n",
    "        auc = roc_auc_score(y_true, y_proba)\n",
    "    else:\n",
    "        auc = np.nan\n",
    "    ap  = average_precision_score(y_true, y_proba)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1  = f1_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred)\n",
    "    rec  = recall_score(y_true, y_pred)\n",
    "    return dict(AUC=auc, AP=ap, ACC=acc, PREC=prec, REC=rec, F1=f1)\n",
    "\n",
    "def compute_metrics_reg(y_true, y_pred):\n",
    "    mse  = mean_squared_error(y_true, y_pred)\n",
    "    rmse = mse ** 0.5\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    return dict(RMSE=rmse, MAE=mae)\n",
    "\n",
    "def compute_metrics_multiclass(y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    macro_f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    macro_prec = precision_score(y_true, y_pred, average=\"macro\")\n",
    "    macro_rec  = recall_score(y_true, y_pred, average=\"macro\")\n",
    "    return dict(ACC=acc, PREC=macro_prec, REC=macro_rec, F1=macro_f1)\n",
    "\n",
    "def eval_epoch(model, loader, threshold=0.5):\n",
    "    model.eval()\n",
    "    all_mort_y, all_mort_p = [], []\n",
    "    all_delay_y, all_delay_p = [], []\n",
    "    all_next_y, all_next_pred = [], []\n",
    "    all_ttn_y_std, all_ttn_pred_std = [], []\n",
    "    all_los_y_std, all_los_pred_std = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, mb, y_mort_b, y_next_b, y_ttn_b, y_los_b, y_delay_b in loader:\n",
    "            xb = xb.to(device).float()\n",
    "            mb = mb.to(device).float()\n",
    "            y_mort_b = y_mort_b.to(device).float()\n",
    "            y_next_b = y_next_b.to(device).long()\n",
    "            y_ttn_b = y_ttn_b.to(device).float()\n",
    "            y_los_b = y_los_b.to(device).float()\n",
    "            y_delay_b = y_delay_b.to(device).float()\n",
    "\n",
    "            out = model(xb, mb)\n",
    "            mort_logits  = out[\"mort_logits\"]\n",
    "            delay_logits = out[\"delay_logits\"]\n",
    "            next_logits  = out[\"next_logits\"]\n",
    "            ttn_pred     = out[\"ttn_pred\"]\n",
    "            los_pred     = out[\"los_pred\"]\n",
    "\n",
    "            mask_flat = mb.view(-1) > 0\n",
    "            if mask_flat.sum() == 0:\n",
    "                continue\n",
    "\n",
    "            mort_p = torch.sigmoid(mort_logits.view(-1)[mask_flat])\n",
    "            delay_p = torch.sigmoid(delay_logits.view(-1)[mask_flat])\n",
    "            next_pred = torch.argmax(next_logits.view(-1, num_next_classes)[mask_flat], dim=1)\n",
    "            ttn_p_std = ttn_pred.view(-1)[mask_flat]\n",
    "            los_p_std = los_pred.view(-1)[mask_flat]\n",
    "\n",
    "            mort_y = y_mort_b.view(-1)[mask_flat]\n",
    "            delay_y = y_delay_b.view(-1)[mask_flat]\n",
    "            next_y = y_next_b.view(-1)[mask_flat]\n",
    "            ttn_y_std = y_ttn_b.view(-1)[mask_flat]\n",
    "            los_y_std = y_los_b.view(-1)[mask_flat]\n",
    "\n",
    "            all_mort_y.append(mort_y.cpu().numpy())\n",
    "            all_mort_p.append(mort_p.cpu().numpy())\n",
    "            all_delay_y.append(delay_y.cpu().numpy())\n",
    "            all_delay_p.append(delay_p.cpu().numpy())\n",
    "            all_next_y.append(next_y.cpu().numpy())\n",
    "            all_next_pred.append(next_pred.cpu().numpy())\n",
    "            all_ttn_y_std.append(ttn_y_std.cpu().numpy())\n",
    "            all_ttn_pred_std.append(ttn_p_std.cpu().numpy())\n",
    "            all_los_y_std.append(los_y_std.cpu().numpy())\n",
    "            all_los_pred_std.append(los_p_std.cpu().numpy())\n",
    "\n",
    "    def concat_or_empty(lst):\n",
    "        return np.concatenate(lst, axis=0) if len(lst) > 0 else np.array([])\n",
    "\n",
    "    mort_y = concat_or_empty(all_mort_y)\n",
    "    mort_p = concat_or_empty(all_mort_p)\n",
    "    delay_y = concat_or_empty(all_delay_y)\n",
    "    delay_p = concat_or_empty(all_delay_p)\n",
    "    next_y = concat_or_empty(all_next_y)\n",
    "    next_pred = concat_or_empty(all_next_pred)\n",
    "    ttn_y_std = concat_or_empty(all_ttn_y_std)\n",
    "    ttn_pred_std = concat_or_empty(all_ttn_pred_std)\n",
    "    los_y_std = concat_or_empty(all_los_y_std)\n",
    "    los_pred_std = concat_or_empty(all_los_pred_std)\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    if len(mort_y) > 0:\n",
    "        metrics[\"mortality\"] = compute_metrics_binary(mort_y, mort_p, threshold)\n",
    "\n",
    "    if len(delay_y) > 0:\n",
    "        metrics[\"delay\"] = compute_metrics_binary(delay_y, delay_p, threshold)\n",
    "\n",
    "    if len(next_y) > 0:\n",
    "        metrics[\"next_event\"] = compute_metrics_multiclass(next_y, next_pred)\n",
    "\n",
    "    # 회귀는 표준화 → log1p/일 단위로 역변환 후 RMSE 계산\n",
    "    if len(ttn_y_std) > 0:\n",
    "        y_true_log1p = ttn_y_std * ttn_std + ttn_mean\n",
    "        y_pred_log1p = ttn_pred_std * ttn_std + ttn_mean\n",
    "        metrics[\"time_to_next\"] = compute_metrics_reg(y_true_log1p, y_pred_log1p)\n",
    "\n",
    "    if len(los_y_std) > 0:\n",
    "        y_true_los = los_y_std * los_std + los_mean\n",
    "        y_pred_los = los_pred_std * los_std + los_mean\n",
    "        metrics[\"remain_los\"] = compute_metrics_reg(y_true_los, y_pred_los)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# ================================\n",
    "# 12. Training Loop (loss 가중치 강화 + early-best by Val AUC)\n",
    "# ================================\n",
    "best_val_auc = -1.0\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for xb, mb, y_mort_b, y_next_b, y_ttn_b, y_los_b, y_delay_b in train_loader:\n",
    "        xb = xb.to(device).float()\n",
    "        mb = mb.to(device).float()\n",
    "        y_mort_b = y_mort_b.to(device).float()\n",
    "        y_next_b = y_next_b.to(device).long()\n",
    "        y_ttn_b = y_ttn_b.to(device).float()\n",
    "        y_los_b = y_los_b.to(device).float()\n",
    "        y_delay_b = y_delay_b.to(device).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(xb, mb)\n",
    "\n",
    "        mort_logits  = out[\"mort_logits\"]\n",
    "        delay_logits = out[\"delay_logits\"]\n",
    "        next_logits  = out[\"next_logits\"]\n",
    "        ttn_pred     = out[\"ttn_pred\"]\n",
    "        los_pred     = out[\"los_pred\"]\n",
    "\n",
    "        mask_flat = mb.view(-1) > 0\n",
    "        if mask_flat.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        mort_logits_flat  = mort_logits.view(-1)[mask_flat]\n",
    "        delay_logits_flat = delay_logits.view(-1)[mask_flat]\n",
    "        next_logits_flat  = next_logits.view(-1, num_next_classes)[mask_flat]\n",
    "        ttn_pred_flat     = ttn_pred.view(-1)[mask_flat]\n",
    "        los_pred_flat     = los_pred.view(-1)[mask_flat]\n",
    "\n",
    "        y_mort_flat  = y_mort_b.view(-1)[mask_flat]\n",
    "        y_delay_flat = y_delay_b.view(-1)[mask_flat]\n",
    "        y_next_flat  = y_next_b.view(-1)[mask_flat]\n",
    "        y_ttn_flat   = y_ttn_b.view(-1)[mask_flat]\n",
    "        y_los_flat   = y_los_b.view(-1)[mask_flat]\n",
    "\n",
    "        loss_mort  = bce_mort(mort_logits_flat, y_mort_flat).mean()\n",
    "        loss_delay = bce_delay(delay_logits_flat, y_delay_flat).mean()\n",
    "        loss_next  = ce_loss(next_logits_flat, y_next_flat).mean()\n",
    "        loss_ttn   = mse_loss(ttn_pred_flat, y_ttn_flat).mean()\n",
    "        loss_los   = mse_loss(los_pred_flat, y_los_flat).mean()\n",
    "\n",
    "        # 태스크별 가중치 (사망/다음 이벤트를 더 강하게, 회귀는 상대적으로 줄임)\n",
    "        loss = (\n",
    "            4.0 * loss_mort +\n",
    "            2.0 * loss_delay +\n",
    "            4.0 * loss_next +\n",
    "            1.0 * loss_ttn +\n",
    "            1.0 * loss_los\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / max(n_batches, 1)\n",
    "    val_metrics = eval_epoch(model, val_loader, threshold=0.5)\n",
    "    val_auc = val_metrics.get(\"mortality\", {}).get(\"AUC\", np.nan)\n",
    "\n",
    "    print(f\"[Epoch {epoch:02d}] TrainLoss={avg_loss:.4f} | \"\n",
    "          f\"Val mortality AUC={val_auc:.4f}\")\n",
    "\n",
    "    if not np.isnan(val_auc) and val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict({k: v.to(device) for k, v in best_state.items()})\n",
    "    print(f\"[INFO] Best Val mortality AUC: {best_val_auc:.4f} 모델 로드 완료\")\n",
    "\n",
    "# ================================\n",
    "# 13. 최종 평가\n",
    "# ================================\n",
    "print(\"\\n[FINAL EVAL]\")\n",
    "\n",
    "train_metrics = eval_epoch(model, train_loader)\n",
    "val_metrics   = eval_epoch(model, val_loader)\n",
    "test_metrics  = eval_epoch(model, test_loader)\n",
    "\n",
    "print(\"Train:\", train_metrics)\n",
    "print(\"Val  :\", val_metrics)\n",
    "print(\"Test :\", test_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c83ffef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
