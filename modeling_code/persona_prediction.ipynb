{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec6adf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] device: cpu\n",
      "[LOAD] 데이터 로딩 완료\n",
      " - shape : (40817, 27)\n",
      "[PREP] race -> race_enc 인코딩 완료\n",
      "[PREP] delay_label 생성 완료 (75% 기준: 168.10)\n",
      "[INFO] 사용 feature 수: 26\n",
      "[INFO] Feature 예시: ['age', 'gender', 'race_enc', 'arrival_transport', 'prefix_len', 'current_event_id', 'time_since_start_min', 'time_since_ed', 'time_since_last', 'is_night']\n",
      "[PREP] 결측 제거 후: (40817, 39)\n",
      "[SPLIT] hadm_id 기준 분할 완료\n",
      " - 전체 hadm_id: 1929\n",
      " - train hadm_id: 1350 , rows: 28412\n",
      " - val   hadm_id: 289 , rows: 5723\n",
      " - test  hadm_id: 290 , rows: 6682\n",
      "[TRAIN] LGBM Mortality 학습\n",
      "[LightGBM] [Info] Number of positive: 2300, number of negative: 26112\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000992 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2256\n",
      "[LightGBM] [Info] Number of data points in the train set: 28412, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[SEQ] Train: (1350, 128, 26), Val: (289, 128, 26), Test: (290, 128, 26)\n",
      "[INFO] delay_pos_ratio=0.2489, pos_weight_delay=3.02\n",
      "[Epoch 01] TrainLoss=17.2762\n",
      "[Epoch 02] TrainLoss=8.5662\n",
      "[Epoch 03] TrainLoss=6.8687\n",
      "[Epoch 04] TrainLoss=5.6992\n",
      "[Epoch 05] TrainLoss=4.7607\n",
      "[Epoch 06] TrainLoss=4.1960\n",
      "[Epoch 07] TrainLoss=3.7809\n",
      "[Epoch 08] TrainLoss=3.4304\n",
      "[Epoch 09] TrainLoss=3.1806\n",
      "[Epoch 10] TrainLoss=2.8996\n",
      "[EVAL] df_pred 생성 완료: (6558, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVE] 테스트셋 예측 CSV 저장 완료: ./ppm_test_pred_with_pairs.csv\n",
      "\n",
      "[INFO] 선택된 환자 (accuracy 1이 아닌 환자 중 최고, 없으면 전체 최고)\n",
      " subject_id: 17953273\n",
      " hadm_id   : 23590849\n",
      " accuracy  : 0.9412\n",
      "\n",
      "[ONLINE PRED TABLE] hadm_id=23590849, event 개수=17\n",
      " step  prefix_len  time_since_start_min  current_event_id current_event_name  mortality_prob  delay_prob  next_event_id next_event_name  next_event_prob  time_to_next_min  remain_los_days\n",
      "    1           1                   0.0                 1          ECG_TAKEN          0.0843      0.0111              3      TROP_TAKEN           0.9302              1.74             4.76\n",
      "    2           2                   3.6                 3         TROP_TAKEN          0.1133      0.1251              3      TROP_TAKEN           0.5714              5.97             3.07\n",
      "    3           3                   3.6                 3         TROP_TAKEN          0.0853      0.0210              3      TROP_TAKEN           0.8534              1.05             3.61\n",
      "    4           4                   3.6                 3         TROP_TAKEN          0.0866      0.0170              3      TROP_TAKEN           0.8967              0.91             3.99\n",
      "    5           5                   3.6                 3         TROP_TAKEN          0.0806      0.0158              3      TROP_TAKEN           0.9145              0.88             4.19\n",
      "    6           6                   3.6                 3         TROP_TAKEN          0.0725      0.0155              3      TROP_TAKEN           0.9243              0.87             4.33\n",
      "    7           7                   3.6                 3         TROP_TAKEN          0.0713      0.0154              3      TROP_TAKEN           0.9307              0.86             4.43\n",
      "    8           8                   3.6                 3         TROP_TAKEN          0.0413      0.0155              3      TROP_TAKEN           0.9353              0.86             4.50\n",
      "    9           9                   3.6                 3         TROP_TAKEN          0.0314      0.0159              3      TROP_TAKEN           0.9389              0.86             4.57\n",
      "   10          10                   3.6                 3         TROP_TAKEN          0.0282      0.0163              3      TROP_TAKEN           0.9420              0.88             4.63\n",
      "   11          11                   3.6                 3         TROP_TAKEN          0.0234      0.0169              3      TROP_TAKEN           0.9447              0.90             4.69\n",
      "   12          12                   3.6                 3         TROP_TAKEN          0.0234      0.0174              3      TROP_TAKEN           0.9473              0.92             4.76\n",
      "   13          13                   3.6                 3         TROP_TAKEN          0.0234      0.0180              3      TROP_TAKEN           0.9497              0.94             4.83\n",
      "   14          14                   3.6                 2     ECG_STEMI_FLAG          0.0345      0.0315              3      TROP_TAKEN           0.9694              0.62             4.90\n",
      "   15          15                   3.6                 3         TROP_TAKEN          0.0267      0.0277              3      TROP_TAKEN           0.9509              1.33             4.76\n",
      "   16          16                   6.3                 9              EVT_9          0.0177      0.2169             14          EVT_14           0.9342              9.73             1.85\n",
      "   17          17                   8.3                11             EVT_11          0.0009      0.0527             14          EVT_14           0.9894              0.67             0.18\n",
      "[SAVE] 온라인 예측 로그 저장 완료: ./online_pred_best_nonperfect_23590849.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import lightgbm as lgb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 0. 공통 설정\n",
    "# =========================================\n",
    "DATA_PATH = \"./../cohort/cohort_ver151_reorder_col.csv\"\n",
    "RANDOM_STATE = 42\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 10\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "set_seed(RANDOM_STATE)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[INFO] device: {device}\")\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 1. 데이터 로딩 및 기본 전처리\n",
    "# =========================================\n",
    "df = pd.read_csv(DATA_PATH, low_memory=False)\n",
    "print(\"[LOAD] 데이터 로딩 완료\")\n",
    "print(\" - shape :\", df.shape)\n",
    "\n",
    "required_cols = [\n",
    "    \"hadm_id\",\n",
    "    \"target_mortality\",\n",
    "    \"target_next_evt\",\n",
    "    \"target_time_to_next\",\n",
    "    \"target_remain_los\",\n",
    "]\n",
    "for c in required_cols:\n",
    "    if c not in df.columns:\n",
    "        raise ValueError(f\"필수 컬럼 {c} 이(가) 없습니다: {c}\")\n",
    "\n",
    "if \"subject_id\" not in df.columns:\n",
    "    df[\"subject_id\"] = df[\"hadm_id\"]\n",
    "\n",
    "# race 인코딩\n",
    "if \"race\" in df.columns:\n",
    "    le_race = LabelEncoder()\n",
    "    df[\"race_enc\"] = le_race.fit_transform(df[\"race\"].astype(str))\n",
    "    print(\"[PREP] race -> race_enc 인코딩 완료\")\n",
    "else:\n",
    "    df[\"race_enc\"] = 0\n",
    "    print(\"[WARN] race 컬럼 없음, race_enc=0으로 채움\")\n",
    "\n",
    "# time_to_next 클리핑 + log1p\n",
    "clip_value = df[\"target_time_to_next\"].quantile(0.995)\n",
    "df[\"time_to_next_clip\"] = df[\"target_time_to_next\"].clip(upper=clip_value)\n",
    "df[\"time_to_next_log1p\"] = np.log1p(df[\"time_to_next_clip\"])\n",
    "\n",
    "# delay_label (75% 기준)\n",
    "if \"delay_label\" not in df.columns:\n",
    "    delay_thr = df[\"time_to_next_clip\"].quantile(0.75)\n",
    "    df[\"delay_label\"] = (df[\"time_to_next_clip\"] > delay_thr).astype(int)\n",
    "    print(f\"[PREP] delay_label 생성 완료 (75% 기준: {delay_thr:.2f})\")\n",
    "else:\n",
    "    print(\"[PREP] 기존 delay_label 사용\")\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 2. Feature Engineering\n",
    "# =========================================\n",
    "def add_features(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_fe = df_in.copy()\n",
    "\n",
    "    if \"current_heart_rate\" in df_fe.columns and \"current_mean_bp\" in df_fe.columns:\n",
    "        df_fe[\"hr_bp_ratio\"] = df_fe[\"current_heart_rate\"] / (df_fe[\"current_mean_bp\"].abs() + 1.0)\n",
    "    else:\n",
    "        df_fe[\"hr_bp_ratio\"] = 0.0\n",
    "\n",
    "    if \"time_since_last\" in df_fe.columns and \"time_since_start_min\" in df_fe.columns:\n",
    "        df_fe[\"time_last_ratio\"] = df_fe[\"time_since_last\"] / (df_fe[\"time_since_start_min\"].abs() + 1.0)\n",
    "    else:\n",
    "        df_fe[\"time_last_ratio\"] = 0.0\n",
    "\n",
    "    if \"pathway_stage\" in df_fe.columns and \"prefix_len\" in df_fe.columns:\n",
    "        df_fe[\"event_progress\"] = df_fe[\"pathway_stage\"] / (df_fe[\"prefix_len\"] + 1.0)\n",
    "    else:\n",
    "        df_fe[\"event_progress\"] = 0.0\n",
    "\n",
    "    if \"time_since_last\" in df.columns:\n",
    "        global_median = df[\"time_since_last\"].median()\n",
    "        df_fe[\"risk_delay\"] = (df_fe[\"time_since_last\"] > global_median).astype(int)\n",
    "    else:\n",
    "        df_fe[\"risk_delay\"] = 0\n",
    "\n",
    "    has_stemi = \"stemi_flag\" in df_fe.columns\n",
    "    has_cum_stemi = \"cum_stemi_cnt\" in df_fe.columns\n",
    "    if has_stemi and has_cum_stemi:\n",
    "        df_fe[\"risk_stemi\"] = df_fe[\"stemi_flag\"] * df_fe[\"cum_stemi_cnt\"]\n",
    "    elif has_stemi:\n",
    "        df_fe[\"risk_stemi\"] = df_fe[\"stemi_flag\"]\n",
    "    else:\n",
    "        df_fe[\"risk_stemi\"] = 0\n",
    "\n",
    "    if \"last_trop\" in df_fe.columns:\n",
    "        df_fe[\"trop_abnormal\"] = (df_fe[\"last_trop\"] > 0.04).astype(int)\n",
    "    else:\n",
    "        df_fe[\"trop_abnormal\"] = 0\n",
    "\n",
    "    return df_fe\n",
    "\n",
    "\n",
    "df = add_features(df)\n",
    "\n",
    "# next_event 0-index\n",
    "df[\"next_evt_label\"] = df[\"target_next_evt\"].astype(int)\n",
    "min_next_label = df[\"next_evt_label\"].min()\n",
    "df[\"next_evt_label0\"] = df[\"next_evt_label\"] - min_next_label\n",
    "num_next_classes = df[\"next_evt_label0\"].max() + 1\n",
    "\n",
    "candidate_cols = [\n",
    "    \"age\", \"gender\", \"race_enc\",\n",
    "    \"arrival_transport\",\n",
    "    \"prefix_len\", \"current_event_id\",\n",
    "    \"time_since_start_min\", \"time_since_ed\", \"time_since_last\",\n",
    "    \"is_night\",\n",
    "    \"cum_ecg_cnt\", \"cum_trop_cnt\", \"cum_stemi_cnt\",\n",
    "    \"stemi_flag\", \"trop_pos_flag\",\n",
    "    \"last_trop\", \"run_max_trop\", \"trop_trend\",\n",
    "    \"pci_status\",\n",
    "    \"current_heart_rate\", \"current_mean_bp\",\n",
    "    \"hr_bp_ratio\", \"time_last_ratio\", \"event_progress\",\n",
    "    \"risk_delay\", \"risk_stemi\", \"trop_abnormal\",\n",
    "]\n",
    "\n",
    "feature_cols = [c for c in candidate_cols if c in df.columns]\n",
    "print(\"[INFO] 사용 feature 수:\", len(feature_cols))\n",
    "print(\"[INFO] Feature 예시:\", feature_cols[:10])\n",
    "\n",
    "target_cols = {\n",
    "    \"mortality\": \"target_mortality\",\n",
    "    \"delay\": \"delay_label\",\n",
    "    \"next_event0\": \"next_evt_label0\",\n",
    "    \"time_to_next_log1p\": \"time_to_next_log1p\",\n",
    "    \"remain_los\": \"target_remain_los\",\n",
    "}\n",
    "\n",
    "df = df.dropna(subset=feature_cols + list(target_cols.values()))\n",
    "print(\"[PREP] 결측 제거 후:\", df.shape)\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 3. hadm_id 기반 Split\n",
    "# =========================================\n",
    "def split_by_hadm(df_in, random_state=42, train_ratio=0.7, val_ratio=0.15):\n",
    "    hadm_ids = df_in[\"hadm_id\"].unique()\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    rng.shuffle(hadm_ids)\n",
    "\n",
    "    n = len(hadm_ids)\n",
    "    n_train = int(n * train_ratio)\n",
    "    n_val = int(n * val_ratio)\n",
    "\n",
    "    hadm_train = hadm_ids[:n_train]\n",
    "    hadm_val = hadm_ids[n_train:n_train + n_val]\n",
    "    hadm_test = hadm_ids[n_train + n_val:]\n",
    "\n",
    "    df_train = df_in[df_in[\"hadm_id\"].isin(hadm_train)].copy()\n",
    "    df_val = df_in[df_in[\"hadm_id\"].isin(hadm_val)].copy()\n",
    "    df_test = df_in[df_in[\"hadm_id\"].isin(hadm_test)].copy()\n",
    "\n",
    "    print(\"[SPLIT] hadm_id 기준 분할 완료\")\n",
    "    print(\" - 전체 hadm_id:\", n)\n",
    "    print(\" - train hadm_id:\", len(hadm_train), \", rows:\", len(df_train))\n",
    "    print(\" - val   hadm_id:\", len(hadm_val), \", rows:\", len(df_val))\n",
    "    print(\" - test  hadm_id:\", len(hadm_test), \", rows:\", len(df_test))\n",
    "\n",
    "    return df_train, df_val, df_test\n",
    "\n",
    "\n",
    "df_train, df_val, df_test = split_by_hadm(df, random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 4. LightGBM: 사망 예측 모델\n",
    "# =========================================\n",
    "X_train_lgbm = df_train[feature_cols].values\n",
    "y_train_mort = df_train[target_cols[\"mortality\"]].values\n",
    "\n",
    "mort_lgbm = lgb.LGBMClassifier(\n",
    "    objective=\"binary\",\n",
    "    n_estimators=400,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "print(\"[TRAIN] LGBM Mortality 학습\")\n",
    "mort_lgbm.fit(X_train_lgbm, y_train_mort)\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 5. Transformer용 시퀀스 데이터 구성\n",
    "# =========================================\n",
    "scaler = StandardScaler()\n",
    "# 이름 없는 ndarray로 fit해서 feature name 경고 방지\n",
    "scaler.fit(df_train[feature_cols].values)\n",
    "\n",
    "\n",
    "def build_sequences_with_ids(df_split, feature_cols, max_len=128, scaler=None):\n",
    "    groups = list(df_split.groupby(\"hadm_id\"))\n",
    "    n_seq = len(groups)\n",
    "    d = len(feature_cols)\n",
    "\n",
    "    X_seq = np.zeros((n_seq, max_len, d), dtype=np.float32)\n",
    "    pad_mask = np.ones((n_seq, max_len), dtype=bool)\n",
    "    y_delay = np.zeros((n_seq, max_len), dtype=np.float32)\n",
    "    y_next = np.zeros((n_seq, max_len), dtype=np.int64)\n",
    "    y_ttn = np.zeros((n_seq, max_len), dtype=np.float32)\n",
    "    y_los = np.zeros((n_seq, max_len), dtype=np.float32)\n",
    "    hadm_ids = []\n",
    "\n",
    "    for i, (hid, g) in enumerate(groups):\n",
    "        if \"time_since_start_min\" in g.columns:\n",
    "            g = g.sort_values(\"time_since_start_min\")\n",
    "        elif \"prefix_len\" in g.columns:\n",
    "            g = g.sort_values(\"prefix_len\")\n",
    "        g = g.reset_index(drop=True)\n",
    "\n",
    "        X_raw = g[feature_cols].values.astype(float)\n",
    "        if scaler is not None:\n",
    "            X_raw = scaler.transform(X_raw)\n",
    "\n",
    "        L = min(len(g), max_len)\n",
    "        X_seq[i, :L, :] = X_raw[:L]\n",
    "        pad_mask[i, :L] = False\n",
    "\n",
    "        y_delay[i, :L] = g[target_cols[\"delay\"]].values[:L]\n",
    "        y_next[i, :L] = g[target_cols[\"next_event0\"]].values[:L]\n",
    "        y_ttn[i, :L] = g[target_cols[\"time_to_next_log1p\"]].values[:L]\n",
    "        y_los[i, :L] = g[target_cols[\"remain_los\"]].values[:L]\n",
    "\n",
    "        hadm_ids.append(hid)\n",
    "\n",
    "    return X_seq, pad_mask, y_delay, y_next, y_ttn, y_los, np.array(hadm_ids)\n",
    "\n",
    "\n",
    "X_tr, mask_tr, y_tr_delay, y_tr_next, y_tr_ttn, y_tr_los, hadm_tr = build_sequences_with_ids(\n",
    "    df_train, feature_cols, max_len=MAX_LEN, scaler=scaler\n",
    ")\n",
    "X_va, mask_va, y_va_delay, y_va_next, y_va_ttn, y_va_los, hadm_va = build_sequences_with_ids(\n",
    "    df_val, feature_cols, max_len=MAX_LEN, scaler=scaler\n",
    ")\n",
    "X_te, mask_te, y_te_delay, y_te_next, y_te_ttn, y_te_los, hadm_te = build_sequences_with_ids(\n",
    "    df_test, feature_cols, max_len=MAX_LEN, scaler=scaler\n",
    ")\n",
    "\n",
    "print(f\"[SEQ] Train: {X_tr.shape}, Val: {X_va.shape}, Test: {X_te.shape}\")\n",
    "\n",
    "\n",
    "class PPMDataset(Dataset):\n",
    "    def __init__(self, X, mask, y_delay, y_next, y_ttn, y_los):\n",
    "        self.X = X\n",
    "        self.mask = mask\n",
    "        self.y_delay = y_delay\n",
    "        self.y_next = y_next\n",
    "        self.y_ttn = y_ttn\n",
    "        self.y_los = y_los\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.from_numpy(self.X[idx]),\n",
    "            torch.from_numpy(self.mask[idx]),\n",
    "            torch.from_numpy(self.y_delay[idx]),\n",
    "            torch.from_numpy(self.y_next[idx]),\n",
    "            torch.from_numpy(self.y_ttn[idx]),\n",
    "            torch.from_numpy(self.y_los[idx]),\n",
    "        )\n",
    "\n",
    "\n",
    "train_ds = PPMDataset(X_tr, mask_tr, y_tr_delay, y_tr_next, y_tr_ttn, y_tr_los)\n",
    "val_ds = PPMDataset(X_va, mask_va, y_va_delay, y_va_next, y_va_ttn, y_va_los)\n",
    "test_ds = PPMDataset(X_te, mask_te, y_te_delay, y_te_next, y_te_ttn, y_te_los)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 6. Multi-task Transformer 정의\n",
    "# =========================================\n",
    "class MultiTaskTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_heads, num_layers, num_next_classes, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.in_proj = nn.Linear(input_dim, hidden_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.delay_head = nn.Linear(hidden_dim, 1)\n",
    "        self.next_head = nn.Linear(hidden_dim, num_next_classes)\n",
    "        self.ttn_head = nn.Linear(hidden_dim, 1)\n",
    "        self.los_head = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x, key_padding_mask):\n",
    "        h = self.in_proj(x)\n",
    "        h = self.encoder(h, src_key_padding_mask=key_padding_mask)\n",
    "\n",
    "        delay_logit = self.delay_head(h).squeeze(-1)\n",
    "        next_logits = self.next_head(h)\n",
    "        ttn_out = self.ttn_head(h).squeeze(-1)\n",
    "        los_out = self.los_head(h).squeeze(-1)\n",
    "\n",
    "        return {\n",
    "            \"delay_logit\": delay_logit,\n",
    "            \"next_event_logits\": next_logits,\n",
    "            \"time_to_next\": ttn_out,\n",
    "            \"remain_los\": los_out,\n",
    "        }\n",
    "\n",
    "\n",
    "model = MultiTaskTransformer(\n",
    "    input_dim=len(feature_cols),\n",
    "    hidden_dim=64,\n",
    "    num_heads=4,\n",
    "    num_layers=2,\n",
    "    num_next_classes=num_next_classes,\n",
    "    dropout=0.1,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 7. Transformer 학습\n",
    "# =========================================\n",
    "pos_ratio_delay = df_train[target_cols[\"delay\"]].mean()\n",
    "pos_weight_delay = (1 - pos_ratio_delay) / (pos_ratio_delay + 1e-8)\n",
    "print(f\"[INFO] delay_pos_ratio={pos_ratio_delay:.4f}, pos_weight_delay={pos_weight_delay:.2f}\")\n",
    "\n",
    "bce_delay = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight_delay, device=device))\n",
    "ce_next = nn.CrossEntropyLoss()\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for X, mask, y_delay, y_next, y_ttn, y_los in train_loader:\n",
    "        X = X.to(device)\n",
    "        mask = mask.to(device)\n",
    "        y_delay = y_delay.to(device)\n",
    "        y_next = y_next.to(device)\n",
    "        y_ttn = y_ttn.to(device)\n",
    "        y_los = y_los.to(device)\n",
    "\n",
    "        outputs = model(X, key_padding_mask=mask)\n",
    "\n",
    "        valid = ~mask\n",
    "        if valid.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        delay_logit = outputs[\"delay_logit\"][valid]\n",
    "        loss_delay = bce_delay(delay_logit, y_delay[valid])\n",
    "\n",
    "        next_logits = outputs[\"next_event_logits\"][valid]\n",
    "        loss_next = ce_next(next_logits, y_next[valid])\n",
    "\n",
    "        ttn_pred = outputs[\"time_to_next\"][valid]\n",
    "        loss_ttn = mse_loss(ttn_pred, y_ttn[valid])\n",
    "\n",
    "        los_pred = outputs[\"remain_los\"][valid]\n",
    "        loss_los = mse_loss(los_pred, y_los[valid])\n",
    "\n",
    "        loss = loss_delay + loss_next + 0.5 * loss_ttn + 0.5 * loss_los\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * X.size(0)\n",
    "\n",
    "    train_loss /= len(train_ds)\n",
    "    print(f\"[Epoch {epoch:02d}] TrainLoss={train_loss:.4f}\")\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 8. 온라인 예측용 헬퍼들\n",
    "# =========================================\n",
    "def event_name(event_id: int):\n",
    "    mapping = {\n",
    "        0: \"ED_ARRIVAL\",\n",
    "        1: \"ECG_TAKEN\",\n",
    "        2: \"ECG_STEMI_FLAG\",\n",
    "        3: \"TROP_TAKEN\",\n",
    "        4: \"TROP_POSITIVE\",\n",
    "        5: \"PCI_START\",\n",
    "        6: \"PCI_END\",\n",
    "        7: \"ICU_INTIME\",\n",
    "    }\n",
    "    return mapping.get(event_id, f\"EVT_{event_id}\")\n",
    "\n",
    "\n",
    "def build_patient_sequence(df_all: pd.DataFrame,\n",
    "                           hadm_id: int,\n",
    "                           feature_cols: list,\n",
    "                           scaler,\n",
    "                           max_len: int = 128):\n",
    "    df_pat = df_all[df_all[\"hadm_id\"] == hadm_id].copy()\n",
    "    if df_pat.empty:\n",
    "        raise ValueError(f\"hadm_id={hadm_id} 에 해당하는 row가 없습니다.\")\n",
    "\n",
    "    if \"time_since_start_min\" in df_pat.columns:\n",
    "        df_pat = df_pat.sort_values(\"time_since_start_min\")\n",
    "    elif \"prefix_len\" in df_pat.columns:\n",
    "        df_pat = df_pat.sort_values(\"prefix_len\")\n",
    "    df_pat = df_pat.reset_index(drop=True)\n",
    "\n",
    "    X_raw = df_pat[feature_cols].values.astype(float)\n",
    "    X_scaled = scaler.transform(X_raw)\n",
    "\n",
    "    L, D = X_scaled.shape\n",
    "    if L > max_len:\n",
    "        X_scaled = X_scaled[:max_len]\n",
    "        L = max_len\n",
    "\n",
    "    seq = np.zeros((max_len, D), dtype=np.float32)\n",
    "    seq[:L] = X_scaled\n",
    "\n",
    "    key_padding_mask = np.ones((max_len,), dtype=bool)\n",
    "    key_padding_mask[:L] = False\n",
    "\n",
    "    return df_pat, seq, key_padding_mask\n",
    "\n",
    "\n",
    "def make_prefix_inputs(seq_full: np.ndarray,\n",
    "                       pad_mask_full: np.ndarray,\n",
    "                       t: int,\n",
    "                       device: torch.device):\n",
    "    max_len, D = seq_full.shape\n",
    "\n",
    "    seq_prefix = seq_full.copy()\n",
    "    if t < max_len:\n",
    "        seq_prefix[t:] = 0.0\n",
    "\n",
    "    key_padding_mask = np.ones((max_len,), dtype=bool)\n",
    "    key_padding_mask[:t] = False\n",
    "\n",
    "    seq_tensor = torch.from_numpy(seq_prefix).unsqueeze(0).to(device)\n",
    "    mask_tensor = torch.from_numpy(key_padding_mask).unsqueeze(0).to(device)\n",
    "\n",
    "    return seq_tensor, mask_tensor\n",
    "\n",
    "\n",
    "def decode_transformer_outputs(outputs, step_index: int, min_next_label: int):\n",
    "    delay_logit = outputs[\"delay_logit\"][:, step_index]\n",
    "    delay_prob = torch.sigmoid(delay_logit).item()\n",
    "\n",
    "    next_logits = outputs[\"next_event_logits\"][:, step_index, :]\n",
    "    next_prob = F.softmax(next_logits, dim=-1)\n",
    "    cls0 = int(torch.argmax(next_prob, dim=-1).item())\n",
    "    next_event_id = cls0 + min_next_label\n",
    "    next_event_prob = float(torch.max(next_prob).item())\n",
    "\n",
    "    ttn_pred = outputs[\"time_to_next\"][:, step_index].item()\n",
    "    time_to_next_min = float(np.expm1(ttn_pred))\n",
    "\n",
    "    los_pred = outputs[\"remain_los\"][:, step_index].item()\n",
    "    remain_los_days = float(los_pred)\n",
    "\n",
    "    return {\n",
    "        \"delay_prob\": delay_prob,\n",
    "        \"next_event_id\": next_event_id,\n",
    "        \"next_event_prob\": next_event_prob,\n",
    "        \"time_to_next_min\": time_to_next_min,\n",
    "        \"remain_los_days\": remain_los_days,\n",
    "    }\n",
    "\n",
    "\n",
    "def run_online_prediction_for_hadm(hadm_id: int):\n",
    "    model.eval()\n",
    "\n",
    "    df_pat, seq_full, pad_mask_full = build_patient_sequence(\n",
    "        df_all=df,\n",
    "        hadm_id=hadm_id,\n",
    "        feature_cols=feature_cols,\n",
    "        scaler=scaler,\n",
    "        max_len=MAX_LEN\n",
    "    )\n",
    "\n",
    "    L = len(df_pat)\n",
    "    print(f\"\\n[ONLINE PRED TABLE] hadm_id={hadm_id}, event 개수={L}\")\n",
    "\n",
    "    X_lgbm = df_pat[feature_cols].values\n",
    "    time_col = \"time_since_start_min\" if \"time_since_start_min\" in df_pat.columns else None\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for t in range(1, L + 1):\n",
    "        row = df_pat.iloc[t - 1]\n",
    "\n",
    "        # ===== 실제값(target) =====\n",
    "        true_delay = int(row[target_cols[\"delay\"]])\n",
    "        true_next = int(row[\"target_next_evt\"])\n",
    "        true_ttn = float(row[\"target_time_to_next\"])\n",
    "        true_los = float(row[\"target_remain_los\"])\n",
    "\n",
    "        # ===== LGBM mortality 예측 =====\n",
    "        mort_proba = float(mort_lgbm.predict_proba(X_lgbm[t - 1:t])[0, 1])\n",
    "        pred_mort = int(mort_proba >= 0.5)\n",
    "\n",
    "        # ===== Transformer 예측 =====\n",
    "        seq_tensor, mask_tensor = make_prefix_inputs(\n",
    "            seq_full=seq_full,\n",
    "            pad_mask_full=pad_mask_full,\n",
    "            t=t,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(seq_tensor, key_padding_mask=mask_tensor)\n",
    "\n",
    "        trans_res = decode_transformer_outputs(\n",
    "            outputs,\n",
    "            step_index=t - 1,\n",
    "            min_next_label=min_next_label\n",
    "        )\n",
    "\n",
    "        pred_delay = int(trans_res[\"delay_prob\"] >= 0.5)\n",
    "        pred_next = trans_res[\"next_event_id\"]\n",
    "        pred_ttn = float(trans_res[\"time_to_next_min\"])\n",
    "        pred_los = float(trans_res[\"remain_los_days\"])\n",
    "\n",
    "        ev_id = int(row[\"current_event_id\"]) if \"current_event_id\" in row else -1\n",
    "        ev_name = event_name(ev_id)\n",
    "\n",
    "        if time_col is not None:\n",
    "            time_since_start = float(row[time_col])\n",
    "        else:\n",
    "            time_since_start = np.nan\n",
    "\n",
    "        prefix_len_val = int(row[\"prefix_len\"]) if \"prefix_len\" in row else t\n",
    "\n",
    "        # ===== pair 문자열 생성 =====\n",
    "        mortality_pair = f\"{int(row[target_cols['mortality']])}|{pred_mort}\"\n",
    "        delay_pair = f\"{true_delay}|{pred_delay}\"\n",
    "        next_event_pair = f\"{true_next}|{pred_next}\"\n",
    "        time_to_next_pair = f\"{true_ttn:.2f}|{pred_ttn:.2f}\"\n",
    "        remain_los_pair = f\"{true_los:.2f}|{pred_los:.2f}\"\n",
    "\n",
    "        # ===== row 저장 =====\n",
    "        rows.append({\n",
    "            \"step\": t,\n",
    "            \"prefix_len\": prefix_len_val,\n",
    "            \"time_since_start_min\": round(time_since_start, 1) if not np.isnan(time_since_start) else np.nan,\n",
    "            \"current_event_id\": ev_id,\n",
    "            \"current_event_name\": ev_name,\n",
    "\n",
    "            # 실제값\n",
    "            \"true_mortality\": int(row[target_cols[\"mortality\"]]),\n",
    "            \"true_delay\": true_delay,\n",
    "            \"true_next_event\": true_next,\n",
    "            \"true_time_to_next_min\": true_ttn,\n",
    "            \"true_remain_los_days\": true_los,\n",
    "\n",
    "            # 예측값\n",
    "            \"pred_mortality\": pred_mort,\n",
    "            \"pred_delay\": pred_delay,\n",
    "            \"pred_next_event\": pred_next,\n",
    "            \"pred_time_to_next_min\": round(pred_ttn, 2),\n",
    "            \"pred_remain_los_days\": round(pred_los, 2),\n",
    "\n",
    "            # pair (원본 | 예측)\n",
    "            \"mortality_pair\": mortality_pair,\n",
    "            \"delay_pair\": delay_pair,\n",
    "            \"next_event_pair\": next_event_pair,\n",
    "            \"time_to_next_pair\": time_to_next_pair,\n",
    "            \"remain_los_pair\": remain_los_pair\n",
    "        })\n",
    "\n",
    "    log_df = pd.DataFrame(rows)\n",
    "    print(log_df.to_string(index=False))\n",
    "    return log_df\n",
    "\n",
    "# =========================================\n",
    "# 9. 테스트셋에서 \"원본값|예측값\" CSV 생성\n",
    "# =========================================\n",
    "def build_df_pred_from_test_with_pairs():\n",
    "    model.eval()\n",
    "\n",
    "    # hadm_id -> subject_id\n",
    "    hadm_to_subj = (\n",
    "        df[[\"hadm_id\", \"subject_id\"]]\n",
    "        .drop_duplicates()\n",
    "        .set_index(\"hadm_id\")[\"subject_id\"]\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "    # hadm_id -> mortality(원본)\n",
    "    hadm_to_mort_true = (\n",
    "        df[[\"hadm_id\", \"target_mortality\"]]\n",
    "        .drop_duplicates()\n",
    "        .set_index(\"hadm_id\")[\"target_mortality\"]\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "    # hadm_id -> mortality(예측, LGBM)\n",
    "    tmp = df_test.copy()\n",
    "    if \"time_since_start_min\" in tmp.columns:\n",
    "        tmp = tmp.sort_values([\"hadm_id\", \"time_since_start_min\"])\n",
    "    elif \"prefix_len\" in tmp.columns:\n",
    "        tmp = tmp.sort_values([\"hadm_id\", \"prefix_len\"])\n",
    "    tmp_first = tmp.groupby(\"hadm_id\").head(1).copy()\n",
    "    X_tmp = tmp_first[feature_cols].values\n",
    "    mort_proba = mort_lgbm.predict_proba(X_tmp)[:, 1]\n",
    "    mort_pred_label = (mort_proba >= 0.5).astype(int)\n",
    "\n",
    "    hadm_to_mort_pred = {\n",
    "        hid: pred for hid, pred in zip(tmp_first[\"hadm_id\"].values, mort_pred_label)\n",
    "    }\n",
    "\n",
    "    records = []\n",
    "    global_idx = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, mask, y_delay, y_next, y_ttn, y_los in test_loader:\n",
    "            bs = X.size(0)\n",
    "\n",
    "            X = X.to(device)\n",
    "            mask = mask.to(device)\n",
    "            y_delay = y_delay.to(device)\n",
    "            y_next = y_next.to(device)\n",
    "            y_ttn = y_ttn.to(device)\n",
    "            y_los = y_los.to(device)\n",
    "\n",
    "            outputs = model(X, key_padding_mask=mask)\n",
    "            logits_next = outputs[\"next_event_logits\"]\n",
    "            logits_delay = outputs[\"delay_logit\"]\n",
    "            pred_next0 = torch.argmax(logits_next, dim=-1)\n",
    "            pred_delay_prob = torch.sigmoid(logits_delay)\n",
    "            pred_delay_label = (pred_delay_prob >= 0.5).long()\n",
    "\n",
    "            pred_ttn = outputs[\"time_to_next\"]\n",
    "            pred_los = outputs[\"remain_los\"]\n",
    "\n",
    "            mask_np = mask.cpu().numpy()\n",
    "            y_delay_np = y_delay.cpu().numpy()\n",
    "            y_next_np = y_next.cpu().numpy()\n",
    "            y_ttn_np = y_ttn.cpu().numpy()\n",
    "            y_los_np = y_los.cpu().numpy()\n",
    "\n",
    "            pred_next0_np = pred_next0.cpu().numpy()\n",
    "            pred_delay_label_np = pred_delay_label.cpu().numpy()\n",
    "            pred_ttn_np = pred_ttn.cpu().numpy()\n",
    "            pred_los_np = pred_los.cpu().numpy()\n",
    "\n",
    "            for b in range(bs):\n",
    "                hadm_id = int(hadm_te[global_idx + b])\n",
    "                subject_id = int(hadm_to_subj[hadm_id])\n",
    "                mort_true = int(hadm_to_mort_true[hadm_id])\n",
    "                mort_pred = int(hadm_to_mort_pred[hadm_id])\n",
    "\n",
    "                valid = ~mask_np[b]\n",
    "\n",
    "                true_delay = y_delay_np[b][valid]\n",
    "                true_next0 = y_next_np[b][valid]\n",
    "                true_ttn_log = y_ttn_np[b][valid]\n",
    "                true_ttn_min = np.expm1(true_ttn_log)\n",
    "                true_los = y_los_np[b][valid]\n",
    "\n",
    "                pred_delay = pred_delay_label_np[b][valid]\n",
    "                pred_next0_b = pred_next0_np[b][valid]\n",
    "                pred_ttn_log = pred_ttn_np[b][valid]\n",
    "                pred_ttn_min = np.expm1(pred_ttn_log)\n",
    "                pred_los = pred_los_np[b][valid]\n",
    "\n",
    "                for step_idx in range(len(true_delay)):\n",
    "                    td = int(true_delay[step_idx])\n",
    "                    pd_ = int(pred_delay[step_idx])\n",
    "\n",
    "                    t_next_label0 = int(true_next0[step_idx])\n",
    "                    p_next_label0 = int(pred_next0_b[step_idx])\n",
    "                    t_next_label = t_next_label0 + min_next_label\n",
    "                    p_next_label = p_next_label0 + min_next_label\n",
    "\n",
    "                    t_ttn = float(true_ttn_min[step_idx])\n",
    "                    p_ttn = float(pred_ttn_min[step_idx])\n",
    "\n",
    "                    t_los = float(true_los[step_idx])\n",
    "                    p_los = float(pred_los[step_idx])\n",
    "\n",
    "                    records.append({\n",
    "                        \"subject_id\": subject_id,\n",
    "                        \"hadm_id\": hadm_id,\n",
    "                        \"step\": step_idx + 1,\n",
    "\n",
    "                        # 숫자 원본/예측\n",
    "                        \"target_mortality\": mort_true,\n",
    "                        \"pred_mortality\": mort_pred,\n",
    "\n",
    "                        \"target_delay\": td,\n",
    "                        \"pred_delay\": pd_,\n",
    "\n",
    "                        \"target_next_evt\": t_next_label,\n",
    "                        \"pred_next_evt\": p_next_label,\n",
    "\n",
    "                        \"target_time_to_next_min\": t_ttn,\n",
    "                        \"pred_time_to_next_min\": p_ttn,\n",
    "\n",
    "                        \"target_remain_los_days\": t_los,\n",
    "                        \"pred_remain_los_days\": p_los,\n",
    "\n",
    "                        # 원본값|예측값 문자열\n",
    "                        \"mortality_pair\": f\"{mort_true}|{mort_pred}\",\n",
    "                        \"delay_pair\": f\"{td}|{pd_}\",\n",
    "                        \"next_event_pair\": f\"{t_next_label}|{p_next_label}\",\n",
    "                        \"time_to_next_min_pair\": f\"{t_ttn:.2f}|{p_ttn:.2f}\",\n",
    "                        \"remain_los_days_pair\": f\"{t_los:.2f}|{p_los:.2f}\",\n",
    "                    })\n",
    "\n",
    "            global_idx += bs\n",
    "\n",
    "    df_pred = pd.DataFrame(records)\n",
    "    print(\"[EVAL] df_pred 생성 완료:\", df_pred.shape)\n",
    "    return df_pred\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 10. 메인 실행\n",
    "# =========================================\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) 테스트셋 전체에 대해 예측 + \"원본값|예측값\" CSV 생성\n",
    "    df_pred = build_df_pred_from_test_with_pairs()\n",
    "    csv_path = \"./ppm_test_pred_with_pairs.csv\"\n",
    "    df_pred.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[SAVE] 테스트셋 예측 CSV 저장 완료: {csv_path}\")\n",
    "\n",
    "    # 2) next_event 기준 환자별 예측 정확도\n",
    "    df_pred[\"correct_next\"] = (df_pred[\"target_next_evt\"] == df_pred[\"pred_next_evt\"]).astype(int)\n",
    "    patient_perf = (\n",
    "        df_pred.groupby([\"subject_id\", \"hadm_id\"])[\"correct_next\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"correct_next\": \"next_event_accuracy\"})\n",
    "    )\n",
    "\n",
    "    # 3) accuracy < 1.0 인 환자만 필터\n",
    "    filtered = patient_perf[patient_perf[\"next_event_accuracy\"] < 1.0]\n",
    "\n",
    "    if filtered.empty:\n",
    "        print(\"[WARN] accuracy < 1.0 인 환자가 없습니다. 전체 중 최고 accuracy 환자 사용.\")\n",
    "        candidate_df = patient_perf.copy()\n",
    "    else:\n",
    "        candidate_df = filtered\n",
    "\n",
    "    # 4) 그 중 accuracy 최고 환자 선택\n",
    "    best_row = candidate_df.sort_values(\"next_event_accuracy\", ascending=False).iloc[0]\n",
    "    best_subject_id = int(best_row[\"subject_id\"])\n",
    "    best_hadm_id = int(best_row[\"hadm_id\"])\n",
    "    best_acc = float(best_row[\"next_event_accuracy\"])\n",
    "\n",
    "    print(\"\\n[INFO] 선택된 환자 (accuracy 1이 아닌 환자 중 최고, 없으면 전체 최고)\")\n",
    "    print(f\" subject_id: {best_subject_id}\")\n",
    "    print(f\" hadm_id   : {best_hadm_id}\")\n",
    "    print(f\" accuracy  : {best_acc:.4f}\")\n",
    "\n",
    "    # 5) 선택된 환자로 온라인 예측 실행 + CSV 저장\n",
    "    log_df = run_online_prediction_for_hadm(best_hadm_id)\n",
    "    save_path = f\"./online_pred_best_nonperfect_{best_hadm_id}.csv\"\n",
    "    log_df.to_csv(save_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[SAVE] 온라인 예측 로그 저장 완료: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f56296",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
